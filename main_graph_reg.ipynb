{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ck1d-ER/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from splitter import random_split, scaffold_split\n",
    "\n",
    "\n",
    "from datasets.molnet import MoleculeDataset\n",
    "from model.gnn import GNN\n",
    "from model.mlp import MLP\n",
    "\n",
    "def seed_all(seed):\n",
    "    if not seed:\n",
    "        seed = 0\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return\n",
    "\n",
    "def get_num_task(dataset):\n",
    "    # Get output dimensions of different tasks\n",
    "    if dataset == 'mdck':\n",
    "        return 1\n",
    "    \n",
    "def compute_mean_mad(values):\n",
    "    meann = torch.mean(values)\n",
    "    mad = torch.std(values)\n",
    "    return meann, mad\n",
    "\n",
    "def train_general(model, output_layer, params, device, loader, optimizer, reg_criterion, scheduler):\n",
    "    model.train()\n",
    "    output_layer.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        h = global_mean_pool(model(batch), batch.batch)\n",
    "        pred = output_layer(h)\n",
    "        \n",
    "        y = batch.y.view(pred.shape).float()\n",
    "        y = ((y-meann)/mad)\n",
    "        loss = reg_criterion(pred, y)\n",
    "        \n",
    "        # ===== warmup for CosineAnnealingLR =====\n",
    "        global warmup_steps\n",
    "        warmup_steps += 1\n",
    "        if warmup_steps < params['lr_warmup_steps']:\n",
    "            lr_scale = min(\n",
    "                1.0,\n",
    "                float(warmup_steps)\n",
    "                / float(params['lr_warmup_steps']),\n",
    "            )\n",
    "\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * params['lr']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def eval_general(model, output_layer, device, loader):\n",
    "    model.eval()\n",
    "    output_layer.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            h = global_mean_pool(model(batch), batch.batch)\n",
    "            pred = output_layer(h)\n",
    "    \n",
    "        true = batch.y.view(pred.shape).float()\n",
    "        y_true.append(true)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0).cpu().numpy()\n",
    "    y_pred = (torch.cat(y_pred, dim=0)*mad + meann).cpu().numpy()\n",
    "\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    pearson_r = np.corrcoef(y_true.T, y_pred.T)[1,0]\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'pearson_R': pearson_r}, y_true, y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress specific UserWarning from torch_geometric\n",
    "warnings.filterwarnings(\n",
    "    action='ignore', \n",
    "    category=UserWarning, \n",
    "    message=\"It is not recommended to directly access the internal storage format `data`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mdck\n",
      "Data: Data(x=[61735, 2], edge_index=[2, 134804], edge_attr=[134804, 2], id=[2642], fingerprint=[2642, 1024], y=[2642])\n",
      "[ Using Seed :  0  ]\n",
      "GNN(\n",
      "  (x_embedding1): Embedding(120, 300)\n",
      "  (x_embedding2): Embedding(3, 300)\n",
      "  (gnns): ModuleList(\n",
      "    (0-4): 5 x GINConv()\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "randomly split\n",
      "Epoch: 1\n",
      "Loss: 1.0684886508517795\n",
      "RMSE train: 0.692808\tval: 0.668608\n",
      "MAE train: 0.587427\tval: 0.564390\n",
      "pearson_R train: 0.122249\tval: 0.105952\n",
      "Epoch: 2\n",
      "Loss: 1.091959449979994\n",
      "RMSE train: 0.700219\tval: 0.672385\n",
      "MAE train: 0.575258\tval: 0.550448\n",
      "pearson_R train: -0.035875\tval: -0.035870\n",
      "Epoch: 3\n",
      "Loss: 1.0569810337490506\n",
      "RMSE train: 0.703237\tval: 0.674349\n",
      "MAE train: 0.568151\tval: 0.541700\n",
      "pearson_R train: 0.182462\tval: 0.155119\n",
      "Epoch: 4\n",
      "Loss: 1.056644856929779\n",
      "RMSE train: 0.712016\tval: 0.681845\n",
      "MAE train: 0.561945\tval: 0.533677\n",
      "pearson_R train: 0.207362\tval: 0.170433\n",
      "Epoch: 5\n",
      "Loss: 1.0508143703142803\n",
      "RMSE train: 0.724851\tval: 0.692035\n",
      "MAE train: 0.559283\tval: 0.528866\n",
      "pearson_R train: 0.181262\tval: 0.185482\n",
      "Epoch: 6\n",
      "Loss: 1.0495817396375868\n",
      "RMSE train: 0.722929\tval: 0.689295\n",
      "MAE train: 0.557686\tval: 0.527698\n",
      "pearson_R train: 0.158905\tval: 0.183892\n",
      "Epoch: 7\n",
      "Loss: 1.0381668276256986\n",
      "RMSE train: 0.700172\tval: 0.667154\n",
      "MAE train: 0.550172\tval: 0.520010\n",
      "pearson_R train: 0.258975\tval: 0.290460\n",
      "Epoch: 8\n",
      "Loss: 1.0021826227506\n",
      "RMSE train: 0.687026\tval: 0.655892\n",
      "MAE train: 0.544024\tval: 0.513562\n",
      "pearson_R train: 0.317269\tval: 0.323475\n",
      "Epoch: 9\n",
      "Loss: 0.9949275652567545\n",
      "RMSE train: 0.685676\tval: 0.655100\n",
      "MAE train: 0.538550\tval: 0.508129\n",
      "pearson_R train: 0.318981\tval: 0.320791\n",
      "Epoch: 10\n",
      "Loss: 1.0009292761484783\n",
      "RMSE train: 0.688930\tval: 0.657673\n",
      "MAE train: 0.533482\tval: 0.503127\n",
      "pearson_R train: 0.313959\tval: 0.320078\n",
      "Epoch: 11\n",
      "Loss: 0.9895781013700697\n",
      "RMSE train: 0.689004\tval: 0.657467\n",
      "MAE train: 0.530426\tval: 0.500119\n",
      "pearson_R train: 0.313917\tval: 0.322726\n",
      "Epoch: 12\n",
      "Loss: 1.0201522045665317\n",
      "RMSE train: 0.693787\tval: 0.661545\n",
      "MAE train: 0.527190\tval: 0.496716\n",
      "pearson_R train: 0.319299\tval: 0.330031\n",
      "Epoch: 13\n",
      "Loss: 0.9694134460555183\n",
      "RMSE train: 0.697086\tval: 0.664406\n",
      "MAE train: 0.524784\tval: 0.494752\n",
      "pearson_R train: 0.327231\tval: 0.338444\n",
      "Epoch: 14\n",
      "Loss: 1.0118916498290167\n",
      "RMSE train: 0.691951\tval: 0.659207\n",
      "MAE train: 0.522324\tval: 0.492346\n",
      "pearson_R train: 0.334138\tval: 0.346896\n",
      "Epoch: 15\n",
      "Loss: 1.0065120326148138\n",
      "RMSE train: 0.687907\tval: 0.655664\n",
      "MAE train: 0.520199\tval: 0.490286\n",
      "pearson_R train: 0.340590\tval: 0.353580\n",
      "Epoch: 16\n",
      "Loss: 0.9787247313393487\n",
      "RMSE train: 0.683658\tval: 0.651658\n",
      "MAE train: 0.517921\tval: 0.488133\n",
      "pearson_R train: 0.349947\tval: 0.362858\n",
      "Epoch: 17\n",
      "Loss: 0.9957275390625\n",
      "RMSE train: 0.680265\tval: 0.648698\n",
      "MAE train: 0.516019\tval: 0.486660\n",
      "pearson_R train: 0.357935\tval: 0.369215\n",
      "Epoch: 18\n",
      "Loss: 0.9666302071677314\n",
      "RMSE train: 0.676672\tval: 0.645176\n",
      "MAE train: 0.514365\tval: 0.485148\n",
      "pearson_R train: 0.364155\tval: 0.376276\n",
      "Epoch: 19\n",
      "Loss: 0.9628676308525933\n",
      "RMSE train: 0.674362\tval: 0.642740\n",
      "MAE train: 0.512922\tval: 0.483726\n",
      "pearson_R train: 0.369043\tval: 0.383083\n",
      "Epoch: 20\n",
      "Loss: 0.9562009374300638\n",
      "RMSE train: 0.675492\tval: 0.643840\n",
      "MAE train: 0.511473\tval: 0.483102\n",
      "pearson_R train: 0.374945\tval: 0.389005\n",
      "Epoch: 21\n",
      "Loss: 0.943715492884318\n",
      "RMSE train: 0.675171\tval: 0.643264\n",
      "MAE train: 0.510062\tval: 0.482692\n",
      "pearson_R train: 0.381384\tval: 0.396085\n",
      "Epoch: 22\n",
      "Loss: 0.9620621734195285\n",
      "RMSE train: 0.675915\tval: 0.643814\n",
      "MAE train: 0.508973\tval: 0.482164\n",
      "pearson_R train: 0.388919\tval: 0.403185\n",
      "Epoch: 23\n",
      "Loss: 0.9190666543112861\n",
      "RMSE train: 0.673294\tval: 0.640705\n",
      "MAE train: 0.507909\tval: 0.481403\n",
      "pearson_R train: 0.391928\tval: 0.408691\n",
      "Epoch: 24\n",
      "Loss: 0.9671864840719435\n",
      "RMSE train: 0.676100\tval: 0.643288\n",
      "MAE train: 0.508909\tval: 0.483403\n",
      "pearson_R train: 0.391782\tval: 0.407534\n",
      "Epoch: 25\n",
      "Loss: 0.931466089354621\n",
      "RMSE train: 0.683361\tval: 0.650369\n",
      "MAE train: 0.511698\tval: 0.486395\n",
      "pearson_R train: 0.394624\tval: 0.409599\n",
      "Epoch: 26\n",
      "Loss: 0.9235451486375597\n",
      "RMSE train: 0.674477\tval: 0.642168\n",
      "MAE train: 0.508827\tval: 0.484831\n",
      "pearson_R train: 0.400191\tval: 0.414286\n",
      "Epoch: 27\n",
      "Loss: 0.9004472891489664\n",
      "RMSE train: 0.663130\tval: 0.631896\n",
      "MAE train: 0.504965\tval: 0.482019\n",
      "pearson_R train: 0.409034\tval: 0.422049\n",
      "Epoch: 28\n",
      "Loss: 0.9597117371029324\n",
      "RMSE train: 0.658483\tval: 0.628479\n",
      "MAE train: 0.504530\tval: 0.482496\n",
      "pearson_R train: 0.409766\tval: 0.420765\n",
      "Epoch: 29\n",
      "Loss: 0.9345906641748216\n",
      "RMSE train: 0.662803\tval: 0.633063\n",
      "MAE train: 0.505389\tval: 0.483691\n",
      "pearson_R train: 0.415978\tval: 0.424693\n",
      "Epoch: 30\n",
      "Loss: 0.8945856889088949\n",
      "RMSE train: 0.671975\tval: 0.642046\n",
      "MAE train: 0.508138\tval: 0.486525\n",
      "pearson_R train: 0.423202\tval: 0.429274\n",
      "Epoch: 31\n",
      "Loss: 0.9222146736250983\n",
      "RMSE train: 0.677352\tval: 0.647149\n",
      "MAE train: 0.511624\tval: 0.491151\n",
      "pearson_R train: 0.425040\tval: 0.432192\n",
      "Epoch: 32\n",
      "Loss: 0.9132648971345689\n",
      "RMSE train: 0.677271\tval: 0.648583\n",
      "MAE train: 0.514357\tval: 0.496201\n",
      "pearson_R train: 0.422770\tval: 0.428298\n",
      "Epoch: 33\n",
      "Loss: 0.8961250252193875\n",
      "RMSE train: 0.668778\tval: 0.640785\n",
      "MAE train: 0.509125\tval: 0.490840\n",
      "pearson_R train: 0.429342\tval: 0.432908\n",
      "Epoch: 34\n",
      "Loss: 0.9163123832808601\n",
      "RMSE train: 0.656347\tval: 0.629243\n",
      "MAE train: 0.502390\tval: 0.484347\n",
      "pearson_R train: 0.437734\tval: 0.439633\n",
      "Epoch: 35\n",
      "Loss: 0.8952129152086046\n",
      "RMSE train: 0.652061\tval: 0.627445\n",
      "MAE train: 0.500856\tval: 0.483983\n",
      "pearson_R train: 0.443156\tval: 0.439369\n",
      "Epoch: 36\n",
      "Loss: 0.8967359728283353\n",
      "RMSE train: 0.642235\tval: 0.621257\n",
      "MAE train: 0.498854\tval: 0.485165\n",
      "pearson_R train: 0.441748\tval: 0.434355\n",
      "Epoch: 37\n",
      "Loss: 0.8868045806884766\n",
      "RMSE train: 0.641439\tval: 0.621041\n",
      "MAE train: 0.499625\tval: 0.486777\n",
      "pearson_R train: 0.442215\tval: 0.435467\n",
      "Epoch: 38\n",
      "Loss: 0.8658760322464837\n",
      "RMSE train: 0.641256\tval: 0.620191\n",
      "MAE train: 0.499940\tval: 0.487711\n",
      "pearson_R train: 0.445730\tval: 0.439802\n",
      "Epoch: 39\n",
      "Loss: 0.8568871551089816\n",
      "RMSE train: 0.629573\tval: 0.610340\n",
      "MAE train: 0.497460\tval: 0.485463\n",
      "pearson_R train: 0.450138\tval: 0.443637\n",
      "Epoch: 40\n",
      "Loss: 0.8796913690037198\n",
      "RMSE train: 0.633641\tval: 0.615967\n",
      "MAE train: 0.497036\tval: 0.487284\n",
      "pearson_R train: 0.453398\tval: 0.442182\n",
      "Epoch: 41\n",
      "Loss: 0.8895896673202515\n",
      "RMSE train: 0.636769\tval: 0.619892\n",
      "MAE train: 0.496997\tval: 0.487732\n",
      "pearson_R train: 0.458799\tval: 0.444647\n",
      "Epoch: 42\n",
      "Loss: 0.8792076640658908\n",
      "RMSE train: 0.645133\tval: 0.628200\n",
      "MAE train: 0.499595\tval: 0.489828\n",
      "pearson_R train: 0.460016\tval: 0.442350\n",
      "Epoch: 43\n",
      "Loss: 0.8606501552793715\n",
      "RMSE train: 0.635181\tval: 0.620875\n",
      "MAE train: 0.494783\tval: 0.487576\n",
      "pearson_R train: 0.466960\tval: 0.445160\n",
      "Epoch: 44\n",
      "Loss: 0.9009885854191251\n",
      "RMSE train: 0.637212\tval: 0.621331\n",
      "MAE train: 0.493627\tval: 0.484889\n",
      "pearson_R train: 0.473142\tval: 0.451098\n",
      "Epoch: 45\n",
      "Loss: 0.8750602602958679\n",
      "RMSE train: 0.633083\tval: 0.616393\n",
      "MAE train: 0.491615\tval: 0.481566\n",
      "pearson_R train: 0.478267\tval: 0.456897\n",
      "Epoch: 46\n",
      "Loss: 0.8583997156884935\n",
      "RMSE train: 0.621123\tval: 0.605379\n",
      "MAE train: 0.488108\tval: 0.478735\n",
      "pearson_R train: 0.480973\tval: 0.462181\n",
      "Epoch: 47\n",
      "Loss: 0.8624215920766195\n",
      "RMSE train: 0.621043\tval: 0.606351\n",
      "MAE train: 0.489700\tval: 0.483914\n",
      "pearson_R train: 0.480130\tval: 0.463543\n",
      "Epoch: 48\n",
      "Loss: 0.8306324217054579\n",
      "RMSE train: 0.614741\tval: 0.600030\n",
      "MAE train: 0.486747\tval: 0.479994\n",
      "pearson_R train: 0.488613\tval: 0.472561\n",
      "Epoch: 49\n",
      "Loss: 0.8598132464620802\n",
      "RMSE train: 0.617786\tval: 0.601617\n",
      "MAE train: 0.485566\tval: 0.476917\n",
      "pearson_R train: 0.492522\tval: 0.475515\n",
      "Epoch: 50\n",
      "Loss: 0.8701495594448514\n",
      "RMSE train: 0.618006\tval: 0.603447\n",
      "MAE train: 0.486987\tval: 0.480509\n",
      "pearson_R train: 0.488817\tval: 0.471815\n",
      "Epoch: 51\n",
      "Loss: 0.859796855184767\n",
      "RMSE train: 0.628307\tval: 0.614035\n",
      "MAE train: 0.489739\tval: 0.483930\n",
      "pearson_R train: 0.491142\tval: 0.469022\n",
      "Epoch: 52\n",
      "Loss: 0.8419859276877509\n",
      "RMSE train: 0.623969\tval: 0.610547\n",
      "MAE train: 0.487243\tval: 0.482193\n",
      "pearson_R train: 0.493760\tval: 0.469842\n",
      "Epoch: 53\n",
      "Loss: 0.8584199945131937\n",
      "RMSE train: 0.620600\tval: 0.607231\n",
      "MAE train: 0.484918\tval: 0.479870\n",
      "pearson_R train: 0.495125\tval: 0.470280\n",
      "Epoch: 54\n",
      "Loss: 0.8574051592085097\n",
      "RMSE train: 0.621931\tval: 0.606531\n",
      "MAE train: 0.484524\tval: 0.476646\n",
      "pearson_R train: 0.496177\tval: 0.472481\n",
      "Epoch: 55\n",
      "Loss: 0.8568213979403178\n",
      "RMSE train: 0.627300\tval: 0.612513\n",
      "MAE train: 0.485872\tval: 0.477768\n",
      "pearson_R train: 0.494563\tval: 0.466948\n",
      "Epoch: 56\n",
      "Loss: 0.8500348130861918\n",
      "RMSE train: 0.640120\tval: 0.624229\n",
      "MAE train: 0.490770\tval: 0.482984\n",
      "pearson_R train: 0.493894\tval: 0.464272\n",
      "Epoch: 57\n",
      "Loss: 0.8391104870372348\n",
      "RMSE train: 0.625065\tval: 0.612256\n",
      "MAE train: 0.484803\tval: 0.480964\n",
      "pearson_R train: 0.499751\tval: 0.470882\n",
      "Epoch: 58\n",
      "Loss: 0.8402891953786215\n",
      "RMSE train: 0.612901\tval: 0.604610\n",
      "MAE train: 0.481505\tval: 0.481937\n",
      "pearson_R train: 0.501812\tval: 0.470837\n",
      "Epoch: 59\n",
      "Loss: 0.8110301295916239\n",
      "RMSE train: 0.614165\tval: 0.605911\n",
      "MAE train: 0.480774\tval: 0.480939\n",
      "pearson_R train: 0.505541\tval: 0.473146\n",
      "Epoch: 60\n",
      "Loss: 0.8217284613185458\n",
      "RMSE train: 0.627262\tval: 0.614550\n",
      "MAE train: 0.489849\tval: 0.485664\n",
      "pearson_R train: 0.503163\tval: 0.479255\n",
      "Epoch: 61\n",
      "Loss: 0.8152381115489535\n",
      "RMSE train: 0.627773\tval: 0.616562\n",
      "MAE train: 0.490032\tval: 0.486974\n",
      "pearson_R train: 0.501736\tval: 0.474310\n",
      "Epoch: 62\n",
      "Loss: 0.8261163234710693\n",
      "RMSE train: 0.621408\tval: 0.616418\n",
      "MAE train: 0.488133\tval: 0.490544\n",
      "pearson_R train: 0.503297\tval: 0.469483\n",
      "Epoch: 63\n",
      "Loss: 0.808758364783393\n",
      "RMSE train: 0.615859\tval: 0.615042\n",
      "MAE train: 0.485746\tval: 0.491057\n",
      "pearson_R train: 0.503337\tval: 0.466645\n",
      "Epoch: 64\n",
      "Loss: 0.8388183845414056\n",
      "RMSE train: 0.607728\tval: 0.607064\n",
      "MAE train: 0.480676\tval: 0.485727\n",
      "pearson_R train: 0.513193\tval: 0.473847\n",
      "Epoch: 65\n",
      "Loss: 0.8263734910223219\n",
      "RMSE train: 0.608219\tval: 0.607323\n",
      "MAE train: 0.480753\tval: 0.485080\n",
      "pearson_R train: 0.513566\tval: 0.472903\n",
      "Epoch: 66\n",
      "Loss: 0.8510627349217733\n",
      "RMSE train: 0.612271\tval: 0.610745\n",
      "MAE train: 0.482707\tval: 0.487175\n",
      "pearson_R train: 0.510678\tval: 0.469735\n",
      "Epoch: 67\n",
      "Loss: 0.827426963382297\n",
      "RMSE train: 0.620931\tval: 0.616296\n",
      "MAE train: 0.487578\tval: 0.489338\n",
      "pearson_R train: 0.509243\tval: 0.474111\n",
      "Epoch: 68\n",
      "Loss: 0.8072783946990967\n",
      "RMSE train: 0.617851\tval: 0.613221\n",
      "MAE train: 0.482738\tval: 0.486116\n",
      "pearson_R train: 0.516588\tval: 0.480974\n",
      "Epoch: 69\n",
      "Loss: 0.8294413619571261\n",
      "RMSE train: 0.604975\tval: 0.598928\n",
      "MAE train: 0.474934\tval: 0.475542\n",
      "pearson_R train: 0.524687\tval: 0.490668\n",
      "Epoch: 70\n",
      "Loss: 0.8302768071492513\n",
      "RMSE train: 0.623504\tval: 0.613588\n",
      "MAE train: 0.484774\tval: 0.482515\n",
      "pearson_R train: 0.516376\tval: 0.484604\n",
      "Epoch: 71\n",
      "Loss: 0.8059294290012784\n",
      "RMSE train: 0.629636\tval: 0.619403\n",
      "MAE train: 0.490382\tval: 0.490096\n",
      "pearson_R train: 0.507269\tval: 0.481018\n",
      "Epoch: 72\n",
      "Loss: 0.8164914184146457\n",
      "RMSE train: 0.616588\tval: 0.608070\n",
      "MAE train: 0.481593\tval: 0.482990\n",
      "pearson_R train: 0.516914\tval: 0.487672\n",
      "Epoch: 73\n",
      "Loss: 0.8121302061610751\n",
      "RMSE train: 0.598028\tval: 0.592879\n",
      "MAE train: 0.474979\tval: 0.479194\n",
      "pearson_R train: 0.523771\tval: 0.495424\n",
      "Epoch: 74\n",
      "Loss: 0.7942465013927884\n",
      "RMSE train: 0.602909\tval: 0.602096\n",
      "MAE train: 0.479464\tval: 0.489066\n",
      "pearson_R train: 0.514579\tval: 0.481486\n",
      "Epoch: 75\n",
      "Loss: 0.810101204448276\n",
      "RMSE train: 0.599572\tval: 0.598618\n",
      "MAE train: 0.474754\tval: 0.480564\n",
      "pearson_R train: 0.525701\tval: 0.486228\n",
      "Epoch: 76\n",
      "Loss: 0.8383790983094109\n",
      "RMSE train: 0.621541\tval: 0.613947\n",
      "MAE train: 0.485621\tval: 0.486764\n",
      "pearson_R train: 0.515597\tval: 0.485956\n",
      "Epoch: 77\n",
      "Loss: 0.7863239513503181\n",
      "RMSE train: 0.618714\tval: 0.614977\n",
      "MAE train: 0.483664\tval: 0.489116\n",
      "pearson_R train: 0.521001\tval: 0.485881\n",
      "Epoch: 78\n",
      "Loss: 0.8080214394463433\n",
      "RMSE train: 0.603046\tval: 0.601917\n",
      "MAE train: 0.478406\tval: 0.485704\n",
      "pearson_R train: 0.522572\tval: 0.490817\n",
      "Epoch: 79\n",
      "Loss: 0.78341586722268\n",
      "RMSE train: 0.602255\tval: 0.610741\n",
      "MAE train: 0.489202\tval: 0.503779\n",
      "pearson_R train: 0.525337\tval: 0.493358\n",
      "Epoch: 80\n",
      "Loss: 0.776364286740621\n",
      "RMSE train: 0.593024\tval: 0.597196\n",
      "MAE train: 0.478473\tval: 0.488446\n",
      "pearson_R train: 0.530159\tval: 0.498999\n",
      "Epoch: 81\n",
      "Loss: 0.8171796732478671\n",
      "RMSE train: 0.600752\tval: 0.599217\n",
      "MAE train: 0.474421\tval: 0.477861\n",
      "pearson_R train: 0.530419\tval: 0.495544\n",
      "Epoch: 82\n",
      "Loss: 0.796819825967153\n",
      "RMSE train: 0.603932\tval: 0.608276\n",
      "MAE train: 0.476237\tval: 0.484688\n",
      "pearson_R train: 0.530029\tval: 0.487637\n",
      "Epoch: 83\n",
      "Loss: 0.8033444881439209\n",
      "RMSE train: 0.624925\tval: 0.620732\n",
      "MAE train: 0.488200\tval: 0.490048\n",
      "pearson_R train: 0.525894\tval: 0.490217\n",
      "Epoch: 84\n",
      "Loss: 0.7963104446729025\n",
      "RMSE train: 0.595990\tval: 0.602604\n",
      "MAE train: 0.475712\tval: 0.489974\n",
      "pearson_R train: 0.533532\tval: 0.497735\n",
      "Epoch: 85\n",
      "Loss: 0.8080929319063822\n",
      "RMSE train: 0.596507\tval: 0.599151\n",
      "MAE train: 0.478007\tval: 0.489462\n",
      "pearson_R train: 0.528695\tval: 0.498916\n",
      "Epoch: 86\n",
      "Loss: 0.78420932425393\n",
      "RMSE train: 0.597085\tval: 0.596716\n",
      "MAE train: 0.477027\tval: 0.485901\n",
      "pearson_R train: 0.529156\tval: 0.503343\n",
      "Epoch: 87\n",
      "Loss: 0.7953615585962931\n",
      "RMSE train: 0.605080\tval: 0.602141\n",
      "MAE train: 0.480768\tval: 0.487821\n",
      "pearson_R train: 0.524275\tval: 0.502177\n",
      "Epoch: 88\n",
      "Loss: 0.794744869073232\n",
      "RMSE train: 0.598533\tval: 0.595846\n",
      "MAE train: 0.472328\tval: 0.476920\n",
      "pearson_R train: 0.530079\tval: 0.498389\n",
      "Epoch: 89\n",
      "Loss: 0.7974417938126458\n",
      "RMSE train: 0.615255\tval: 0.609514\n",
      "MAE train: 0.478023\tval: 0.479038\n",
      "pearson_R train: 0.533147\tval: 0.495345\n",
      "Epoch: 90\n",
      "Loss: 0.7916109032101102\n",
      "RMSE train: 0.600932\tval: 0.598855\n",
      "MAE train: 0.470403\tval: 0.474691\n",
      "pearson_R train: 0.538816\tval: 0.498632\n",
      "Epoch: 91\n",
      "Loss: 0.7891314559512668\n",
      "RMSE train: 0.591038\tval: 0.603967\n",
      "MAE train: 0.474106\tval: 0.493514\n",
      "pearson_R train: 0.538199\tval: 0.496199\n",
      "Epoch: 92\n",
      "Loss: 0.7717237803671095\n",
      "RMSE train: 0.588855\tval: 0.596046\n",
      "MAE train: 0.473959\tval: 0.488495\n",
      "pearson_R train: 0.537127\tval: 0.497530\n",
      "Epoch: 93\n",
      "Loss: 0.8091411656803555\n",
      "RMSE train: 0.591681\tval: 0.603173\n",
      "MAE train: 0.480225\tval: 0.499055\n",
      "pearson_R train: 0.537204\tval: 0.492500\n",
      "Epoch: 94\n",
      "Loss: 0.8028449747297499\n",
      "RMSE train: 0.608610\tval: 0.623883\n",
      "MAE train: 0.496204\tval: 0.518442\n",
      "pearson_R train: 0.530903\tval: 0.488699\n",
      "Epoch: 95\n",
      "Loss: 0.7896906203693814\n",
      "RMSE train: 0.599448\tval: 0.614055\n",
      "MAE train: 0.494329\tval: 0.511276\n",
      "pearson_R train: 0.546029\tval: 0.500379\n",
      "Epoch: 96\n",
      "Loss: 0.7784042821990119\n",
      "RMSE train: 0.585170\tval: 0.596885\n",
      "MAE train: 0.474823\tval: 0.492146\n",
      "pearson_R train: 0.551628\tval: 0.511256\n",
      "Epoch: 97\n",
      "Loss: 0.8001167045699226\n",
      "RMSE train: 0.593534\tval: 0.604309\n",
      "MAE train: 0.484748\tval: 0.500988\n",
      "pearson_R train: 0.547194\tval: 0.509873\n",
      "Epoch: 98\n",
      "Loss: 0.7810796101888021\n",
      "RMSE train: 0.611372\tval: 0.631007\n",
      "MAE train: 0.504017\tval: 0.525087\n",
      "pearson_R train: 0.549011\tval: 0.503950\n",
      "Epoch: 99\n",
      "Loss: 0.7734656267695956\n",
      "RMSE train: 0.587454\tval: 0.600793\n",
      "MAE train: 0.475390\tval: 0.492731\n",
      "pearson_R train: 0.550909\tval: 0.510516\n",
      "Epoch: 100\n",
      "Loss: 0.7643504407670763\n",
      "RMSE train: 0.598832\tval: 0.599284\n",
      "MAE train: 0.468269\tval: 0.475543\n",
      "pearson_R train: 0.554044\tval: 0.506152\n",
      "Epoch: 101\n",
      "Loss: 0.7678456505139669\n",
      "RMSE train: 0.582313\tval: 0.588591\n",
      "MAE train: 0.458964\tval: 0.472451\n",
      "pearson_R train: 0.557802\tval: 0.500464\n",
      "Epoch: 102\n",
      "Loss: 0.7711439596282111\n",
      "RMSE train: 0.579693\tval: 0.587867\n",
      "MAE train: 0.461480\tval: 0.476644\n",
      "pearson_R train: 0.552572\tval: 0.503712\n",
      "Epoch: 103\n",
      "Loss: 0.7986128264003329\n",
      "RMSE train: 0.583174\tval: 0.590117\n",
      "MAE train: 0.463377\tval: 0.476532\n",
      "pearson_R train: 0.547964\tval: 0.505385\n",
      "Epoch: 104\n",
      "Loss: 0.7808658944235908\n",
      "RMSE train: 0.582827\tval: 0.587988\n",
      "MAE train: 0.460160\tval: 0.471615\n",
      "pearson_R train: 0.552618\tval: 0.502244\n",
      "Epoch: 105\n",
      "Loss: 0.776480085319943\n",
      "RMSE train: 0.580075\tval: 0.595787\n",
      "MAE train: 0.462182\tval: 0.482291\n",
      "pearson_R train: 0.553752\tval: 0.496976\n",
      "Epoch: 106\n",
      "Loss: 0.7896543476316664\n",
      "RMSE train: 0.599348\tval: 0.620680\n",
      "MAE train: 0.488284\tval: 0.508942\n",
      "pearson_R train: 0.534147\tval: 0.478218\n",
      "Epoch: 107\n",
      "Loss: 0.7864365180333456\n",
      "RMSE train: 0.606437\tval: 0.638874\n",
      "MAE train: 0.499290\tval: 0.527838\n",
      "pearson_R train: 0.558601\tval: 0.495893\n",
      "Epoch: 108\n",
      "Loss: 0.7718068758646647\n",
      "RMSE train: 0.587266\tval: 0.608006\n",
      "MAE train: 0.476397\tval: 0.498837\n",
      "pearson_R train: 0.551011\tval: 0.497367\n",
      "Epoch: 109\n",
      "Loss: 0.7655677596728007\n",
      "RMSE train: 0.582888\tval: 0.602517\n",
      "MAE train: 0.473863\tval: 0.494680\n",
      "pearson_R train: 0.556609\tval: 0.500841\n",
      "Epoch: 110\n",
      "Loss: 0.791731576124827\n",
      "RMSE train: 0.585915\tval: 0.600696\n",
      "MAE train: 0.465696\tval: 0.484851\n",
      "pearson_R train: 0.555936\tval: 0.507242\n",
      "Epoch: 111\n",
      "Loss: 0.7612324953079224\n",
      "RMSE train: 0.614796\tval: 0.609016\n",
      "MAE train: 0.475209\tval: 0.480989\n",
      "pearson_R train: 0.539957\tval: 0.502464\n",
      "Epoch: 112\n",
      "Loss: 0.7705428070492215\n",
      "RMSE train: 0.580682\tval: 0.590366\n",
      "MAE train: 0.462431\tval: 0.478121\n",
      "pearson_R train: 0.555828\tval: 0.510216\n",
      "Epoch: 113\n",
      "Loss: 0.7635957598686218\n",
      "RMSE train: 0.597631\tval: 0.629501\n",
      "MAE train: 0.490806\tval: 0.521142\n",
      "pearson_R train: 0.572672\tval: 0.508467\n",
      "Epoch: 114\n",
      "Loss: 0.7782199250327216\n",
      "RMSE train: 0.598485\tval: 0.627806\n",
      "MAE train: 0.490187\tval: 0.519283\n",
      "pearson_R train: 0.568656\tval: 0.509238\n",
      "Epoch: 115\n",
      "Loss: 0.7743960552745395\n",
      "RMSE train: 0.576224\tval: 0.598114\n",
      "MAE train: 0.469325\tval: 0.491714\n",
      "pearson_R train: 0.573746\tval: 0.515457\n",
      "Epoch: 116\n",
      "Loss: 0.7740539775954353\n",
      "RMSE train: 0.575598\tval: 0.583619\n",
      "MAE train: 0.455143\tval: 0.466327\n",
      "pearson_R train: 0.566967\tval: 0.512159\n",
      "Epoch: 117\n",
      "Loss: 0.7718511753612094\n",
      "RMSE train: 0.574815\tval: 0.592519\n",
      "MAE train: 0.459718\tval: 0.478338\n",
      "pearson_R train: 0.566185\tval: 0.508179\n",
      "Epoch: 118\n",
      "Loss: 0.745370434390174\n",
      "RMSE train: 0.587034\tval: 0.618087\n",
      "MAE train: 0.471866\tval: 0.504466\n",
      "pearson_R train: 0.566547\tval: 0.509187\n",
      "Epoch: 119\n",
      "Loss: 0.773458308643765\n",
      "RMSE train: 0.577616\tval: 0.595020\n",
      "MAE train: 0.460586\tval: 0.482193\n",
      "pearson_R train: 0.562966\tval: 0.507544\n",
      "Epoch: 120\n",
      "Loss: 0.7679418126742045\n",
      "RMSE train: 0.565951\tval: 0.584219\n",
      "MAE train: 0.451340\tval: 0.469463\n",
      "pearson_R train: 0.577981\tval: 0.508026\n",
      "Epoch: 121\n",
      "Loss: 0.7823829385969374\n",
      "RMSE train: 0.585432\tval: 0.594345\n",
      "MAE train: 0.457434\tval: 0.470288\n",
      "pearson_R train: 0.561527\tval: 0.505771\n",
      "Epoch: 122\n",
      "Loss: 0.7482100261582268\n",
      "RMSE train: 0.563962\tval: 0.587671\n",
      "MAE train: 0.456142\tval: 0.477821\n",
      "pearson_R train: 0.582308\tval: 0.504771\n",
      "Epoch: 123\n",
      "Loss: 0.7683925694889493\n",
      "RMSE train: 0.570704\tval: 0.599427\n",
      "MAE train: 0.457600\tval: 0.484483\n",
      "pearson_R train: 0.577571\tval: 0.510057\n",
      "Epoch: 124\n",
      "Loss: 0.7718249162038168\n",
      "RMSE train: 0.572905\tval: 0.595866\n",
      "MAE train: 0.462987\tval: 0.487217\n",
      "pearson_R train: 0.574759\tval: 0.513725\n",
      "Epoch: 125\n",
      "Loss: 0.7657434410519071\n",
      "RMSE train: 0.603579\tval: 0.642951\n",
      "MAE train: 0.496526\tval: 0.533381\n",
      "pearson_R train: 0.579654\tval: 0.506534\n",
      "Epoch: 126\n",
      "Loss: 0.7460731863975525\n",
      "RMSE train: 0.575954\tval: 0.614146\n",
      "MAE train: 0.466704\tval: 0.502564\n",
      "pearson_R train: 0.580813\tval: 0.503682\n",
      "Epoch: 127\n",
      "Loss: 0.7700228691101074\n",
      "RMSE train: 0.577918\tval: 0.603433\n",
      "MAE train: 0.458838\tval: 0.484900\n",
      "pearson_R train: 0.569201\tval: 0.504418\n",
      "Epoch: 128\n",
      "Loss: 0.7425155838330587\n",
      "RMSE train: 0.566365\tval: 0.597672\n",
      "MAE train: 0.458282\tval: 0.486856\n",
      "pearson_R train: 0.580182\tval: 0.498082\n",
      "Epoch: 129\n",
      "Loss: 0.7705592579311795\n",
      "RMSE train: 0.574509\tval: 0.604752\n",
      "MAE train: 0.462038\tval: 0.490831\n",
      "pearson_R train: 0.574484\tval: 0.505242\n",
      "Epoch: 130\n",
      "Loss: 0.7404870192209879\n",
      "RMSE train: 0.599512\tval: 0.638520\n",
      "MAE train: 0.495318\tval: 0.529025\n",
      "pearson_R train: 0.577014\tval: 0.496798\n",
      "Epoch: 131\n",
      "Loss: 0.7484654519293044\n",
      "RMSE train: 0.569188\tval: 0.590336\n",
      "MAE train: 0.452385\tval: 0.471167\n",
      "pearson_R train: 0.575459\tval: 0.508077\n",
      "Epoch: 132\n",
      "Loss: 0.7378681699434916\n",
      "RMSE train: 0.588071\tval: 0.599183\n",
      "MAE train: 0.453787\tval: 0.467169\n",
      "pearson_R train: 0.572300\tval: 0.504321\n",
      "Epoch: 133\n",
      "Loss: 0.7690832416216532\n",
      "RMSE train: 0.565089\tval: 0.584625\n",
      "MAE train: 0.448390\tval: 0.470764\n",
      "pearson_R train: 0.582148\tval: 0.512467\n",
      "Epoch: 134\n",
      "Loss: 0.7726184593306648\n",
      "RMSE train: 0.587834\tval: 0.593268\n",
      "MAE train: 0.459442\tval: 0.470154\n",
      "pearson_R train: 0.564390\tval: 0.518550\n",
      "Epoch: 135\n",
      "Loss: 0.74786067671246\n",
      "RMSE train: 0.569308\tval: 0.579949\n",
      "MAE train: 0.447550\tval: 0.459756\n",
      "pearson_R train: 0.579915\tval: 0.524191\n",
      "Epoch: 136\n",
      "Loss: 0.7320122453901503\n",
      "RMSE train: 0.565970\tval: 0.591338\n",
      "MAE train: 0.450809\tval: 0.478774\n",
      "pearson_R train: 0.584148\tval: 0.520022\n",
      "Epoch: 137\n",
      "Loss: 0.7538679838180542\n",
      "RMSE train: 0.570284\tval: 0.600044\n",
      "MAE train: 0.454685\tval: 0.486398\n",
      "pearson_R train: 0.582704\tval: 0.519266\n",
      "Epoch: 138\n",
      "Loss: 0.7649203605122037\n",
      "RMSE train: 0.592097\tval: 0.633183\n",
      "MAE train: 0.486906\tval: 0.524530\n",
      "pearson_R train: 0.587661\tval: 0.510107\n",
      "Epoch: 139\n",
      "Loss: 0.7323876354429457\n",
      "RMSE train: 0.572992\tval: 0.604358\n",
      "MAE train: 0.466825\tval: 0.496405\n",
      "pearson_R train: 0.586076\tval: 0.516679\n",
      "Epoch: 140\n",
      "Loss: 0.752919594446818\n",
      "RMSE train: 0.598770\tval: 0.637967\n",
      "MAE train: 0.491855\tval: 0.527281\n",
      "pearson_R train: 0.588621\tval: 0.517039\n",
      "Epoch: 141\n",
      "Loss: 0.7424369388156467\n",
      "RMSE train: 0.563532\tval: 0.590456\n",
      "MAE train: 0.454875\tval: 0.484689\n",
      "pearson_R train: 0.587799\tval: 0.513381\n",
      "Epoch: 142\n",
      "Loss: 0.7436119251781039\n",
      "RMSE train: 0.571318\tval: 0.584529\n",
      "MAE train: 0.444264\tval: 0.461713\n",
      "pearson_R train: 0.586813\tval: 0.523749\n",
      "Epoch: 143\n",
      "Loss: 0.7815201547410753\n",
      "RMSE train: 0.580489\tval: 0.587649\n",
      "MAE train: 0.445357\tval: 0.455046\n",
      "pearson_R train: 0.586945\tval: 0.517257\n",
      "Epoch: 144\n",
      "Loss: 0.7512937254375882\n",
      "RMSE train: 0.567846\tval: 0.604612\n",
      "MAE train: 0.463397\tval: 0.493962\n",
      "pearson_R train: 0.593979\tval: 0.513937\n",
      "Epoch: 145\n",
      "Loss: 0.7338970171080695\n",
      "RMSE train: 0.620516\tval: 0.666904\n",
      "MAE train: 0.513339\tval: 0.549614\n",
      "pearson_R train: 0.584155\tval: 0.505440\n",
      "Epoch: 146\n",
      "Loss: 0.7279549373520745\n",
      "RMSE train: 0.586485\tval: 0.627976\n",
      "MAE train: 0.482186\tval: 0.516712\n",
      "pearson_R train: 0.588657\tval: 0.508669\n",
      "Epoch: 147\n",
      "Loss: 0.757167398929596\n",
      "RMSE train: 0.555866\tval: 0.582458\n",
      "MAE train: 0.440926\tval: 0.466068\n",
      "pearson_R train: 0.599917\tval: 0.519100\n",
      "Epoch: 148\n",
      "Loss: 0.7165931794378493\n",
      "RMSE train: 0.553575\tval: 0.590068\n",
      "MAE train: 0.442628\tval: 0.474536\n",
      "pearson_R train: 0.604631\tval: 0.520519\n",
      "Epoch: 149\n",
      "Loss: 0.7057773868242899\n",
      "RMSE train: 0.575095\tval: 0.621736\n",
      "MAE train: 0.470568\tval: 0.510004\n",
      "pearson_R train: 0.601050\tval: 0.512895\n",
      "Epoch: 150\n",
      "Loss: 0.729689810011122\n",
      "RMSE train: 0.573859\tval: 0.623220\n",
      "MAE train: 0.471339\tval: 0.513228\n",
      "pearson_R train: 0.605100\tval: 0.512419\n",
      "Epoch: 151\n",
      "Loss: 0.7133872045411004\n",
      "RMSE train: 0.652329\tval: 0.707073\n",
      "MAE train: 0.542012\tval: 0.585539\n",
      "pearson_R train: 0.600042\tval: 0.516455\n",
      "Epoch: 152\n",
      "Loss: 0.7382144530614217\n",
      "RMSE train: 0.570801\tval: 0.614426\n",
      "MAE train: 0.460932\tval: 0.500898\n",
      "pearson_R train: 0.603493\tval: 0.526260\n",
      "Epoch: 153\n",
      "Loss: 0.7575398087501526\n",
      "RMSE train: 0.558527\tval: 0.588795\n",
      "MAE train: 0.447567\tval: 0.476990\n",
      "pearson_R train: 0.596107\tval: 0.517870\n",
      "Epoch: 154\n",
      "Loss: 0.7370362215571933\n",
      "RMSE train: 0.587055\tval: 0.631934\n",
      "MAE train: 0.476880\tval: 0.515638\n",
      "pearson_R train: 0.605064\tval: 0.526368\n",
      "Epoch: 155\n",
      "Loss: 0.7126289010047913\n",
      "RMSE train: 0.561301\tval: 0.588310\n",
      "MAE train: 0.445234\tval: 0.470049\n",
      "pearson_R train: 0.595851\tval: 0.527450\n",
      "Epoch: 156\n",
      "Loss: 0.7168693741162618\n",
      "RMSE train: 0.578574\tval: 0.623359\n",
      "MAE train: 0.472020\tval: 0.509547\n",
      "pearson_R train: 0.608922\tval: 0.527473\n",
      "Epoch: 157\n",
      "Loss: 0.7399983604749044\n",
      "RMSE train: 0.572291\tval: 0.615393\n",
      "MAE train: 0.467798\tval: 0.504488\n",
      "pearson_R train: 0.609890\tval: 0.526272\n",
      "Epoch: 158\n",
      "Loss: 0.7086596422725253\n",
      "RMSE train: 0.576660\tval: 0.622090\n",
      "MAE train: 0.474981\tval: 0.512955\n",
      "pearson_R train: 0.604303\tval: 0.508494\n",
      "Epoch: 159\n",
      "Loss: 0.7508210208680894\n",
      "RMSE train: 0.557516\tval: 0.598803\n",
      "MAE train: 0.448588\tval: 0.482522\n",
      "pearson_R train: 0.609087\tval: 0.528901\n",
      "Epoch: 160\n",
      "Loss: 0.7274999419848124\n",
      "RMSE train: 0.565671\tval: 0.610487\n",
      "MAE train: 0.460381\tval: 0.498752\n",
      "pearson_R train: 0.608921\tval: 0.526314\n",
      "Epoch: 161\n",
      "Loss: 0.707945810423957\n",
      "RMSE train: 0.587046\tval: 0.636019\n",
      "MAE train: 0.482656\tval: 0.521561\n",
      "pearson_R train: 0.607952\tval: 0.530731\n",
      "Epoch: 162\n",
      "Loss: 0.7229091789987352\n",
      "RMSE train: 0.551378\tval: 0.583283\n",
      "MAE train: 0.440586\tval: 0.466770\n",
      "pearson_R train: 0.607086\tval: 0.525011\n",
      "Epoch: 163\n",
      "Loss: 0.7706006632910835\n",
      "RMSE train: 0.561517\tval: 0.577966\n",
      "MAE train: 0.435448\tval: 0.448787\n",
      "pearson_R train: 0.603661\tval: 0.534228\n",
      "Epoch: 164\n",
      "Loss: 0.7446130315462748\n",
      "RMSE train: 0.565881\tval: 0.604751\n",
      "MAE train: 0.464209\tval: 0.495614\n",
      "pearson_R train: 0.612970\tval: 0.531804\n",
      "Epoch: 165\n",
      "Loss: 0.7195286949475607\n",
      "RMSE train: 0.586320\tval: 0.628347\n",
      "MAE train: 0.483619\tval: 0.517399\n",
      "pearson_R train: 0.611030\tval: 0.532018\n",
      "Epoch: 166\n",
      "Loss: 0.7343724634912279\n",
      "RMSE train: 0.547755\tval: 0.582604\n",
      "MAE train: 0.436376\tval: 0.464421\n",
      "pearson_R train: 0.612959\tval: 0.520363\n",
      "Epoch: 167\n",
      "Loss: 0.7406000296274821\n",
      "RMSE train: 0.637700\tval: 0.694828\n",
      "MAE train: 0.532689\tval: 0.578979\n",
      "pearson_R train: 0.604736\tval: 0.511971\n",
      "Epoch: 168\n",
      "Loss: 0.7168268495135837\n",
      "RMSE train: 0.629216\tval: 0.689039\n",
      "MAE train: 0.516592\tval: 0.565114\n",
      "pearson_R train: 0.609889\tval: 0.522314\n",
      "Epoch: 169\n",
      "Loss: 0.7093419896231757\n",
      "RMSE train: 0.546242\tval: 0.588929\n",
      "MAE train: 0.438824\tval: 0.475495\n",
      "pearson_R train: 0.618802\tval: 0.520293\n",
      "Epoch: 170\n",
      "Loss: 0.7082122895452712\n",
      "RMSE train: 0.555465\tval: 0.602064\n",
      "MAE train: 0.446667\tval: 0.485402\n",
      "pearson_R train: 0.614329\tval: 0.525812\n",
      "Epoch: 171\n",
      "Loss: 0.6971942583719889\n",
      "RMSE train: 0.557767\tval: 0.596984\n",
      "MAE train: 0.453494\tval: 0.486489\n",
      "pearson_R train: 0.611155\tval: 0.524092\n",
      "Epoch: 172\n",
      "Loss: 0.7292515105671353\n",
      "RMSE train: 0.549482\tval: 0.587484\n",
      "MAE train: 0.443644\tval: 0.474677\n",
      "pearson_R train: 0.617747\tval: 0.531144\n",
      "Epoch: 173\n",
      "Loss: 0.7227864927715726\n",
      "RMSE train: 0.548872\tval: 0.591877\n",
      "MAE train: 0.444444\tval: 0.481975\n",
      "pearson_R train: 0.624215\tval: 0.530799\n",
      "Epoch: 174\n",
      "Loss: 0.7114988565444946\n",
      "RMSE train: 0.545722\tval: 0.576465\n",
      "MAE train: 0.431997\tval: 0.455930\n",
      "pearson_R train: 0.617854\tval: 0.529562\n",
      "Epoch: 175\n",
      "Loss: 0.7309255070156522\n",
      "RMSE train: 0.540732\tval: 0.583872\n",
      "MAE train: 0.431620\tval: 0.468422\n",
      "pearson_R train: 0.625607\tval: 0.519687\n",
      "Epoch: 176\n",
      "Loss: 0.7245225840144687\n",
      "RMSE train: 0.560601\tval: 0.613747\n",
      "MAE train: 0.457604\tval: 0.501595\n",
      "pearson_R train: 0.617285\tval: 0.511449\n",
      "Epoch: 177\n",
      "Loss: 0.7287927932209439\n",
      "RMSE train: 0.575425\tval: 0.627881\n",
      "MAE train: 0.471624\tval: 0.516841\n",
      "pearson_R train: 0.611195\tval: 0.514224\n",
      "Epoch: 178\n",
      "Loss: 0.7156187693277994\n",
      "RMSE train: 0.640077\tval: 0.699253\n",
      "MAE train: 0.538880\tval: 0.586639\n",
      "pearson_R train: 0.611328\tval: 0.512375\n",
      "Epoch: 179\n",
      "Loss: 0.70007942782508\n",
      "RMSE train: 0.538006\tval: 0.580797\n",
      "MAE train: 0.430026\tval: 0.464856\n",
      "pearson_R train: 0.632214\tval: 0.537787\n",
      "Epoch: 180\n",
      "Loss: 0.7229329016473558\n",
      "RMSE train: 0.541941\tval: 0.571132\n",
      "MAE train: 0.423439\tval: 0.449445\n",
      "pearson_R train: 0.627750\tval: 0.538792\n",
      "Epoch: 181\n",
      "Loss: 0.6918691330485873\n",
      "RMSE train: 0.538438\tval: 0.577447\n",
      "MAE train: 0.423329\tval: 0.454580\n",
      "pearson_R train: 0.633826\tval: 0.540320\n",
      "Epoch: 182\n",
      "Loss: 0.6919531557295058\n",
      "RMSE train: 0.548523\tval: 0.597656\n",
      "MAE train: 0.445175\tval: 0.481928\n",
      "pearson_R train: 0.630434\tval: 0.536345\n",
      "Epoch: 183\n",
      "Loss: 0.7016473280058967\n",
      "RMSE train: 0.568713\tval: 0.615569\n",
      "MAE train: 0.462384\tval: 0.501768\n",
      "pearson_R train: 0.621309\tval: 0.540120\n",
      "Epoch: 184\n",
      "Loss: 0.6916137867503696\n",
      "RMSE train: 0.545566\tval: 0.591735\n",
      "MAE train: 0.439309\tval: 0.479527\n",
      "pearson_R train: 0.627816\tval: 0.533122\n",
      "Epoch: 185\n",
      "Loss: 0.702682806385888\n",
      "RMSE train: 0.562948\tval: 0.588634\n",
      "MAE train: 0.434228\tval: 0.455017\n",
      "pearson_R train: 0.610868\tval: 0.530624\n",
      "Epoch: 186\n",
      "Loss: 0.7061045368512472\n",
      "RMSE train: 0.550639\tval: 0.584359\n",
      "MAE train: 0.425659\tval: 0.455313\n",
      "pearson_R train: 0.626492\tval: 0.530646\n",
      "Epoch: 187\n",
      "Loss: 0.6978189216719733\n",
      "RMSE train: 0.549213\tval: 0.599002\n",
      "MAE train: 0.437178\tval: 0.475102\n",
      "pearson_R train: 0.620457\tval: 0.526589\n",
      "Epoch: 188\n",
      "Loss: 0.6800817714797126\n",
      "RMSE train: 0.551141\tval: 0.601654\n",
      "MAE train: 0.444602\tval: 0.483352\n",
      "pearson_R train: 0.624796\tval: 0.530031\n",
      "Epoch: 189\n",
      "Loss: 0.6938110060162015\n",
      "RMSE train: 0.551128\tval: 0.589581\n",
      "MAE train: 0.440921\tval: 0.470071\n",
      "pearson_R train: 0.613293\tval: 0.528098\n",
      "Epoch: 190\n",
      "Loss: 0.7068637940618727\n",
      "RMSE train: 0.622556\tval: 0.683751\n",
      "MAE train: 0.512552\tval: 0.552153\n",
      "pearson_R train: 0.615242\tval: 0.530647\n",
      "Epoch: 191\n",
      "Loss: 0.6928389933374193\n",
      "RMSE train: 0.550373\tval: 0.597372\n",
      "MAE train: 0.445803\tval: 0.481641\n",
      "pearson_R train: 0.629147\tval: 0.540476\n",
      "Epoch: 192\n",
      "Loss: 0.6890944441159567\n",
      "RMSE train: 0.558125\tval: 0.605529\n",
      "MAE train: 0.455571\tval: 0.496131\n",
      "pearson_R train: 0.631296\tval: 0.537456\n",
      "Epoch: 193\n",
      "Loss: 0.7029924525154961\n",
      "RMSE train: 0.550793\tval: 0.572805\n",
      "MAE train: 0.426432\tval: 0.448244\n",
      "pearson_R train: 0.616305\tval: 0.538044\n",
      "Epoch: 194\n",
      "Loss: 0.703131635983785\n",
      "RMSE train: 0.585317\tval: 0.592203\n",
      "MAE train: 0.440245\tval: 0.438893\n",
      "pearson_R train: 0.627034\tval: 0.547566\n",
      "Epoch: 195\n",
      "Loss: 0.7064612838957045\n",
      "RMSE train: 0.550935\tval: 0.596732\n",
      "MAE train: 0.447842\tval: 0.485747\n",
      "pearson_R train: 0.634171\tval: 0.546542\n",
      "Epoch: 196\n",
      "Loss: 0.6877720687124464\n",
      "RMSE train: 0.561350\tval: 0.614718\n",
      "MAE train: 0.456719\tval: 0.499522\n",
      "pearson_R train: 0.627723\tval: 0.530553\n",
      "Epoch: 197\n",
      "Loss: 0.7134162982304891\n",
      "RMSE train: 0.566899\tval: 0.633907\n",
      "MAE train: 0.461443\tval: 0.512971\n",
      "pearson_R train: 0.633352\tval: 0.527099\n",
      "Epoch: 198\n",
      "Loss: 0.7069025105900235\n",
      "RMSE train: 0.686740\tval: 0.751723\n",
      "MAE train: 0.573533\tval: 0.629822\n",
      "pearson_R train: 0.610233\tval: 0.507961\n",
      "Epoch: 199\n",
      "Loss: 0.6754816704326205\n",
      "RMSE train: 0.598728\tval: 0.667544\n",
      "MAE train: 0.492750\tval: 0.550485\n",
      "pearson_R train: 0.635471\tval: 0.528574\n",
      "Epoch: 200\n",
      "Loss: 0.693819112247891\n",
      "RMSE train: 0.600563\tval: 0.613413\n",
      "MAE train: 0.454357\tval: 0.460311\n",
      "pearson_R train: 0.625302\tval: 0.532950\n",
      "Epoch: 201\n",
      "Loss: 0.7004225651423136\n",
      "RMSE train: 0.540280\tval: 0.572735\n",
      "MAE train: 0.421252\tval: 0.444466\n",
      "pearson_R train: 0.634702\tval: 0.531558\n",
      "Epoch: 202\n",
      "Loss: 0.7093667056825426\n",
      "RMSE train: 0.539281\tval: 0.582689\n",
      "MAE train: 0.432649\tval: 0.466736\n",
      "pearson_R train: 0.631229\tval: 0.529630\n",
      "Epoch: 203\n",
      "Loss: 0.6744532320234511\n",
      "RMSE train: 0.534977\tval: 0.579948\n",
      "MAE train: 0.423051\tval: 0.459934\n",
      "pearson_R train: 0.636071\tval: 0.531497\n",
      "Epoch: 204\n",
      "Loss: 0.692462166150411\n",
      "RMSE train: 0.535976\tval: 0.576727\n",
      "MAE train: 0.415690\tval: 0.444535\n",
      "pearson_R train: 0.645601\tval: 0.533955\n",
      "Epoch: 205\n",
      "Loss: 0.7059747510486178\n",
      "RMSE train: 0.532008\tval: 0.567042\n",
      "MAE train: 0.416176\tval: 0.440872\n",
      "pearson_R train: 0.643821\tval: 0.543656\n",
      "Epoch: 206\n",
      "Loss: 0.6593009233474731\n",
      "RMSE train: 0.540766\tval: 0.593197\n",
      "MAE train: 0.434876\tval: 0.474158\n",
      "pearson_R train: 0.641191\tval: 0.547326\n",
      "Epoch: 207\n",
      "Loss: 0.6751077241367764\n",
      "RMSE train: 0.651698\tval: 0.726867\n",
      "MAE train: 0.543810\tval: 0.606204\n",
      "pearson_R train: 0.625407\tval: 0.514310\n",
      "Epoch: 208\n",
      "Loss: 0.6994177963998582\n",
      "RMSE train: 0.544255\tval: 0.589040\n",
      "MAE train: 0.442522\tval: 0.482794\n",
      "pearson_R train: 0.635469\tval: 0.527531\n",
      "Epoch: 209\n",
      "Loss: 0.6931331356366476\n",
      "RMSE train: 0.549613\tval: 0.577218\n",
      "MAE train: 0.419545\tval: 0.435491\n",
      "pearson_R train: 0.638055\tval: 0.542062\n",
      "Epoch: 210\n",
      "Loss: 0.6741857859823439\n",
      "RMSE train: 0.648892\tval: 0.653367\n",
      "MAE train: 0.489635\tval: 0.485358\n",
      "pearson_R train: 0.635120\tval: 0.538655\n",
      "Epoch: 211\n",
      "Loss: 0.6974280873934428\n",
      "RMSE train: 0.530459\tval: 0.574017\n",
      "MAE train: 0.417250\tval: 0.451267\n",
      "pearson_R train: 0.644579\tval: 0.539595\n",
      "Epoch: 212\n",
      "Loss: 0.668724901146359\n",
      "RMSE train: 0.663132\tval: 0.722423\n",
      "MAE train: 0.548793\tval: 0.594239\n",
      "pearson_R train: 0.604120\tval: 0.516192\n",
      "Epoch: 213\n",
      "Loss: 0.6570364832878113\n",
      "RMSE train: 0.664035\tval: 0.749315\n",
      "MAE train: 0.546530\tval: 0.615468\n",
      "pearson_R train: 0.619587\tval: 0.516714\n",
      "Epoch: 214\n",
      "Loss: 0.6812751558091905\n",
      "RMSE train: 0.547428\tval: 0.599555\n",
      "MAE train: 0.442188\tval: 0.486007\n",
      "pearson_R train: 0.630113\tval: 0.529356\n",
      "Epoch: 215\n",
      "Loss: 0.6861372921201918\n",
      "RMSE train: 0.531227\tval: 0.579633\n",
      "MAE train: 0.425006\tval: 0.460737\n",
      "pearson_R train: 0.643461\tval: 0.536932\n",
      "Epoch: 216\n",
      "Loss: 0.6760066482755873\n",
      "RMSE train: 0.537438\tval: 0.560978\n",
      "MAE train: 0.415116\tval: 0.427174\n",
      "pearson_R train: 0.656293\tval: 0.560819\n",
      "Epoch: 217\n",
      "Loss: 0.6695828040440878\n",
      "RMSE train: 0.528942\tval: 0.572221\n",
      "MAE train: 0.419723\tval: 0.457046\n",
      "pearson_R train: 0.646865\tval: 0.547836\n",
      "Epoch: 218\n",
      "Loss: 0.6741945412423875\n",
      "RMSE train: 0.533562\tval: 0.569034\n",
      "MAE train: 0.419051\tval: 0.443222\n",
      "pearson_R train: 0.647430\tval: 0.552290\n",
      "Epoch: 219\n",
      "Loss: 0.6806312799453735\n",
      "RMSE train: 0.538196\tval: 0.572370\n",
      "MAE train: 0.416975\tval: 0.437869\n",
      "pearson_R train: 0.647960\tval: 0.541200\n",
      "Epoch: 220\n",
      "Loss: 0.6747123135460747\n",
      "RMSE train: 0.520503\tval: 0.569084\n",
      "MAE train: 0.412844\tval: 0.450321\n",
      "pearson_R train: 0.660483\tval: 0.551748\n",
      "Epoch: 221\n",
      "Loss: 0.6753486527336968\n",
      "RMSE train: 0.523705\tval: 0.558166\n",
      "MAE train: 0.406829\tval: 0.432436\n",
      "pearson_R train: 0.662548\tval: 0.562671\n",
      "Epoch: 222\n",
      "Loss: 0.6603595614433289\n",
      "RMSE train: 0.524610\tval: 0.557587\n",
      "MAE train: 0.407014\tval: 0.425821\n",
      "pearson_R train: 0.662269\tval: 0.562661\n",
      "Epoch: 223\n",
      "Loss: 0.7146235571967231\n",
      "RMSE train: 0.529445\tval: 0.568750\n",
      "MAE train: 0.406836\tval: 0.433676\n",
      "pearson_R train: 0.660610\tval: 0.547187\n",
      "Epoch: 224\n",
      "Loss: 0.6522391041119894\n",
      "RMSE train: 0.538239\tval: 0.593680\n",
      "MAE train: 0.431053\tval: 0.475360\n",
      "pearson_R train: 0.635590\tval: 0.515719\n",
      "Epoch: 225\n",
      "Loss: 0.6553005907270644\n",
      "RMSE train: 0.530415\tval: 0.580805\n",
      "MAE train: 0.425956\tval: 0.464785\n",
      "pearson_R train: 0.650803\tval: 0.542297\n",
      "Epoch: 226\n",
      "Loss: 0.660491234726376\n",
      "RMSE train: 0.520189\tval: 0.572665\n",
      "MAE train: 0.411364\tval: 0.452397\n",
      "pearson_R train: 0.660856\tval: 0.544574\n",
      "Epoch: 227\n",
      "Loss: 0.6577755080329047\n",
      "RMSE train: 0.562563\tval: 0.605751\n",
      "MAE train: 0.434509\tval: 0.460676\n",
      "pearson_R train: 0.633898\tval: 0.525804\n",
      "Epoch: 228\n",
      "Loss: 0.6564162174860636\n",
      "RMSE train: 0.518022\tval: 0.566842\n",
      "MAE train: 0.407543\tval: 0.445206\n",
      "pearson_R train: 0.669824\tval: 0.546932\n",
      "Epoch: 229\n",
      "Loss: 0.6699348621898227\n",
      "RMSE train: 0.530442\tval: 0.566833\n",
      "MAE train: 0.410031\tval: 0.437273\n",
      "pearson_R train: 0.655118\tval: 0.550281\n",
      "Epoch: 230\n",
      "Loss: 0.6823755966292487\n",
      "RMSE train: 0.531398\tval: 0.571426\n",
      "MAE train: 0.410121\tval: 0.436089\n",
      "pearson_R train: 0.658711\tval: 0.556320\n",
      "Epoch: 231\n",
      "Loss: 0.6716023352411058\n",
      "RMSE train: 0.527522\tval: 0.586651\n",
      "MAE train: 0.423766\tval: 0.471912\n",
      "pearson_R train: 0.666879\tval: 0.560239\n",
      "Epoch: 232\n",
      "Loss: 0.6735627849896749\n",
      "RMSE train: 0.533902\tval: 0.598107\n",
      "MAE train: 0.431729\tval: 0.479949\n",
      "pearson_R train: 0.663647\tval: 0.558500\n",
      "Epoch: 233\n",
      "Loss: 0.6724613573816087\n",
      "RMSE train: 0.553017\tval: 0.624455\n",
      "MAE train: 0.449276\tval: 0.508621\n",
      "pearson_R train: 0.649932\tval: 0.536266\n",
      "Epoch: 234\n",
      "Loss: 0.6484986676110162\n",
      "RMSE train: 0.534313\tval: 0.561718\n",
      "MAE train: 0.412875\tval: 0.428790\n",
      "pearson_R train: 0.649104\tval: 0.562023\n",
      "Epoch: 235\n",
      "Loss: 0.6609372761514452\n",
      "RMSE train: 0.517561\tval: 0.580964\n",
      "MAE train: 0.411776\tval: 0.457953\n",
      "pearson_R train: 0.666337\tval: 0.540804\n",
      "Epoch: 236\n",
      "Loss: 0.6457888417773776\n",
      "RMSE train: 0.523589\tval: 0.577806\n",
      "MAE train: 0.412454\tval: 0.453139\n",
      "pearson_R train: 0.656139\tval: 0.534986\n",
      "Epoch: 237\n",
      "Loss: 0.6668155855602689\n",
      "RMSE train: 0.521865\tval: 0.562313\n",
      "MAE train: 0.406556\tval: 0.428707\n",
      "pearson_R train: 0.667478\tval: 0.556354\n",
      "Epoch: 238\n",
      "Loss: 0.6572422318988376\n",
      "RMSE train: 0.516025\tval: 0.583351\n",
      "MAE train: 0.408549\tval: 0.458681\n",
      "pearson_R train: 0.667638\tval: 0.526359\n",
      "Epoch: 239\n",
      "Loss: 0.6635744637913175\n",
      "RMSE train: 0.521176\tval: 0.580991\n",
      "MAE train: 0.410457\tval: 0.452615\n",
      "pearson_R train: 0.660610\tval: 0.546354\n",
      "Epoch: 240\n",
      "Loss: 0.634824812412262\n",
      "RMSE train: 0.539769\tval: 0.622383\n",
      "MAE train: 0.441003\tval: 0.506188\n",
      "pearson_R train: 0.668777\tval: 0.530367\n",
      "Epoch: 241\n",
      "Loss: 0.6423593163490295\n",
      "RMSE train: 0.613998\tval: 0.695130\n",
      "MAE train: 0.507532\tval: 0.574353\n",
      "pearson_R train: 0.650536\tval: 0.520266\n",
      "Epoch: 242\n",
      "Loss: 0.6519181198543973\n",
      "RMSE train: 0.561401\tval: 0.597561\n",
      "MAE train: 0.426699\tval: 0.443975\n",
      "pearson_R train: 0.641901\tval: 0.523747\n",
      "Epoch: 243\n",
      "Loss: 0.656729347176022\n",
      "RMSE train: 0.558503\tval: 0.589008\n",
      "MAE train: 0.424460\tval: 0.432108\n",
      "pearson_R train: 0.674353\tval: 0.558093\n",
      "Epoch: 244\n",
      "Loss: 0.6251655154758029\n",
      "RMSE train: 0.529420\tval: 0.596987\n",
      "MAE train: 0.425513\tval: 0.482266\n",
      "pearson_R train: 0.662375\tval: 0.551312\n",
      "Epoch: 245\n",
      "Loss: 0.6670566466119554\n",
      "RMSE train: 0.520925\tval: 0.575782\n",
      "MAE train: 0.418830\tval: 0.463275\n",
      "pearson_R train: 0.664087\tval: 0.549233\n",
      "Epoch: 246\n",
      "Loss: 0.6519796715842353\n",
      "RMSE train: 0.528252\tval: 0.585209\n",
      "MAE train: 0.409623\tval: 0.445661\n",
      "pearson_R train: 0.654500\tval: 0.537037\n",
      "Epoch: 247\n",
      "Loss: 0.6785561508602567\n",
      "RMSE train: 0.525319\tval: 0.581366\n",
      "MAE train: 0.424052\tval: 0.468479\n",
      "pearson_R train: 0.662394\tval: 0.549952\n",
      "Epoch: 248\n",
      "Loss: 0.656470795472463\n",
      "RMSE train: 0.513270\tval: 0.566535\n",
      "MAE train: 0.406220\tval: 0.443092\n",
      "pearson_R train: 0.672216\tval: 0.566454\n",
      "Epoch: 249\n",
      "Loss: 0.6730330122841729\n",
      "RMSE train: 0.514063\tval: 0.554608\n",
      "MAE train: 0.399502\tval: 0.420226\n",
      "pearson_R train: 0.680988\tval: 0.573759\n",
      "Epoch: 250\n",
      "Loss: 0.6477492252985636\n",
      "RMSE train: 0.515942\tval: 0.580276\n",
      "MAE train: 0.402583\tval: 0.446194\n",
      "pearson_R train: 0.673728\tval: 0.544493\n",
      "Epoch: 251\n",
      "Loss: 0.6622234384218851\n",
      "RMSE train: 0.524663\tval: 0.601375\n",
      "MAE train: 0.421854\tval: 0.475342\n",
      "pearson_R train: 0.675314\tval: 0.556204\n",
      "Epoch: 252\n",
      "Loss: 0.6352163420783149\n",
      "RMSE train: 0.521535\tval: 0.586387\n",
      "MAE train: 0.420768\tval: 0.470730\n",
      "pearson_R train: 0.671108\tval: 0.554122\n",
      "Epoch: 253\n",
      "Loss: 0.6545738048023648\n",
      "RMSE train: 0.503153\tval: 0.560555\n",
      "MAE train: 0.399238\tval: 0.442837\n",
      "pearson_R train: 0.688522\tval: 0.565613\n",
      "Epoch: 254\n",
      "Loss: 0.6634695927302042\n",
      "RMSE train: 0.516954\tval: 0.574270\n",
      "MAE train: 0.397160\tval: 0.445226\n",
      "pearson_R train: 0.676283\tval: 0.538308\n",
      "Epoch: 255\n",
      "Loss: 0.6292604406674703\n",
      "RMSE train: 0.509250\tval: 0.570694\n",
      "MAE train: 0.404987\tval: 0.448118\n",
      "pearson_R train: 0.680562\tval: 0.569915\n",
      "Epoch: 256\n",
      "Loss: 0.622803356912401\n",
      "RMSE train: 0.545352\tval: 0.580682\n",
      "MAE train: 0.415778\tval: 0.432802\n",
      "pearson_R train: 0.681006\tval: 0.566111\n",
      "Epoch: 257\n",
      "Loss: 0.6609828472137451\n",
      "RMSE train: 0.522485\tval: 0.586886\n",
      "MAE train: 0.405198\tval: 0.449050\n",
      "pearson_R train: 0.661180\tval: 0.527627\n",
      "Epoch: 258\n",
      "Loss: 0.6404497159851922\n",
      "RMSE train: 0.515763\tval: 0.593859\n",
      "MAE train: 0.418332\tval: 0.474250\n",
      "pearson_R train: 0.683084\tval: 0.545448\n",
      "Epoch: 259\n",
      "Loss: 0.6227653192149268\n",
      "RMSE train: 0.514198\tval: 0.573071\n",
      "MAE train: 0.408873\tval: 0.455599\n",
      "pearson_R train: 0.673122\tval: 0.552242\n",
      "Epoch: 260\n",
      "Loss: 0.6500528256098429\n",
      "RMSE train: 0.511829\tval: 0.571900\n",
      "MAE train: 0.402319\tval: 0.443901\n",
      "pearson_R train: 0.675745\tval: 0.557373\n",
      "Epoch: 261\n",
      "Loss: 0.626932852798038\n",
      "RMSE train: 0.514544\tval: 0.586976\n",
      "MAE train: 0.415248\tval: 0.471625\n",
      "pearson_R train: 0.685342\tval: 0.559851\n",
      "Epoch: 262\n",
      "Loss: 0.6625720659891764\n",
      "RMSE train: 0.538650\tval: 0.575308\n",
      "MAE train: 0.407049\tval: 0.425744\n",
      "pearson_R train: 0.679639\tval: 0.558255\n",
      "Epoch: 263\n",
      "Loss: 0.6430649161338806\n",
      "RMSE train: 0.509043\tval: 0.590416\n",
      "MAE train: 0.410133\tval: 0.475086\n",
      "pearson_R train: 0.689496\tval: 0.541714\n",
      "Epoch: 264\n",
      "Loss: 0.6214431391821967\n",
      "RMSE train: 0.627672\tval: 0.727920\n",
      "MAE train: 0.519183\tval: 0.594100\n",
      "pearson_R train: 0.670578\tval: 0.510540\n",
      "Epoch: 265\n",
      "Loss: 0.6098291476567587\n",
      "RMSE train: 0.505731\tval: 0.573226\n",
      "MAE train: 0.397363\tval: 0.443485\n",
      "pearson_R train: 0.683842\tval: 0.552035\n",
      "Epoch: 266\n",
      "Loss: 0.635954863495297\n",
      "RMSE train: 0.518548\tval: 0.572981\n",
      "MAE train: 0.400249\tval: 0.439908\n",
      "pearson_R train: 0.672484\tval: 0.549438\n",
      "Epoch: 267\n",
      "Loss: 0.6287472512986925\n",
      "RMSE train: 0.514122\tval: 0.561015\n",
      "MAE train: 0.395936\tval: 0.421605\n",
      "pearson_R train: 0.690793\tval: 0.571644\n",
      "Epoch: 268\n",
      "Loss: 0.6450598372353448\n",
      "RMSE train: 0.565989\tval: 0.600468\n",
      "MAE train: 0.425795\tval: 0.444694\n",
      "pearson_R train: 0.669011\tval: 0.548596\n",
      "Epoch: 269\n",
      "Loss: 0.637050211429596\n",
      "RMSE train: 0.517781\tval: 0.579480\n",
      "MAE train: 0.403619\tval: 0.450198\n",
      "pearson_R train: 0.668369\tval: 0.541470\n",
      "Epoch: 270\n",
      "Loss: 0.6297860211796231\n",
      "RMSE train: 0.512104\tval: 0.596527\n",
      "MAE train: 0.402448\tval: 0.460549\n",
      "pearson_R train: 0.680159\tval: 0.542308\n",
      "Epoch: 271\n",
      "Loss: 0.6170381572511461\n",
      "RMSE train: 0.520092\tval: 0.570029\n",
      "MAE train: 0.405412\tval: 0.438171\n",
      "pearson_R train: 0.664084\tval: 0.559111\n",
      "Epoch: 272\n",
      "Loss: 0.6096980373064677\n",
      "RMSE train: 0.509547\tval: 0.576284\n",
      "MAE train: 0.399096\tval: 0.441792\n",
      "pearson_R train: 0.680117\tval: 0.552697\n",
      "Epoch: 273\n",
      "Loss: 0.629064699014028\n",
      "RMSE train: 0.516557\tval: 0.559322\n",
      "MAE train: 0.393669\tval: 0.423381\n",
      "pearson_R train: 0.684555\tval: 0.569442\n",
      "Epoch: 274\n",
      "Loss: 0.6249803768263923\n",
      "RMSE train: 0.502891\tval: 0.568658\n",
      "MAE train: 0.393437\tval: 0.439109\n",
      "pearson_R train: 0.690697\tval: 0.557466\n",
      "Epoch: 275\n",
      "Loss: 0.6092597246170044\n",
      "RMSE train: 0.493162\tval: 0.562478\n",
      "MAE train: 0.391006\tval: 0.443943\n",
      "pearson_R train: 0.703897\tval: 0.560991\n",
      "Epoch: 276\n",
      "Loss: 0.6187944677140977\n",
      "RMSE train: 0.497478\tval: 0.571210\n",
      "MAE train: 0.400578\tval: 0.455363\n",
      "pearson_R train: 0.705854\tval: 0.569283\n",
      "Epoch: 277\n",
      "Loss: 0.6135116418202718\n",
      "RMSE train: 0.505396\tval: 0.558453\n",
      "MAE train: 0.392971\tval: 0.437046\n",
      "pearson_R train: 0.689790\tval: 0.556951\n",
      "Epoch: 278\n",
      "Loss: 0.6246374713049995\n",
      "RMSE train: 0.523675\tval: 0.575561\n",
      "MAE train: 0.398417\tval: 0.426195\n",
      "pearson_R train: 0.685341\tval: 0.543907\n",
      "Epoch: 279\n",
      "Loss: 0.6307114760080973\n",
      "RMSE train: 0.511010\tval: 0.576507\n",
      "MAE train: 0.393042\tval: 0.437939\n",
      "pearson_R train: 0.687043\tval: 0.547081\n",
      "Epoch: 280\n",
      "Loss: 0.6035320361455282\n",
      "RMSE train: 0.490383\tval: 0.555631\n",
      "MAE train: 0.385526\tval: 0.429482\n",
      "pearson_R train: 0.707350\tval: 0.583638\n",
      "Epoch: 281\n",
      "Loss: 0.6139308147960238\n",
      "RMSE train: 0.524052\tval: 0.567758\n",
      "MAE train: 0.401776\tval: 0.425455\n",
      "pearson_R train: 0.680188\tval: 0.570848\n",
      "Epoch: 282\n",
      "Loss: 0.6203146245744493\n",
      "RMSE train: 0.511779\tval: 0.579431\n",
      "MAE train: 0.410838\tval: 0.469147\n",
      "pearson_R train: 0.676688\tval: 0.535087\n",
      "Epoch: 283\n",
      "Loss: 0.6395437253846062\n",
      "RMSE train: 0.495930\tval: 0.560578\n",
      "MAE train: 0.389600\tval: 0.436462\n",
      "pearson_R train: 0.699567\tval: 0.572199\n",
      "Epoch: 284\n",
      "Loss: 0.6173640224668715\n",
      "RMSE train: 0.509550\tval: 0.584082\n",
      "MAE train: 0.406828\tval: 0.466764\n",
      "pearson_R train: 0.679942\tval: 0.527714\n",
      "Epoch: 285\n",
      "Loss: 0.603389196925693\n",
      "RMSE train: 0.509555\tval: 0.562468\n",
      "MAE train: 0.394268\tval: 0.424941\n",
      "pearson_R train: 0.693101\tval: 0.563993\n",
      "Epoch: 286\n",
      "Loss: 0.6146604220072428\n",
      "RMSE train: 0.509682\tval: 0.570998\n",
      "MAE train: 0.391626\tval: 0.429796\n",
      "pearson_R train: 0.692898\tval: 0.545038\n",
      "Epoch: 287\n",
      "Loss: 0.6061567399236891\n",
      "RMSE train: 0.508136\tval: 0.601091\n",
      "MAE train: 0.406114\tval: 0.469122\n",
      "pearson_R train: 0.690119\tval: 0.547298\n",
      "Epoch: 288\n",
      "Loss: 0.5898149609565735\n",
      "RMSE train: 0.508476\tval: 0.564328\n",
      "MAE train: 0.392834\tval: 0.428343\n",
      "pearson_R train: 0.684988\tval: 0.572277\n",
      "Epoch: 289\n",
      "Loss: 0.6343533595403036\n",
      "RMSE train: 0.501135\tval: 0.569692\n",
      "MAE train: 0.395549\tval: 0.449686\n",
      "pearson_R train: 0.690880\tval: 0.553032\n",
      "Epoch: 290\n",
      "Loss: 0.614351855383979\n",
      "RMSE train: 0.536325\tval: 0.620929\n",
      "MAE train: 0.436270\tval: 0.502621\n",
      "pearson_R train: 0.665338\tval: 0.521755\n",
      "Epoch: 291\n",
      "Loss: 0.6558102435535855\n",
      "RMSE train: 0.540912\tval: 0.634269\n",
      "MAE train: 0.443882\tval: 0.516672\n",
      "pearson_R train: 0.691781\tval: 0.536040\n",
      "Epoch: 292\n",
      "Loss: 0.5998354819085863\n",
      "RMSE train: 0.549442\tval: 0.661776\n",
      "MAE train: 0.443672\tval: 0.530975\n",
      "pearson_R train: 0.694180\tval: 0.539720\n",
      "Epoch: 293\n",
      "Loss: 0.6046298940976461\n",
      "RMSE train: 0.543046\tval: 0.599523\n",
      "MAE train: 0.412404\tval: 0.445082\n",
      "pearson_R train: 0.668586\tval: 0.537810\n",
      "Epoch: 294\n",
      "Loss: 0.627448234293196\n",
      "RMSE train: 0.495307\tval: 0.567102\n",
      "MAE train: 0.393742\tval: 0.442854\n",
      "pearson_R train: 0.701503\tval: 0.559386\n",
      "Epoch: 295\n",
      "Loss: 0.6106997927029928\n",
      "RMSE train: 0.508972\tval: 0.603968\n",
      "MAE train: 0.410523\tval: 0.480997\n",
      "pearson_R train: 0.704997\tval: 0.563730\n",
      "Epoch: 296\n",
      "Loss: 0.5894462135103014\n",
      "RMSE train: 0.518872\tval: 0.575723\n",
      "MAE train: 0.393819\tval: 0.425314\n",
      "pearson_R train: 0.697662\tval: 0.557578\n",
      "Epoch: 297\n",
      "Loss: 0.6187117033534579\n",
      "RMSE train: 0.492277\tval: 0.574624\n",
      "MAE train: 0.391327\tval: 0.454972\n",
      "pearson_R train: 0.705636\tval: 0.549227\n",
      "Epoch: 298\n",
      "Loss: 0.6086186104350619\n",
      "RMSE train: 0.544215\tval: 0.596206\n",
      "MAE train: 0.414541\tval: 0.450085\n",
      "pearson_R train: 0.674537\tval: 0.547945\n",
      "Epoch: 299\n",
      "Loss: 0.5903386076291403\n",
      "RMSE train: 0.507926\tval: 0.585559\n",
      "MAE train: 0.398459\tval: 0.455987\n",
      "pearson_R train: 0.683927\tval: 0.527448\n",
      "Epoch: 300\n",
      "Loss: 0.6197360157966614\n",
      "RMSE train: 0.491486\tval: 0.565293\n",
      "MAE train: 0.379500\tval: 0.429921\n",
      "pearson_R train: 0.715737\tval: 0.555570\n",
      "Epoch: 301\n",
      "Loss: 0.5900832414627075\n",
      "RMSE train: 0.519216\tval: 0.576579\n",
      "MAE train: 0.395400\tval: 0.429461\n",
      "pearson_R train: 0.707970\tval: 0.560917\n",
      "Epoch: 302\n",
      "Loss: 0.6078678634431627\n",
      "RMSE train: 0.503898\tval: 0.592485\n",
      "MAE train: 0.405006\tval: 0.474272\n",
      "pearson_R train: 0.699552\tval: 0.548807\n",
      "Epoch: 303\n",
      "Loss: 0.576345788107978\n",
      "RMSE train: 0.491738\tval: 0.556440\n",
      "MAE train: 0.382174\tval: 0.421471\n",
      "pearson_R train: 0.707676\tval: 0.586750\n",
      "Epoch: 304\n",
      "Loss: 0.600449906455146\n",
      "RMSE train: 0.505178\tval: 0.581305\n",
      "MAE train: 0.396929\tval: 0.448021\n",
      "pearson_R train: 0.690853\tval: 0.568841\n",
      "Epoch: 305\n",
      "Loss: 0.6049863563643562\n",
      "RMSE train: 0.505712\tval: 0.589983\n",
      "MAE train: 0.405631\tval: 0.470136\n",
      "pearson_R train: 0.700674\tval: 0.557142\n",
      "Epoch: 306\n",
      "Loss: 0.6044386029243469\n",
      "RMSE train: 0.527248\tval: 0.612692\n",
      "MAE train: 0.418325\tval: 0.481403\n",
      "pearson_R train: 0.693521\tval: 0.566044\n",
      "Epoch: 307\n",
      "Loss: 0.6162885891066657\n",
      "RMSE train: 0.500273\tval: 0.604621\n",
      "MAE train: 0.397215\tval: 0.475282\n",
      "pearson_R train: 0.702895\tval: 0.534263\n",
      "Epoch: 308\n",
      "Loss: 0.6033165454864502\n",
      "RMSE train: 0.498439\tval: 0.586052\n",
      "MAE train: 0.397135\tval: 0.459713\n",
      "pearson_R train: 0.701634\tval: 0.546924\n",
      "Epoch: 309\n",
      "Loss: 0.6110690434773763\n",
      "RMSE train: 0.623652\tval: 0.729139\n",
      "MAE train: 0.509191\tval: 0.587785\n",
      "pearson_R train: 0.671242\tval: 0.522687\n",
      "Epoch: 310\n",
      "Loss: 0.6188729537857903\n",
      "RMSE train: 0.577902\tval: 0.598327\n",
      "MAE train: 0.438253\tval: 0.448039\n",
      "pearson_R train: 0.638838\tval: 0.549025\n",
      "Epoch: 311\n",
      "Loss: 0.5916595922576057\n",
      "RMSE train: 0.492781\tval: 0.565991\n",
      "MAE train: 0.386384\tval: 0.443030\n",
      "pearson_R train: 0.703709\tval: 0.561252\n",
      "Epoch: 312\n",
      "Loss: 0.5935590200954013\n",
      "RMSE train: 0.497302\tval: 0.567416\n",
      "MAE train: 0.386613\tval: 0.429642\n",
      "pearson_R train: 0.698073\tval: 0.561654\n",
      "Epoch: 313\n",
      "Loss: 0.6022049850887723\n",
      "RMSE train: 0.528437\tval: 0.580775\n",
      "MAE train: 0.400230\tval: 0.436837\n",
      "pearson_R train: 0.675270\tval: 0.539074\n",
      "Epoch: 314\n",
      "Loss: 0.5916531748241849\n",
      "RMSE train: 0.518383\tval: 0.573889\n",
      "MAE train: 0.393798\tval: 0.426784\n",
      "pearson_R train: 0.705621\tval: 0.558911\n",
      "Epoch: 315\n",
      "Loss: 0.5835105379422506\n",
      "RMSE train: 0.498049\tval: 0.587663\n",
      "MAE train: 0.393390\tval: 0.459724\n",
      "pearson_R train: 0.702144\tval: 0.565647\n",
      "Epoch: 316\n",
      "Loss: 0.6125081976254781\n",
      "RMSE train: 0.550862\tval: 0.635466\n",
      "MAE train: 0.445387\tval: 0.509805\n",
      "pearson_R train: 0.688418\tval: 0.547001\n",
      "Epoch: 317\n",
      "Loss: 0.6037878923945956\n",
      "RMSE train: 0.534787\tval: 0.599331\n",
      "MAE train: 0.413357\tval: 0.455107\n",
      "pearson_R train: 0.647390\tval: 0.526662\n",
      "Epoch: 318\n",
      "Loss: 0.5882199340396457\n",
      "RMSE train: 0.481332\tval: 0.562663\n",
      "MAE train: 0.375143\tval: 0.430906\n",
      "pearson_R train: 0.720829\tval: 0.563885\n",
      "Epoch: 319\n",
      "Loss: 0.5630769795841641\n",
      "RMSE train: 0.514254\tval: 0.582120\n",
      "MAE train: 0.388953\tval: 0.438463\n",
      "pearson_R train: 0.698926\tval: 0.554756\n",
      "Epoch: 320\n",
      "Loss: 0.569622771607505\n",
      "RMSE train: 0.559802\tval: 0.649998\n",
      "MAE train: 0.451818\tval: 0.516056\n",
      "pearson_R train: 0.678438\tval: 0.539062\n",
      "Epoch: 321\n",
      "Loss: 0.586219396856096\n",
      "RMSE train: 0.514749\tval: 0.583071\n",
      "MAE train: 0.396589\tval: 0.440009\n",
      "pearson_R train: 0.688018\tval: 0.538021\n",
      "Epoch: 322\n",
      "Loss: 0.5747784442371793\n",
      "RMSE train: 0.506052\tval: 0.565161\n",
      "MAE train: 0.387526\tval: 0.426393\n",
      "pearson_R train: 0.715458\tval: 0.580785\n",
      "Epoch: 323\n",
      "Loss: 0.5790934496455722\n",
      "RMSE train: 0.504707\tval: 0.601123\n",
      "MAE train: 0.396050\tval: 0.463711\n",
      "pearson_R train: 0.691107\tval: 0.533862\n",
      "Epoch: 324\n",
      "Loss: 0.5739872488710616\n",
      "RMSE train: 0.502678\tval: 0.575844\n",
      "MAE train: 0.385738\tval: 0.431620\n",
      "pearson_R train: 0.715479\tval: 0.566215\n",
      "Epoch: 325\n",
      "Loss: 0.6083616150750054\n",
      "RMSE train: 0.535663\tval: 0.650766\n",
      "MAE train: 0.436120\tval: 0.525147\n",
      "pearson_R train: 0.715659\tval: 0.547661\n",
      "Epoch: 326\n",
      "Loss: 0.5825655725267198\n",
      "RMSE train: 0.522751\tval: 0.609200\n",
      "MAE train: 0.424647\tval: 0.493074\n",
      "pearson_R train: 0.696898\tval: 0.549399\n",
      "Epoch: 327\n",
      "Loss: 0.569922450515959\n",
      "RMSE train: 0.496444\tval: 0.561534\n",
      "MAE train: 0.378253\tval: 0.421938\n",
      "pearson_R train: 0.719842\tval: 0.577802\n",
      "Epoch: 328\n",
      "Loss: 0.6060201525688171\n",
      "RMSE train: 0.533131\tval: 0.586586\n",
      "MAE train: 0.406771\tval: 0.439179\n",
      "pearson_R train: 0.683104\tval: 0.554754\n",
      "Epoch: 329\n",
      "Loss: 0.6050211389859518\n",
      "RMSE train: 0.509035\tval: 0.603747\n",
      "MAE train: 0.415160\tval: 0.492144\n",
      "pearson_R train: 0.718640\tval: 0.560928\n",
      "Epoch: 330\n",
      "Loss: 0.590510778956943\n",
      "RMSE train: 0.548495\tval: 0.616786\n",
      "MAE train: 0.422498\tval: 0.465257\n",
      "pearson_R train: 0.659123\tval: 0.522670\n",
      "Epoch: 331\n",
      "Loss: 0.59794110722012\n",
      "RMSE train: 0.475146\tval: 0.582754\n",
      "MAE train: 0.375910\tval: 0.457833\n",
      "pearson_R train: 0.730097\tval: 0.563374\n",
      "Epoch: 332\n",
      "Loss: 0.5588120453887515\n",
      "RMSE train: 0.504923\tval: 0.609464\n",
      "MAE train: 0.406243\tval: 0.482567\n",
      "pearson_R train: 0.720042\tval: 0.566036\n",
      "Epoch: 333\n",
      "Loss: 0.5632887052165138\n",
      "RMSE train: 0.541557\tval: 0.591852\n",
      "MAE train: 0.410083\tval: 0.446199\n",
      "pearson_R train: 0.699889\tval: 0.565931\n",
      "Epoch: 334\n",
      "Loss: 0.5765614774492052\n",
      "RMSE train: 0.496815\tval: 0.573099\n",
      "MAE train: 0.370561\tval: 0.421356\n",
      "pearson_R train: 0.740287\tval: 0.564360\n",
      "Epoch: 335\n",
      "Loss: 0.5531099306212531\n",
      "RMSE train: 0.491237\tval: 0.571904\n",
      "MAE train: 0.371590\tval: 0.427963\n",
      "pearson_R train: 0.734341\tval: 0.576899\n",
      "Epoch: 336\n",
      "Loss: 0.5660220152801938\n",
      "RMSE train: 0.494727\tval: 0.566983\n",
      "MAE train: 0.372222\tval: 0.424946\n",
      "pearson_R train: 0.729170\tval: 0.579426\n",
      "Epoch: 337\n",
      "Loss: 0.5868891543812222\n",
      "RMSE train: 0.569084\tval: 0.604542\n",
      "MAE train: 0.421590\tval: 0.441651\n",
      "pearson_R train: 0.717035\tval: 0.576628\n",
      "Epoch: 338\n",
      "Loss: 0.5641491678025987\n",
      "RMSE train: 0.489781\tval: 0.576450\n",
      "MAE train: 0.393252\tval: 0.457397\n",
      "pearson_R train: 0.712983\tval: 0.547783\n",
      "Epoch: 339\n",
      "Loss: 0.5445546838972304\n",
      "RMSE train: 0.488514\tval: 0.597520\n",
      "MAE train: 0.389562\tval: 0.470736\n",
      "pearson_R train: 0.720866\tval: 0.555755\n",
      "Epoch: 340\n",
      "Loss: 0.5776372750600179\n",
      "RMSE train: 0.476270\tval: 0.578473\n",
      "MAE train: 0.375189\tval: 0.455723\n",
      "pearson_R train: 0.727839\tval: 0.567419\n",
      "Epoch: 341\n",
      "Loss: 0.5571369032065073\n",
      "RMSE train: 0.463623\tval: 0.572633\n",
      "MAE train: 0.363458\tval: 0.435526\n",
      "pearson_R train: 0.744023\tval: 0.573872\n",
      "Epoch: 342\n",
      "Loss: 0.5664090249273512\n",
      "RMSE train: 0.496835\tval: 0.584254\n",
      "MAE train: 0.379288\tval: 0.437138\n",
      "pearson_R train: 0.714417\tval: 0.562986\n",
      "Epoch: 343\n",
      "Loss: 0.5699533555242751\n",
      "RMSE train: 0.491103\tval: 0.595281\n",
      "MAE train: 0.393853\tval: 0.463588\n",
      "pearson_R train: 0.725230\tval: 0.561530\n",
      "Epoch: 344\n",
      "Loss: 0.5498665836122301\n",
      "RMSE train: 0.466752\tval: 0.559045\n",
      "MAE train: 0.358826\tval: 0.422878\n",
      "pearson_R train: 0.750284\tval: 0.580581\n",
      "Epoch: 345\n",
      "Loss: 0.5740243130260043\n",
      "RMSE train: 0.478831\tval: 0.572862\n",
      "MAE train: 0.374470\tval: 0.443869\n",
      "pearson_R train: 0.723957\tval: 0.565324\n",
      "Epoch: 346\n",
      "Loss: 0.5694760812653435\n",
      "RMSE train: 0.485378\tval: 0.574773\n",
      "MAE train: 0.380189\tval: 0.440184\n",
      "pearson_R train: 0.717937\tval: 0.538737\n",
      "Epoch: 347\n",
      "Loss: 0.5453039374616411\n",
      "RMSE train: 0.490537\tval: 0.569064\n",
      "MAE train: 0.370088\tval: 0.417407\n",
      "pearson_R train: 0.737944\tval: 0.575707\n",
      "Epoch: 348\n",
      "Loss: 0.5789742867151896\n",
      "RMSE train: 0.541136\tval: 0.602202\n",
      "MAE train: 0.406018\tval: 0.445915\n",
      "pearson_R train: 0.709924\tval: 0.550497\n",
      "Epoch: 349\n",
      "Loss: 0.5724000400967069\n",
      "RMSE train: 0.477289\tval: 0.568006\n",
      "MAE train: 0.372904\tval: 0.433778\n",
      "pearson_R train: 0.726837\tval: 0.553347\n",
      "Epoch: 350\n",
      "Loss: 0.5719325343767802\n",
      "RMSE train: 0.469804\tval: 0.558323\n",
      "MAE train: 0.366092\tval: 0.430964\n",
      "pearson_R train: 0.738378\tval: 0.580911\n",
      "Epoch: 351\n",
      "Loss: 0.5436573392815061\n",
      "RMSE train: 0.471175\tval: 0.575283\n",
      "MAE train: 0.375430\tval: 0.452605\n",
      "pearson_R train: 0.741467\tval: 0.579531\n",
      "Epoch: 352\n",
      "Loss: 0.5593596266375648\n",
      "RMSE train: 0.470503\tval: 0.573711\n",
      "MAE train: 0.360458\tval: 0.432032\n",
      "pearson_R train: 0.741998\tval: 0.566195\n",
      "Epoch: 353\n",
      "Loss: 0.5698557164933946\n",
      "RMSE train: 0.472364\tval: 0.576227\n",
      "MAE train: 0.375365\tval: 0.450712\n",
      "pearson_R train: 0.737819\tval: 0.574271\n",
      "Epoch: 354\n",
      "Loss: 0.5559845831659105\n",
      "RMSE train: 0.504990\tval: 0.621622\n",
      "MAE train: 0.409339\tval: 0.497569\n",
      "pearson_R train: 0.730010\tval: 0.548744\n",
      "Epoch: 355\n",
      "Loss: 0.5537963840696547\n",
      "RMSE train: 0.465852\tval: 0.574194\n",
      "MAE train: 0.358315\tval: 0.436206\n",
      "pearson_R train: 0.748505\tval: 0.556804\n",
      "Epoch: 356\n",
      "Loss: 0.5448667307694753\n",
      "RMSE train: 0.496139\tval: 0.596205\n",
      "MAE train: 0.382235\tval: 0.451937\n",
      "pearson_R train: 0.717910\tval: 0.543003\n",
      "Epoch: 357\n",
      "Loss: 0.5574657453431023\n",
      "RMSE train: 0.476796\tval: 0.579184\n",
      "MAE train: 0.366664\tval: 0.432136\n",
      "pearson_R train: 0.735678\tval: 0.552257\n",
      "Epoch: 358\n",
      "Loss: 0.5605621404118009\n",
      "RMSE train: 0.468581\tval: 0.582018\n",
      "MAE train: 0.366831\tval: 0.452891\n",
      "pearson_R train: 0.736832\tval: 0.545407\n",
      "Epoch: 359\n",
      "Loss: 0.5498258670171102\n",
      "RMSE train: 0.564130\tval: 0.621173\n",
      "MAE train: 0.442919\tval: 0.477304\n",
      "pearson_R train: 0.686563\tval: 0.564382\n",
      "Epoch: 360\n",
      "Loss: 0.5331518451372782\n",
      "RMSE train: 0.551734\tval: 0.662792\n",
      "MAE train: 0.450510\tval: 0.532184\n",
      "pearson_R train: 0.727478\tval: 0.564897\n",
      "Epoch: 361\n",
      "Loss: 0.5275236202610863\n",
      "RMSE train: 0.472266\tval: 0.577725\n",
      "MAE train: 0.379227\tval: 0.457894\n",
      "pearson_R train: 0.742343\tval: 0.567959\n",
      "Epoch: 362\n",
      "Loss: 0.550798730717765\n",
      "RMSE train: 0.469129\tval: 0.574763\n",
      "MAE train: 0.373542\tval: 0.454968\n",
      "pearson_R train: 0.743900\tval: 0.573378\n",
      "Epoch: 363\n",
      "Loss: 0.5652730067571005\n",
      "RMSE train: 0.462903\tval: 0.575180\n",
      "MAE train: 0.361583\tval: 0.443023\n",
      "pearson_R train: 0.745088\tval: 0.567836\n",
      "Epoch: 364\n",
      "Loss: 0.5425820681783888\n",
      "RMSE train: 0.497573\tval: 0.622775\n",
      "MAE train: 0.398631\tval: 0.484200\n",
      "pearson_R train: 0.744946\tval: 0.579245\n",
      "Epoch: 365\n",
      "Loss: 0.5742136902279324\n",
      "RMSE train: 0.470623\tval: 0.570742\n",
      "MAE train: 0.360770\tval: 0.426452\n",
      "pearson_R train: 0.749610\tval: 0.571830\n",
      "Epoch: 366\n",
      "Loss: 0.5387844940026602\n",
      "RMSE train: 0.502929\tval: 0.572208\n",
      "MAE train: 0.378568\tval: 0.427235\n",
      "pearson_R train: 0.734057\tval: 0.574057\n",
      "Epoch: 367\n",
      "Loss: 0.5442494750022888\n",
      "RMSE train: 0.461067\tval: 0.564898\n",
      "MAE train: 0.353296\tval: 0.428592\n",
      "pearson_R train: 0.754849\tval: 0.576452\n",
      "Epoch: 368\n",
      "Loss: 0.5597644050916036\n",
      "RMSE train: 0.459519\tval: 0.576318\n",
      "MAE train: 0.361878\tval: 0.445696\n",
      "pearson_R train: 0.749591\tval: 0.555968\n",
      "Epoch: 369\n",
      "Loss: 0.5562945074505277\n",
      "RMSE train: 0.462689\tval: 0.581390\n",
      "MAE train: 0.363914\tval: 0.446687\n",
      "pearson_R train: 0.751677\tval: 0.582692\n",
      "Epoch: 370\n",
      "Loss: 0.5400450527667999\n",
      "RMSE train: 0.481748\tval: 0.599898\n",
      "MAE train: 0.382768\tval: 0.476166\n",
      "pearson_R train: 0.738107\tval: 0.555772\n",
      "Epoch: 371\n",
      "Loss: 0.5311533047093285\n",
      "RMSE train: 0.461988\tval: 0.580084\n",
      "MAE train: 0.366671\tval: 0.450669\n",
      "pearson_R train: 0.749694\tval: 0.570191\n",
      "Epoch: 372\n",
      "Loss: 0.5525663958655463\n",
      "RMSE train: 0.491049\tval: 0.571757\n",
      "MAE train: 0.373377\tval: 0.429846\n",
      "pearson_R train: 0.738048\tval: 0.574914\n",
      "Epoch: 373\n",
      "Loss: 0.5502886407905154\n",
      "RMSE train: 0.458317\tval: 0.564395\n",
      "MAE train: 0.352975\tval: 0.426752\n",
      "pearson_R train: 0.758729\tval: 0.577398\n",
      "Epoch: 374\n",
      "Loss: 0.5417163570721945\n",
      "RMSE train: 0.491552\tval: 0.576638\n",
      "MAE train: 0.376066\tval: 0.437271\n",
      "pearson_R train: 0.737045\tval: 0.578808\n",
      "Epoch: 375\n",
      "Loss: 0.546544094880422\n",
      "RMSE train: 0.465907\tval: 0.603423\n",
      "MAE train: 0.370212\tval: 0.471874\n",
      "pearson_R train: 0.753078\tval: 0.563437\n",
      "Epoch: 376\n",
      "Loss: 0.5561099184883965\n",
      "RMSE train: 0.462696\tval: 0.571114\n",
      "MAE train: 0.363855\tval: 0.436241\n",
      "pearson_R train: 0.745081\tval: 0.567151\n",
      "Epoch: 377\n",
      "Loss: 0.5492768022749159\n",
      "RMSE train: 0.457339\tval: 0.587279\n",
      "MAE train: 0.362244\tval: 0.454023\n",
      "pearson_R train: 0.755487\tval: 0.575488\n",
      "Epoch: 378\n",
      "Loss: 0.5287907156679366\n",
      "RMSE train: 0.472368\tval: 0.575404\n",
      "MAE train: 0.363885\tval: 0.433656\n",
      "pearson_R train: 0.754328\tval: 0.572888\n",
      "Epoch: 379\n",
      "Loss: 0.5457330014970567\n",
      "RMSE train: 0.458656\tval: 0.571781\n",
      "MAE train: 0.363511\tval: 0.448149\n",
      "pearson_R train: 0.749844\tval: 0.573736\n",
      "Epoch: 380\n",
      "Loss: 0.5350726114379035\n",
      "RMSE train: 0.494617\tval: 0.609889\n",
      "MAE train: 0.406741\tval: 0.493400\n",
      "pearson_R train: 0.741433\tval: 0.568551\n",
      "Epoch: 381\n",
      "Loss: 0.5739938153160943\n",
      "RMSE train: 0.481219\tval: 0.571516\n",
      "MAE train: 0.377125\tval: 0.446027\n",
      "pearson_R train: 0.722325\tval: 0.554969\n",
      "Epoch: 382\n",
      "Loss: 0.5281694928805033\n",
      "RMSE train: 0.567787\tval: 0.632267\n",
      "MAE train: 0.442142\tval: 0.476873\n",
      "pearson_R train: 0.730484\tval: 0.564775\n",
      "Epoch: 383\n",
      "Loss: 0.5313902298609415\n",
      "RMSE train: 0.466600\tval: 0.579035\n",
      "MAE train: 0.362321\tval: 0.437545\n",
      "pearson_R train: 0.752263\tval: 0.573271\n",
      "Epoch: 384\n",
      "Loss: 0.540171805355284\n",
      "RMSE train: 0.463263\tval: 0.587540\n",
      "MAE train: 0.370546\tval: 0.464819\n",
      "pearson_R train: 0.753425\tval: 0.555531\n",
      "Epoch: 385\n",
      "Loss: 0.5226515266630385\n",
      "RMSE train: 0.488016\tval: 0.597856\n",
      "MAE train: 0.376196\tval: 0.453446\n",
      "pearson_R train: 0.725860\tval: 0.544219\n",
      "Epoch: 386\n",
      "Loss: 0.5284145971139272\n",
      "RMSE train: 0.569403\tval: 0.680713\n",
      "MAE train: 0.458262\tval: 0.533543\n",
      "pearson_R train: 0.684540\tval: 0.512609\n",
      "Epoch: 387\n",
      "Loss: 0.5319726963837942\n",
      "RMSE train: 0.483270\tval: 0.608244\n",
      "MAE train: 0.390485\tval: 0.483013\n",
      "pearson_R train: 0.745959\tval: 0.563091\n",
      "Epoch: 388\n",
      "Loss: 0.5484459168381162\n",
      "RMSE train: 0.462351\tval: 0.579153\n",
      "MAE train: 0.362156\tval: 0.452835\n",
      "pearson_R train: 0.745174\tval: 0.560545\n",
      "Epoch: 389\n",
      "Loss: 0.5369409455193414\n",
      "RMSE train: 0.464286\tval: 0.568057\n",
      "MAE train: 0.365479\tval: 0.443034\n",
      "pearson_R train: 0.743465\tval: 0.582731\n",
      "Epoch: 390\n",
      "Loss: 0.5357585383786095\n",
      "RMSE train: 0.462970\tval: 0.562971\n",
      "MAE train: 0.359854\tval: 0.429284\n",
      "pearson_R train: 0.745974\tval: 0.574795\n",
      "Epoch: 391\n",
      "Loss: 0.5365274945894877\n",
      "RMSE train: 0.481394\tval: 0.603166\n",
      "MAE train: 0.391804\tval: 0.485898\n",
      "pearson_R train: 0.747492\tval: 0.550354\n",
      "Epoch: 392\n",
      "Loss: 0.5336296988858117\n",
      "RMSE train: 0.474337\tval: 0.609678\n",
      "MAE train: 0.379032\tval: 0.476531\n",
      "pearson_R train: 0.746953\tval: 0.555096\n",
      "Epoch: 393\n",
      "Loss: 0.5338727964295281\n",
      "RMSE train: 0.465832\tval: 0.596065\n",
      "MAE train: 0.369039\tval: 0.463322\n",
      "pearson_R train: 0.746979\tval: 0.555543\n",
      "Epoch: 394\n",
      "Loss: 0.5121574269400703\n",
      "RMSE train: 0.465793\tval: 0.575819\n",
      "MAE train: 0.359443\tval: 0.429794\n",
      "pearson_R train: 0.764286\tval: 0.584524\n",
      "Epoch: 395\n",
      "Loss: 0.5275025897555881\n",
      "RMSE train: 0.495551\tval: 0.593587\n",
      "MAE train: 0.380281\tval: 0.443317\n",
      "pearson_R train: 0.743698\tval: 0.569606\n",
      "Epoch: 396\n",
      "Loss: 0.5274796055422889\n",
      "RMSE train: 0.475070\tval: 0.581123\n",
      "MAE train: 0.361950\tval: 0.436113\n",
      "pearson_R train: 0.757468\tval: 0.566157\n",
      "Epoch: 397\n",
      "Loss: 0.5260064899921417\n",
      "RMSE train: 0.447881\tval: 0.574675\n",
      "MAE train: 0.353046\tval: 0.448315\n",
      "pearson_R train: 0.764683\tval: 0.563371\n",
      "Epoch: 398\n",
      "Loss: 0.5060020916991763\n",
      "RMSE train: 0.479841\tval: 0.610746\n",
      "MAE train: 0.386968\tval: 0.484587\n",
      "pearson_R train: 0.754151\tval: 0.574404\n",
      "Epoch: 399\n",
      "Loss: 0.5150046911504533\n",
      "RMSE train: 0.454610\tval: 0.586934\n",
      "MAE train: 0.356594\tval: 0.452866\n",
      "pearson_R train: 0.756313\tval: 0.567049\n",
      "Epoch: 400\n",
      "Loss: 0.49196470777193707\n",
      "RMSE train: 0.439340\tval: 0.565665\n",
      "MAE train: 0.343264\tval: 0.429467\n",
      "pearson_R train: 0.773643\tval: 0.586416\n",
      "Epoch: 401\n",
      "Loss: 0.5150536199410757\n",
      "RMSE train: 0.505476\tval: 0.576199\n",
      "MAE train: 0.388571\tval: 0.437654\n",
      "pearson_R train: 0.730557\tval: 0.585140\n",
      "Epoch: 402\n",
      "Loss: 0.5296235250102149\n",
      "RMSE train: 0.452715\tval: 0.561088\n",
      "MAE train: 0.349627\tval: 0.424606\n",
      "pearson_R train: 0.762723\tval: 0.575431\n",
      "Epoch: 403\n",
      "Loss: 0.49444399277369183\n",
      "RMSE train: 0.497472\tval: 0.583975\n",
      "MAE train: 0.377497\tval: 0.435455\n",
      "pearson_R train: 0.743415\tval: 0.557233\n",
      "Epoch: 404\n",
      "Loss: 0.5326145225101047\n",
      "RMSE train: 0.455137\tval: 0.592783\n",
      "MAE train: 0.361316\tval: 0.459648\n",
      "pearson_R train: 0.763640\tval: 0.573671\n",
      "Epoch: 405\n",
      "Loss: 0.5034039086765714\n",
      "RMSE train: 0.450336\tval: 0.570269\n",
      "MAE train: 0.348945\tval: 0.435857\n",
      "pearson_R train: 0.762427\tval: 0.571322\n",
      "Epoch: 406\n",
      "Loss: 0.5270302759276496\n",
      "RMSE train: 0.491499\tval: 0.582655\n",
      "MAE train: 0.374138\tval: 0.436182\n",
      "pearson_R train: 0.721475\tval: 0.546160\n",
      "Epoch: 407\n",
      "Loss: 0.5146669911013709\n",
      "RMSE train: 0.527552\tval: 0.654184\n",
      "MAE train: 0.419668\tval: 0.514968\n",
      "pearson_R train: 0.723527\tval: 0.543749\n",
      "Epoch: 408\n",
      "Loss: 0.5189773407247331\n",
      "RMSE train: 0.472989\tval: 0.572110\n",
      "MAE train: 0.361556\tval: 0.433706\n",
      "pearson_R train: 0.772186\tval: 0.595719\n",
      "Epoch: 409\n",
      "Loss: 0.5176245603296492\n",
      "RMSE train: 0.444990\tval: 0.575464\n",
      "MAE train: 0.350116\tval: 0.440152\n",
      "pearson_R train: 0.766692\tval: 0.582192\n",
      "Epoch: 410\n",
      "Loss: 0.5006087323029836\n",
      "RMSE train: 0.496490\tval: 0.620934\n",
      "MAE train: 0.402301\tval: 0.496612\n",
      "pearson_R train: 0.743446\tval: 0.551672\n",
      "Epoch: 411\n",
      "Loss: 0.5070783893267313\n",
      "RMSE train: 0.448842\tval: 0.580913\n",
      "MAE train: 0.356343\tval: 0.450011\n",
      "pearson_R train: 0.766910\tval: 0.581588\n",
      "Epoch: 412\n",
      "Loss: 0.5005248321427239\n",
      "RMSE train: 0.469546\tval: 0.571666\n",
      "MAE train: 0.360555\tval: 0.427093\n",
      "pearson_R train: 0.752163\tval: 0.570814\n",
      "Epoch: 413\n",
      "Loss: 0.5251958303981357\n",
      "RMSE train: 0.472228\tval: 0.589631\n",
      "MAE train: 0.376944\tval: 0.466863\n",
      "pearson_R train: 0.749568\tval: 0.571973\n",
      "Epoch: 414\n",
      "Loss: 0.5203588008880615\n",
      "RMSE train: 0.467404\tval: 0.570766\n",
      "MAE train: 0.361122\tval: 0.441984\n",
      "pearson_R train: 0.741325\tval: 0.570242\n",
      "Epoch: 415\n",
      "Loss: 0.5026621222496033\n",
      "RMSE train: 0.501303\tval: 0.586655\n",
      "MAE train: 0.379253\tval: 0.438071\n",
      "pearson_R train: 0.744426\tval: 0.553532\n",
      "Epoch: 416\n",
      "Loss: 0.48500844173961216\n",
      "RMSE train: 0.460110\tval: 0.591396\n",
      "MAE train: 0.354378\tval: 0.448621\n",
      "pearson_R train: 0.753975\tval: 0.556508\n",
      "Epoch: 417\n",
      "Loss: 0.5042251116699643\n",
      "RMSE train: 0.448995\tval: 0.585698\n",
      "MAE train: 0.347457\tval: 0.444141\n",
      "pearson_R train: 0.765949\tval: 0.572938\n",
      "Epoch: 418\n",
      "Loss: 0.49246898624632096\n",
      "RMSE train: 0.448788\tval: 0.590870\n",
      "MAE train: 0.357608\tval: 0.464103\n",
      "pearson_R train: 0.773194\tval: 0.573423\n",
      "Epoch: 419\n",
      "Loss: 0.49004896481831867\n",
      "RMSE train: 0.451802\tval: 0.577209\n",
      "MAE train: 0.354023\tval: 0.439519\n",
      "pearson_R train: 0.759229\tval: 0.570921\n",
      "Epoch: 420\n",
      "Loss: 0.5198030240005918\n",
      "RMSE train: 0.443055\tval: 0.568868\n",
      "MAE train: 0.346446\tval: 0.438168\n",
      "pearson_R train: 0.769086\tval: 0.582322\n",
      "Epoch: 421\n",
      "Loss: 0.5035099056031969\n",
      "RMSE train: 0.445446\tval: 0.558295\n",
      "MAE train: 0.342375\tval: 0.424311\n",
      "pearson_R train: 0.776008\tval: 0.596811\n",
      "Epoch: 422\n",
      "Loss: 0.48794445395469666\n",
      "RMSE train: 0.432639\tval: 0.561817\n",
      "MAE train: 0.341002\tval: 0.432503\n",
      "pearson_R train: 0.782812\tval: 0.596170\n",
      "Epoch: 423\n",
      "Loss: 0.5125751992066702\n",
      "RMSE train: 0.465809\tval: 0.576177\n",
      "MAE train: 0.358316\tval: 0.435990\n",
      "pearson_R train: 0.749502\tval: 0.577248\n",
      "Epoch: 424\n",
      "Loss: 0.5286709600024753\n",
      "RMSE train: 0.458938\tval: 0.574321\n",
      "MAE train: 0.349169\tval: 0.428835\n",
      "pearson_R train: 0.769087\tval: 0.570886\n",
      "Epoch: 425\n",
      "Loss: 0.5110592808988359\n",
      "RMSE train: 0.463947\tval: 0.599707\n",
      "MAE train: 0.363211\tval: 0.465710\n",
      "pearson_R train: 0.754708\tval: 0.580032\n",
      "Epoch: 426\n",
      "Loss: 0.5009838143984476\n",
      "RMSE train: 0.437819\tval: 0.573738\n",
      "MAE train: 0.336184\tval: 0.432714\n",
      "pearson_R train: 0.776493\tval: 0.566514\n",
      "Epoch: 427\n",
      "Loss: 0.5066900054613749\n",
      "RMSE train: 0.471162\tval: 0.573986\n",
      "MAE train: 0.356162\tval: 0.427938\n",
      "pearson_R train: 0.767411\tval: 0.559343\n",
      "Epoch: 428\n",
      "Loss: 0.498053189780977\n",
      "RMSE train: 0.458210\tval: 0.599242\n",
      "MAE train: 0.364341\tval: 0.470647\n",
      "pearson_R train: 0.769043\tval: 0.567526\n",
      "Epoch: 429\n",
      "Loss: 0.48955049448543125\n",
      "RMSE train: 0.483362\tval: 0.624918\n",
      "MAE train: 0.393751\tval: 0.499093\n",
      "pearson_R train: 0.771496\tval: 0.567142\n",
      "Epoch: 430\n",
      "Loss: 0.46343403061230976\n",
      "RMSE train: 0.436703\tval: 0.572593\n",
      "MAE train: 0.337933\tval: 0.435711\n",
      "pearson_R train: 0.779384\tval: 0.574308\n",
      "Epoch: 431\n",
      "Loss: 0.48912497361501056\n",
      "RMSE train: 0.449247\tval: 0.609806\n",
      "MAE train: 0.352186\tval: 0.471676\n",
      "pearson_R train: 0.770139\tval: 0.554747\n",
      "Epoch: 432\n",
      "Loss: 0.49169689416885376\n",
      "RMSE train: 0.541880\tval: 0.624452\n",
      "MAE train: 0.412558\tval: 0.467275\n",
      "pearson_R train: 0.728724\tval: 0.549395\n",
      "Epoch: 433\n",
      "Loss: 0.4949531356493632\n",
      "RMSE train: 0.448430\tval: 0.579387\n",
      "MAE train: 0.356565\tval: 0.453651\n",
      "pearson_R train: 0.766933\tval: 0.575522\n",
      "Epoch: 434\n",
      "Loss: 0.48214928309122723\n",
      "RMSE train: 0.449803\tval: 0.566969\n",
      "MAE train: 0.343912\tval: 0.426601\n",
      "pearson_R train: 0.773219\tval: 0.576220\n",
      "Epoch: 435\n",
      "Loss: 0.5229081643952264\n",
      "RMSE train: 0.435425\tval: 0.582534\n",
      "MAE train: 0.338741\tval: 0.439959\n",
      "pearson_R train: 0.781165\tval: 0.564814\n",
      "Epoch: 436\n",
      "Loss: 0.5038303799099393\n",
      "RMSE train: 0.493217\tval: 0.603277\n",
      "MAE train: 0.387448\tval: 0.466126\n",
      "pearson_R train: 0.716875\tval: 0.530834\n",
      "Epoch: 437\n",
      "Loss: 0.5060091118017832\n",
      "RMSE train: 0.478226\tval: 0.576813\n",
      "MAE train: 0.359509\tval: 0.429066\n",
      "pearson_R train: 0.759778\tval: 0.583070\n",
      "Epoch: 438\n",
      "Loss: 0.4874417185783386\n",
      "RMSE train: 0.497540\tval: 0.639457\n",
      "MAE train: 0.402069\tval: 0.510056\n",
      "pearson_R train: 0.765187\tval: 0.574850\n",
      "Epoch: 439\n",
      "Loss: 0.48478702041837907\n",
      "RMSE train: 0.452878\tval: 0.551685\n",
      "MAE train: 0.347697\tval: 0.425990\n",
      "pearson_R train: 0.765836\tval: 0.586890\n",
      "Epoch: 440\n",
      "Loss: 0.4694744779003991\n",
      "RMSE train: 0.438047\tval: 0.587974\n",
      "MAE train: 0.343804\tval: 0.450156\n",
      "pearson_R train: 0.775579\tval: 0.565761\n",
      "Epoch: 441\n",
      "Loss: 0.506011790699429\n",
      "RMSE train: 0.543298\tval: 0.630999\n",
      "MAE train: 0.418425\tval: 0.468219\n",
      "pearson_R train: 0.736177\tval: 0.552424\n",
      "Epoch: 442\n",
      "Loss: 0.47734138038423324\n",
      "RMSE train: 0.437743\tval: 0.579613\n",
      "MAE train: 0.339783\tval: 0.445974\n",
      "pearson_R train: 0.775913\tval: 0.570700\n",
      "Epoch: 443\n",
      "Loss: 0.45971012115478516\n",
      "RMSE train: 0.466141\tval: 0.626919\n",
      "MAE train: 0.373127\tval: 0.490263\n",
      "pearson_R train: 0.787056\tval: 0.582338\n",
      "Epoch: 444\n",
      "Loss: 0.4779323902395036\n",
      "RMSE train: 0.423826\tval: 0.567233\n",
      "MAE train: 0.326281\tval: 0.432843\n",
      "pearson_R train: 0.792671\tval: 0.586433\n",
      "Epoch: 445\n",
      "Loss: 0.47214535541004604\n",
      "RMSE train: 0.450499\tval: 0.572894\n",
      "MAE train: 0.337735\tval: 0.429393\n",
      "pearson_R train: 0.782876\tval: 0.584253\n",
      "Epoch: 446\n",
      "Loss: 0.5006164809068044\n",
      "RMSE train: 0.474095\tval: 0.614442\n",
      "MAE train: 0.377847\tval: 0.486223\n",
      "pearson_R train: 0.758537\tval: 0.562087\n",
      "Epoch: 447\n",
      "Loss: 0.4851979116598765\n",
      "RMSE train: 0.490821\tval: 0.639805\n",
      "MAE train: 0.394962\tval: 0.508838\n",
      "pearson_R train: 0.758933\tval: 0.558539\n",
      "Epoch: 448\n",
      "Loss: 0.4846025738451216\n",
      "RMSE train: 0.431541\tval: 0.554029\n",
      "MAE train: 0.329641\tval: 0.418364\n",
      "pearson_R train: 0.794592\tval: 0.594314\n",
      "Epoch: 449\n",
      "Loss: 0.4672992792394426\n",
      "RMSE train: 0.468440\tval: 0.583170\n",
      "MAE train: 0.359168\tval: 0.435025\n",
      "pearson_R train: 0.780224\tval: 0.585786\n",
      "Epoch: 450\n",
      "Loss: 0.47685860262976754\n",
      "RMSE train: 0.427522\tval: 0.593543\n",
      "MAE train: 0.339665\tval: 0.462251\n",
      "pearson_R train: 0.795131\tval: 0.573366\n",
      "Epoch: 451\n",
      "Loss: 0.5039346019426981\n",
      "RMSE train: 0.506721\tval: 0.659687\n",
      "MAE train: 0.415914\tval: 0.525800\n",
      "pearson_R train: 0.770239\tval: 0.577959\n",
      "Epoch: 452\n",
      "Loss: 0.4875258141093784\n",
      "RMSE train: 0.429290\tval: 0.578629\n",
      "MAE train: 0.336157\tval: 0.444798\n",
      "pearson_R train: 0.786396\tval: 0.580043\n",
      "Epoch: 453\n",
      "Loss: 0.459993839263916\n",
      "RMSE train: 0.441493\tval: 0.569182\n",
      "MAE train: 0.339591\tval: 0.426822\n",
      "pearson_R train: 0.778375\tval: 0.590465\n",
      "Epoch: 454\n",
      "Loss: 0.4769878254996406\n",
      "RMSE train: 0.428969\tval: 0.567574\n",
      "MAE train: 0.326160\tval: 0.424258\n",
      "pearson_R train: 0.790424\tval: 0.581784\n",
      "Epoch: 455\n",
      "Loss: 0.47391194105148315\n",
      "RMSE train: 0.443958\tval: 0.569121\n",
      "MAE train: 0.336704\tval: 0.427123\n",
      "pearson_R train: 0.795638\tval: 0.569336\n",
      "Epoch: 456\n",
      "Loss: 0.48883359630902606\n",
      "RMSE train: 0.430416\tval: 0.568377\n",
      "MAE train: 0.329541\tval: 0.434774\n",
      "pearson_R train: 0.792343\tval: 0.580820\n",
      "Epoch: 457\n",
      "Loss: 0.4825325508912404\n",
      "RMSE train: 0.429720\tval: 0.585277\n",
      "MAE train: 0.336750\tval: 0.451403\n",
      "pearson_R train: 0.787645\tval: 0.563085\n",
      "Epoch: 458\n",
      "Loss: 0.47498856650458443\n",
      "RMSE train: 0.460858\tval: 0.611863\n",
      "MAE train: 0.367369\tval: 0.479565\n",
      "pearson_R train: 0.778952\tval: 0.584128\n",
      "Epoch: 459\n",
      "Loss: 0.47204531894789803\n",
      "RMSE train: 0.463858\tval: 0.593367\n",
      "MAE train: 0.353407\tval: 0.441140\n",
      "pearson_R train: 0.772948\tval: 0.570635\n",
      "Epoch: 460\n",
      "Loss: 0.4758046037620968\n",
      "RMSE train: 0.426024\tval: 0.582875\n",
      "MAE train: 0.337321\tval: 0.447673\n",
      "pearson_R train: 0.794219\tval: 0.584251\n",
      "Epoch: 461\n",
      "Loss: 0.4641825457413991\n",
      "RMSE train: 0.447085\tval: 0.576625\n",
      "MAE train: 0.351020\tval: 0.450108\n",
      "pearson_R train: 0.765269\tval: 0.547283\n",
      "Epoch: 462\n",
      "Loss: 0.4765766229894426\n",
      "RMSE train: 0.429328\tval: 0.590119\n",
      "MAE train: 0.338328\tval: 0.460684\n",
      "pearson_R train: 0.793428\tval: 0.601507\n",
      "Epoch: 463\n",
      "Loss: 0.4857138130399916\n",
      "RMSE train: 0.433228\tval: 0.568883\n",
      "MAE train: 0.329941\tval: 0.426551\n",
      "pearson_R train: 0.801336\tval: 0.568317\n",
      "Epoch: 464\n",
      "Loss: 0.48123934533860946\n",
      "RMSE train: 0.437174\tval: 0.615398\n",
      "MAE train: 0.349800\tval: 0.483364\n",
      "pearson_R train: 0.798195\tval: 0.577546\n",
      "Epoch: 465\n",
      "Loss: 0.4585493438773685\n",
      "RMSE train: 0.551988\tval: 0.702418\n",
      "MAE train: 0.449882\tval: 0.563617\n",
      "pearson_R train: 0.768526\tval: 0.572977\n",
      "Epoch: 466\n",
      "Loss: 0.43999262650807697\n",
      "RMSE train: 0.431817\tval: 0.583219\n",
      "MAE train: 0.339282\tval: 0.456130\n",
      "pearson_R train: 0.790686\tval: 0.598557\n",
      "Epoch: 467\n",
      "Loss: 0.45322342382536995\n",
      "RMSE train: 0.435779\tval: 0.555392\n",
      "MAE train: 0.333954\tval: 0.421405\n",
      "pearson_R train: 0.788150\tval: 0.596695\n",
      "Epoch: 468\n",
      "Loss: 0.4690937366750505\n",
      "RMSE train: 0.470424\tval: 0.579588\n",
      "MAE train: 0.359315\tval: 0.443554\n",
      "pearson_R train: 0.755543\tval: 0.574509\n",
      "Epoch: 469\n",
      "Loss: 0.46414772669474286\n",
      "RMSE train: 0.440844\tval: 0.570238\n",
      "MAE train: 0.337003\tval: 0.428841\n",
      "pearson_R train: 0.800171\tval: 0.583912\n",
      "Epoch: 470\n",
      "Loss: 0.476275944047504\n",
      "RMSE train: 0.567902\tval: 0.700000\n",
      "MAE train: 0.454813\tval: 0.559586\n",
      "pearson_R train: 0.733797\tval: 0.561102\n",
      "Epoch: 471\n",
      "Loss: 0.4543495873610179\n",
      "RMSE train: 0.468202\tval: 0.631279\n",
      "MAE train: 0.379082\tval: 0.502103\n",
      "pearson_R train: 0.790923\tval: 0.580905\n",
      "Epoch: 472\n",
      "Loss: 0.4532282915380266\n",
      "RMSE train: 0.420387\tval: 0.571253\n",
      "MAE train: 0.329233\tval: 0.439210\n",
      "pearson_R train: 0.795076\tval: 0.594113\n",
      "Epoch: 473\n",
      "Loss: 0.46597718861367965\n",
      "RMSE train: 0.414456\tval: 0.573836\n",
      "MAE train: 0.321793\tval: 0.437451\n",
      "pearson_R train: 0.801827\tval: 0.578886\n",
      "Epoch: 474\n",
      "Loss: 0.470950186252594\n",
      "RMSE train: 0.448324\tval: 0.577570\n",
      "MAE train: 0.338788\tval: 0.430153\n",
      "pearson_R train: 0.784979\tval: 0.577651\n",
      "Epoch: 475\n",
      "Loss: 0.457539869679345\n",
      "RMSE train: 0.410597\tval: 0.573507\n",
      "MAE train: 0.315478\tval: 0.431933\n",
      "pearson_R train: 0.810456\tval: 0.573934\n",
      "Epoch: 476\n",
      "Loss: 0.4344931443532308\n",
      "RMSE train: 0.411852\tval: 0.554156\n",
      "MAE train: 0.319585\tval: 0.424041\n",
      "pearson_R train: 0.805882\tval: 0.599983\n",
      "Epoch: 477\n",
      "Loss: 0.4577433433797624\n",
      "RMSE train: 0.462000\tval: 0.622769\n",
      "MAE train: 0.365769\tval: 0.485785\n",
      "pearson_R train: 0.779889\tval: 0.587846\n",
      "Epoch: 478\n",
      "Loss: 0.47238384352789986\n",
      "RMSE train: 0.410307\tval: 0.566734\n",
      "MAE train: 0.319957\tval: 0.434203\n",
      "pearson_R train: 0.806986\tval: 0.582765\n",
      "Epoch: 479\n",
      "Loss: 0.465226689974467\n",
      "RMSE train: 0.420919\tval: 0.552931\n",
      "MAE train: 0.323100\tval: 0.411977\n",
      "pearson_R train: 0.806607\tval: 0.587099\n",
      "Epoch: 480\n",
      "Loss: 0.46843236022525364\n",
      "RMSE train: 0.419517\tval: 0.571783\n",
      "MAE train: 0.329607\tval: 0.445911\n",
      "pearson_R train: 0.797264\tval: 0.569035\n",
      "Epoch: 481\n",
      "Loss: 0.44425998131434125\n",
      "RMSE train: 0.443237\tval: 0.625783\n",
      "MAE train: 0.350127\tval: 0.478559\n",
      "pearson_R train: 0.801434\tval: 0.581075\n",
      "Epoch: 482\n",
      "Loss: 0.4736336800787184\n",
      "RMSE train: 0.408449\tval: 0.554795\n",
      "MAE train: 0.317594\tval: 0.423568\n",
      "pearson_R train: 0.809126\tval: 0.597377\n",
      "Epoch: 483\n",
      "Loss: 0.4633887939982944\n",
      "RMSE train: 0.465924\tval: 0.570430\n",
      "MAE train: 0.355133\tval: 0.429993\n",
      "pearson_R train: 0.769829\tval: 0.591222\n",
      "Epoch: 484\n",
      "Loss: 0.4478880796167586\n",
      "RMSE train: 0.424364\tval: 0.589581\n",
      "MAE train: 0.330094\tval: 0.440812\n",
      "pearson_R train: 0.795368\tval: 0.595630\n",
      "Epoch: 485\n",
      "Loss: 0.4474129047658708\n",
      "RMSE train: 0.441807\tval: 0.551311\n",
      "MAE train: 0.336492\tval: 0.411289\n",
      "pearson_R train: 0.792374\tval: 0.602925\n",
      "Epoch: 486\n",
      "Loss: 0.46253057652049595\n",
      "RMSE train: 0.402876\tval: 0.564969\n",
      "MAE train: 0.312782\tval: 0.434430\n",
      "pearson_R train: 0.813920\tval: 0.596729\n",
      "Epoch: 487\n",
      "Loss: 0.4492293629381392\n",
      "RMSE train: 0.409385\tval: 0.567006\n",
      "MAE train: 0.321790\tval: 0.433246\n",
      "pearson_R train: 0.811738\tval: 0.610139\n",
      "Epoch: 488\n",
      "Loss: 0.44080718027220833\n",
      "RMSE train: 0.406307\tval: 0.566722\n",
      "MAE train: 0.314540\tval: 0.430241\n",
      "pearson_R train: 0.814534\tval: 0.588712\n",
      "Epoch: 489\n",
      "Loss: 0.4413834015528361\n",
      "RMSE train: 0.421528\tval: 0.591620\n",
      "MAE train: 0.340907\tval: 0.464027\n",
      "pearson_R train: 0.819941\tval: 0.599810\n",
      "Epoch: 490\n",
      "Loss: 0.45019864042599994\n",
      "RMSE train: 0.432329\tval: 0.571178\n",
      "MAE train: 0.326539\tval: 0.434680\n",
      "pearson_R train: 0.808907\tval: 0.586285\n",
      "Epoch: 491\n",
      "Loss: 0.4514802065160539\n",
      "RMSE train: 0.422624\tval: 0.578889\n",
      "MAE train: 0.319919\tval: 0.431844\n",
      "pearson_R train: 0.804902\tval: 0.575423\n",
      "Epoch: 492\n",
      "Loss: 0.45774616797765094\n",
      "RMSE train: 0.435182\tval: 0.592892\n",
      "MAE train: 0.350217\tval: 0.469495\n",
      "pearson_R train: 0.796092\tval: 0.580971\n",
      "Epoch: 493\n",
      "Loss: 0.4464448591073354\n",
      "RMSE train: 0.408831\tval: 0.553877\n",
      "MAE train: 0.316533\tval: 0.424113\n",
      "pearson_R train: 0.809989\tval: 0.598961\n",
      "Epoch: 494\n",
      "Loss: 0.44863879018359715\n",
      "RMSE train: 0.525679\tval: 0.677520\n",
      "MAE train: 0.416585\tval: 0.533042\n",
      "pearson_R train: 0.749615\tval: 0.544104\n",
      "Epoch: 495\n",
      "Loss: 0.4483575655354394\n",
      "RMSE train: 0.417356\tval: 0.597435\n",
      "MAE train: 0.324578\tval: 0.455469\n",
      "pearson_R train: 0.801644\tval: 0.569341\n",
      "Epoch: 496\n",
      "Loss: 0.4740961790084839\n",
      "RMSE train: 0.412038\tval: 0.591333\n",
      "MAE train: 0.321695\tval: 0.443075\n",
      "pearson_R train: 0.809455\tval: 0.599394\n",
      "Epoch: 497\n",
      "Loss: 0.4635024567445119\n",
      "RMSE train: 0.450130\tval: 0.567535\n",
      "MAE train: 0.341026\tval: 0.425479\n",
      "pearson_R train: 0.795026\tval: 0.579855\n",
      "Epoch: 498\n",
      "Loss: 0.45040912760628593\n",
      "RMSE train: 0.414720\tval: 0.583674\n",
      "MAE train: 0.323226\tval: 0.441867\n",
      "pearson_R train: 0.802305\tval: 0.587204\n",
      "Epoch: 499\n",
      "Loss: 0.42829280429416233\n",
      "RMSE train: 0.442785\tval: 0.606589\n",
      "MAE train: 0.354983\tval: 0.480676\n",
      "pearson_R train: 0.791179\tval: 0.569830\n",
      "Epoch: 500\n",
      "Loss: 0.45057467619578045\n",
      "RMSE train: 0.409151\tval: 0.570760\n",
      "MAE train: 0.318777\tval: 0.431022\n",
      "pearson_R train: 0.808508\tval: 0.599813\n",
      "Epoch: 501\n",
      "Loss: 0.4492399990558624\n",
      "RMSE train: 0.406915\tval: 0.560848\n",
      "MAE train: 0.310509\tval: 0.423388\n",
      "pearson_R train: 0.814362\tval: 0.591346\n",
      "Epoch: 502\n",
      "Loss: 0.4419066607952118\n",
      "RMSE train: 0.427865\tval: 0.576393\n",
      "MAE train: 0.324801\tval: 0.431820\n",
      "pearson_R train: 0.802011\tval: 0.563189\n",
      "Epoch: 503\n",
      "Loss: 0.46965841783417595\n",
      "RMSE train: 0.443383\tval: 0.594836\n",
      "MAE train: 0.340305\tval: 0.444111\n",
      "pearson_R train: 0.774062\tval: 0.538021\n",
      "Epoch: 504\n",
      "Loss: 0.44780591130256653\n",
      "RMSE train: 0.442393\tval: 0.578690\n",
      "MAE train: 0.337469\tval: 0.434219\n",
      "pearson_R train: 0.789754\tval: 0.578938\n",
      "Epoch: 505\n",
      "Loss: 0.44110021657413906\n",
      "RMSE train: 0.451829\tval: 0.596803\n",
      "MAE train: 0.360362\tval: 0.465093\n",
      "pearson_R train: 0.784021\tval: 0.578229\n",
      "Epoch: 506\n",
      "Loss: 0.43617035614119637\n",
      "RMSE train: 0.394346\tval: 0.566299\n",
      "MAE train: 0.304516\tval: 0.427891\n",
      "pearson_R train: 0.822909\tval: 0.592713\n",
      "Epoch: 507\n",
      "Loss: 0.46435124344295925\n",
      "RMSE train: 0.432688\tval: 0.575793\n",
      "MAE train: 0.333235\tval: 0.436916\n",
      "pearson_R train: 0.803283\tval: 0.584505\n",
      "Epoch: 508\n",
      "Loss: 0.4355576237042745\n",
      "RMSE train: 0.470341\tval: 0.638637\n",
      "MAE train: 0.373027\tval: 0.499018\n",
      "pearson_R train: 0.776671\tval: 0.549887\n",
      "Epoch: 509\n",
      "Loss: 0.43447523646884495\n",
      "RMSE train: 0.543651\tval: 0.640027\n",
      "MAE train: 0.418593\tval: 0.477219\n",
      "pearson_R train: 0.761198\tval: 0.545689\n",
      "Epoch: 510\n",
      "Loss: 0.4313858548800151\n",
      "RMSE train: 0.387927\tval: 0.577758\n",
      "MAE train: 0.304064\tval: 0.441553\n",
      "pearson_R train: 0.829639\tval: 0.578495\n",
      "Epoch: 511\n",
      "Loss: 0.41555504666434395\n",
      "RMSE train: 0.422479\tval: 0.590921\n",
      "MAE train: 0.325169\tval: 0.438354\n",
      "pearson_R train: 0.809768\tval: 0.573697\n",
      "Epoch: 512\n",
      "Loss: 0.43146665891011554\n",
      "RMSE train: 0.418055\tval: 0.578809\n",
      "MAE train: 0.320984\tval: 0.434292\n",
      "pearson_R train: 0.807539\tval: 0.587206\n",
      "Epoch: 513\n",
      "Loss: 0.44743860099050736\n",
      "RMSE train: 0.482086\tval: 0.594694\n",
      "MAE train: 0.363056\tval: 0.442717\n",
      "pearson_R train: 0.775785\tval: 0.553752\n",
      "Epoch: 514\n",
      "Loss: 0.44776490330696106\n",
      "RMSE train: 0.407510\tval: 0.568243\n",
      "MAE train: 0.311096\tval: 0.423546\n",
      "pearson_R train: 0.823063\tval: 0.592214\n",
      "Epoch: 515\n",
      "Loss: 0.42115020089679295\n",
      "RMSE train: 0.428722\tval: 0.593996\n",
      "MAE train: 0.341608\tval: 0.463541\n",
      "pearson_R train: 0.802587\tval: 0.577951\n",
      "Epoch: 516\n",
      "Loss: 0.4321662353144752\n",
      "RMSE train: 0.396973\tval: 0.556697\n",
      "MAE train: 0.305620\tval: 0.426863\n",
      "pearson_R train: 0.820592\tval: 0.599333\n",
      "Epoch: 517\n",
      "Loss: 0.43834567070007324\n",
      "RMSE train: 0.421232\tval: 0.596180\n",
      "MAE train: 0.332382\tval: 0.462759\n",
      "pearson_R train: 0.816837\tval: 0.600119\n",
      "Epoch: 518\n",
      "Loss: 0.4346936841805776\n",
      "RMSE train: 0.411801\tval: 0.574612\n",
      "MAE train: 0.318706\tval: 0.438913\n",
      "pearson_R train: 0.804905\tval: 0.585475\n",
      "Epoch: 519\n",
      "Loss: 0.4348740379015605\n",
      "RMSE train: 0.401396\tval: 0.583660\n",
      "MAE train: 0.314464\tval: 0.455025\n",
      "pearson_R train: 0.820282\tval: 0.601158\n",
      "Epoch: 520\n",
      "Loss: 0.4238927827941047\n",
      "RMSE train: 0.398721\tval: 0.568761\n",
      "MAE train: 0.305683\tval: 0.429359\n",
      "pearson_R train: 0.820300\tval: 0.602772\n",
      "Epoch: 521\n",
      "Loss: 0.40944409039285445\n",
      "RMSE train: 0.425425\tval: 0.567982\n",
      "MAE train: 0.328479\tval: 0.429897\n",
      "pearson_R train: 0.812225\tval: 0.600025\n",
      "Epoch: 522\n",
      "Loss: 0.44205161266856724\n",
      "RMSE train: 0.409160\tval: 0.598706\n",
      "MAE train: 0.322803\tval: 0.458707\n",
      "pearson_R train: 0.815792\tval: 0.576647\n",
      "Epoch: 523\n",
      "Loss: 0.4186473422580295\n",
      "RMSE train: 0.418257\tval: 0.565315\n",
      "MAE train: 0.318031\tval: 0.426722\n",
      "pearson_R train: 0.806756\tval: 0.600054\n",
      "Epoch: 524\n",
      "Loss: 0.4370577070448134\n",
      "RMSE train: 0.398348\tval: 0.569315\n",
      "MAE train: 0.305894\tval: 0.432148\n",
      "pearson_R train: 0.825531\tval: 0.593010\n",
      "Epoch: 525\n",
      "Loss: 0.4306754834122128\n",
      "RMSE train: 0.415772\tval: 0.595231\n",
      "MAE train: 0.327258\tval: 0.454909\n",
      "pearson_R train: 0.805834\tval: 0.574827\n",
      "Epoch: 526\n",
      "Loss: 0.4336301386356354\n",
      "RMSE train: 0.398387\tval: 0.568619\n",
      "MAE train: 0.305383\tval: 0.422328\n",
      "pearson_R train: 0.828421\tval: 0.598707\n",
      "Epoch: 527\n",
      "Loss: 0.4271804855929481\n",
      "RMSE train: 0.430550\tval: 0.616588\n",
      "MAE train: 0.343313\tval: 0.477725\n",
      "pearson_R train: 0.815202\tval: 0.587672\n",
      "Epoch: 528\n",
      "Loss: 0.408485793405109\n",
      "RMSE train: 0.400715\tval: 0.568122\n",
      "MAE train: 0.304499\tval: 0.423686\n",
      "pearson_R train: 0.826913\tval: 0.591927\n",
      "Epoch: 529\n",
      "Loss: 0.432416879468494\n",
      "RMSE train: 0.453356\tval: 0.638585\n",
      "MAE train: 0.356308\tval: 0.498569\n",
      "pearson_R train: 0.786233\tval: 0.540109\n",
      "Epoch: 530\n",
      "Loss: 0.41947179701593185\n",
      "RMSE train: 0.402388\tval: 0.578050\n",
      "MAE train: 0.309287\tval: 0.431580\n",
      "pearson_R train: 0.822132\tval: 0.565064\n",
      "Epoch: 531\n",
      "Loss: 0.41185732351409066\n",
      "RMSE train: 0.519357\tval: 0.633459\n",
      "MAE train: 0.400684\tval: 0.484117\n",
      "pearson_R train: 0.778256\tval: 0.570874\n",
      "Epoch: 532\n",
      "Loss: 0.4276367889510261\n",
      "RMSE train: 0.430003\tval: 0.626173\n",
      "MAE train: 0.341357\tval: 0.476974\n",
      "pearson_R train: 0.818826\tval: 0.580285\n",
      "Epoch: 533\n",
      "Loss: 0.4070058332549201\n",
      "RMSE train: 0.384952\tval: 0.559305\n",
      "MAE train: 0.296185\tval: 0.419640\n",
      "pearson_R train: 0.832717\tval: 0.601258\n",
      "Epoch: 534\n",
      "Loss: 0.42696739236513775\n",
      "RMSE train: 0.407914\tval: 0.564082\n",
      "MAE train: 0.314872\tval: 0.426683\n",
      "pearson_R train: 0.812473\tval: 0.606244\n",
      "Epoch: 535\n",
      "Loss: 0.4273641374376085\n",
      "RMSE train: 0.419757\tval: 0.562750\n",
      "MAE train: 0.319255\tval: 0.425027\n",
      "pearson_R train: 0.814196\tval: 0.592935\n",
      "Epoch: 536\n",
      "Loss: 0.4392785926659902\n",
      "RMSE train: 0.414243\tval: 0.594049\n",
      "MAE train: 0.317135\tval: 0.439935\n",
      "pearson_R train: 0.815906\tval: 0.570556\n",
      "Epoch: 537\n",
      "Loss: 0.44567353857888115\n",
      "RMSE train: 0.404955\tval: 0.591061\n",
      "MAE train: 0.321235\tval: 0.459445\n",
      "pearson_R train: 0.818381\tval: 0.588225\n",
      "Epoch: 538\n",
      "Loss: 0.4394701255692376\n",
      "RMSE train: 0.390254\tval: 0.589522\n",
      "MAE train: 0.304516\tval: 0.443782\n",
      "pearson_R train: 0.830338\tval: 0.578629\n",
      "Epoch: 539\n",
      "Loss: 0.422804378800922\n",
      "RMSE train: 0.430993\tval: 0.583722\n",
      "MAE train: 0.328417\tval: 0.444015\n",
      "pearson_R train: 0.811226\tval: 0.591718\n",
      "Epoch: 540\n",
      "Loss: 0.4425376223193275\n",
      "RMSE train: 0.402583\tval: 0.579881\n",
      "MAE train: 0.313706\tval: 0.443798\n",
      "pearson_R train: 0.816697\tval: 0.595461\n",
      "Epoch: 541\n",
      "Loss: 0.41048402587572735\n",
      "RMSE train: 0.398627\tval: 0.588478\n",
      "MAE train: 0.310543\tval: 0.446581\n",
      "pearson_R train: 0.826623\tval: 0.601226\n",
      "Epoch: 542\n",
      "Loss: 0.43349041872554356\n",
      "RMSE train: 0.395629\tval: 0.572443\n",
      "MAE train: 0.312288\tval: 0.447341\n",
      "pearson_R train: 0.830999\tval: 0.604712\n",
      "Epoch: 543\n",
      "Loss: 0.43094545271661544\n",
      "RMSE train: 0.473181\tval: 0.583965\n",
      "MAE train: 0.360692\tval: 0.435619\n",
      "pearson_R train: 0.793666\tval: 0.579125\n",
      "Epoch: 544\n",
      "Loss: 0.44588400257958305\n",
      "RMSE train: 0.464417\tval: 0.591918\n",
      "MAE train: 0.353629\tval: 0.438891\n",
      "pearson_R train: 0.787863\tval: 0.575152\n",
      "Epoch: 545\n",
      "Loss: 0.42385663588841754\n",
      "RMSE train: 0.427921\tval: 0.574886\n",
      "MAE train: 0.324932\tval: 0.430991\n",
      "pearson_R train: 0.806389\tval: 0.584723\n",
      "Epoch: 546\n",
      "Loss: 0.4217154350545671\n",
      "RMSE train: 0.381230\tval: 0.569596\n",
      "MAE train: 0.292199\tval: 0.426476\n",
      "pearson_R train: 0.837019\tval: 0.593330\n",
      "Epoch: 547\n",
      "Loss: 0.4331751565138499\n",
      "RMSE train: 0.418399\tval: 0.590586\n",
      "MAE train: 0.319105\tval: 0.448512\n",
      "pearson_R train: 0.816040\tval: 0.562196\n",
      "Epoch: 548\n",
      "Loss: 0.4047074450386895\n",
      "RMSE train: 0.389252\tval: 0.580302\n",
      "MAE train: 0.297102\tval: 0.435234\n",
      "pearson_R train: 0.836427\tval: 0.583979\n",
      "Epoch: 549\n",
      "Loss: 0.41698672042952645\n",
      "RMSE train: 0.387031\tval: 0.582437\n",
      "MAE train: 0.301163\tval: 0.448344\n",
      "pearson_R train: 0.830587\tval: 0.583929\n",
      "Epoch: 550\n",
      "Loss: 0.4238788882891337\n",
      "RMSE train: 0.381630\tval: 0.579454\n",
      "MAE train: 0.295252\tval: 0.436903\n",
      "pearson_R train: 0.834771\tval: 0.586125\n",
      "Epoch: 551\n",
      "Loss: 0.39643800258636475\n",
      "RMSE train: 0.378936\tval: 0.583170\n",
      "MAE train: 0.291812\tval: 0.446563\n",
      "pearson_R train: 0.842308\tval: 0.589943\n",
      "Epoch: 552\n",
      "Loss: 0.4303373694419861\n",
      "RMSE train: 0.421098\tval: 0.574337\n",
      "MAE train: 0.322788\tval: 0.428893\n",
      "pearson_R train: 0.810792\tval: 0.592448\n",
      "Epoch: 553\n",
      "Loss: 0.42788832717471653\n",
      "RMSE train: 0.428621\tval: 0.574189\n",
      "MAE train: 0.325221\tval: 0.430978\n",
      "pearson_R train: 0.824099\tval: 0.600241\n",
      "Epoch: 554\n",
      "Loss: 0.43059490455521476\n",
      "RMSE train: 0.499506\tval: 0.661819\n",
      "MAE train: 0.397022\tval: 0.523609\n",
      "pearson_R train: 0.782349\tval: 0.565074\n",
      "Epoch: 555\n",
      "Loss: 0.410211894247267\n",
      "RMSE train: 0.417723\tval: 0.573728\n",
      "MAE train: 0.317140\tval: 0.429803\n",
      "pearson_R train: 0.821122\tval: 0.593153\n",
      "Epoch: 556\n",
      "Loss: 0.4275192883279588\n",
      "RMSE train: 0.391496\tval: 0.577299\n",
      "MAE train: 0.304740\tval: 0.441167\n",
      "pearson_R train: 0.825865\tval: 0.585447\n",
      "Epoch: 557\n",
      "Loss: 0.4123713771502177\n",
      "RMSE train: 0.464549\tval: 0.577423\n",
      "MAE train: 0.350455\tval: 0.430376\n",
      "pearson_R train: 0.764189\tval: 0.586215\n",
      "Epoch: 558\n",
      "Loss: 0.42659683028856915\n",
      "RMSE train: 0.401735\tval: 0.577135\n",
      "MAE train: 0.308868\tval: 0.426295\n",
      "pearson_R train: 0.819096\tval: 0.567516\n",
      "Epoch: 559\n",
      "Loss: 0.4329722987280952\n",
      "RMSE train: 0.424186\tval: 0.610040\n",
      "MAE train: 0.335226\tval: 0.477602\n",
      "pearson_R train: 0.803361\tval: 0.555193\n",
      "Epoch: 560\n",
      "Loss: 0.40481941567526925\n",
      "RMSE train: 0.456279\tval: 0.583694\n",
      "MAE train: 0.347057\tval: 0.439762\n",
      "pearson_R train: 0.797444\tval: 0.582316\n",
      "Epoch: 561\n",
      "Loss: 0.4098357425795661\n",
      "RMSE train: 0.388934\tval: 0.562669\n",
      "MAE train: 0.300095\tval: 0.422122\n",
      "pearson_R train: 0.833088\tval: 0.599906\n",
      "Epoch: 562\n",
      "Loss: 0.41119859947098625\n",
      "RMSE train: 0.397334\tval: 0.573803\n",
      "MAE train: 0.302401\tval: 0.432223\n",
      "pearson_R train: 0.837027\tval: 0.590398\n",
      "Epoch: 563\n",
      "Loss: 0.3949114878972371\n",
      "RMSE train: 0.389332\tval: 0.574048\n",
      "MAE train: 0.296840\tval: 0.427205\n",
      "pearson_R train: 0.843249\tval: 0.597113\n",
      "Epoch: 564\n",
      "Loss: 0.40422266059451634\n",
      "RMSE train: 0.399129\tval: 0.593910\n",
      "MAE train: 0.312626\tval: 0.456122\n",
      "pearson_R train: 0.825963\tval: 0.588340\n",
      "Epoch: 565\n",
      "Loss: 0.40324415432082283\n",
      "RMSE train: 0.382168\tval: 0.577150\n",
      "MAE train: 0.290762\tval: 0.431337\n",
      "pearson_R train: 0.841643\tval: 0.594758\n",
      "Epoch: 566\n",
      "Loss: 0.37364117635620964\n",
      "RMSE train: 0.388849\tval: 0.572493\n",
      "MAE train: 0.297671\tval: 0.426033\n",
      "pearson_R train: 0.832900\tval: 0.600232\n",
      "Epoch: 567\n",
      "Loss: 0.3762054741382599\n",
      "RMSE train: 0.382365\tval: 0.588582\n",
      "MAE train: 0.299806\tval: 0.449483\n",
      "pearson_R train: 0.839484\tval: 0.583539\n",
      "Epoch: 568\n",
      "Loss: 0.41300997800297207\n",
      "RMSE train: 0.375002\tval: 0.580806\n",
      "MAE train: 0.291591\tval: 0.446178\n",
      "pearson_R train: 0.842236\tval: 0.596478\n",
      "Epoch: 569\n",
      "Loss: 0.40368284781773883\n",
      "RMSE train: 0.429010\tval: 0.607690\n",
      "MAE train: 0.344473\tval: 0.477134\n",
      "pearson_R train: 0.819676\tval: 0.591268\n",
      "Epoch: 570\n",
      "Loss: 0.40853338771396214\n",
      "RMSE train: 0.405254\tval: 0.571219\n",
      "MAE train: 0.307617\tval: 0.427969\n",
      "pearson_R train: 0.834620\tval: 0.593892\n",
      "Epoch: 571\n",
      "Loss: 0.40763310922516716\n",
      "RMSE train: 0.403966\tval: 0.595255\n",
      "MAE train: 0.320776\tval: 0.447938\n",
      "pearson_R train: 0.826949\tval: 0.590149\n",
      "Epoch: 572\n",
      "Loss: 0.39155874649683636\n",
      "RMSE train: 0.370561\tval: 0.578310\n",
      "MAE train: 0.287111\tval: 0.433965\n",
      "pearson_R train: 0.846641\tval: 0.601656\n",
      "Epoch: 573\n",
      "Loss: 0.40803306963708663\n",
      "RMSE train: 0.375866\tval: 0.567079\n",
      "MAE train: 0.289289\tval: 0.425748\n",
      "pearson_R train: 0.842500\tval: 0.608935\n",
      "Epoch: 574\n",
      "Loss: 0.39677772919336957\n",
      "RMSE train: 0.377635\tval: 0.594514\n",
      "MAE train: 0.293842\tval: 0.454700\n",
      "pearson_R train: 0.847336\tval: 0.603078\n",
      "Epoch: 575\n",
      "Loss: 0.40671021739641827\n",
      "RMSE train: 0.392959\tval: 0.569260\n",
      "MAE train: 0.298662\tval: 0.427402\n",
      "pearson_R train: 0.834220\tval: 0.581648\n",
      "Epoch: 576\n",
      "Loss: 0.397280537419849\n",
      "RMSE train: 0.374911\tval: 0.590319\n",
      "MAE train: 0.289168\tval: 0.442122\n",
      "pearson_R train: 0.842207\tval: 0.580058\n",
      "Epoch: 577\n",
      "Loss: 0.37669290436638725\n",
      "RMSE train: 0.384008\tval: 0.597655\n",
      "MAE train: 0.296538\tval: 0.441732\n",
      "pearson_R train: 0.839326\tval: 0.562524\n",
      "Epoch: 578\n",
      "Loss: 0.3990339736143748\n",
      "RMSE train: 0.363546\tval: 0.586453\n",
      "MAE train: 0.280828\tval: 0.442487\n",
      "pearson_R train: 0.851688\tval: 0.576974\n",
      "Epoch: 579\n",
      "Loss: 0.4032646483845181\n",
      "RMSE train: 0.468729\tval: 0.602418\n",
      "MAE train: 0.359068\tval: 0.453439\n",
      "pearson_R train: 0.820876\tval: 0.582845\n",
      "Epoch: 580\n",
      "Loss: 0.38975793454382157\n",
      "RMSE train: 0.404077\tval: 0.580699\n",
      "MAE train: 0.306338\tval: 0.434963\n",
      "pearson_R train: 0.829041\tval: 0.578635\n",
      "Epoch: 581\n",
      "Loss: 0.3793871369626787\n",
      "RMSE train: 0.361093\tval: 0.568058\n",
      "MAE train: 0.277962\tval: 0.416756\n",
      "pearson_R train: 0.856046\tval: 0.601592\n",
      "Epoch: 582\n",
      "Loss: 0.36105472842852276\n",
      "RMSE train: 0.391316\tval: 0.575774\n",
      "MAE train: 0.304245\tval: 0.429692\n",
      "pearson_R train: 0.827956\tval: 0.607894\n",
      "Epoch: 583\n",
      "Loss: 0.39146357774734497\n",
      "RMSE train: 0.394610\tval: 0.593114\n",
      "MAE train: 0.307060\tval: 0.448726\n",
      "pearson_R train: 0.822273\tval: 0.554231\n",
      "Epoch: 584\n",
      "Loss: 0.3733583920531803\n",
      "RMSE train: 0.377125\tval: 0.568012\n",
      "MAE train: 0.289641\tval: 0.428653\n",
      "pearson_R train: 0.843742\tval: 0.583283\n",
      "Epoch: 585\n",
      "Loss: 0.4014399018552568\n",
      "RMSE train: 0.392541\tval: 0.566601\n",
      "MAE train: 0.299030\tval: 0.424927\n",
      "pearson_R train: 0.835421\tval: 0.607246\n",
      "Epoch: 586\n",
      "Loss: 0.39409640100267196\n",
      "RMSE train: 0.492215\tval: 0.685515\n",
      "MAE train: 0.395652\tval: 0.534671\n",
      "pearson_R train: 0.824599\tval: 0.580004\n",
      "Epoch: 587\n",
      "Loss: 0.3878137701087528\n",
      "RMSE train: 0.393402\tval: 0.580236\n",
      "MAE train: 0.301385\tval: 0.442629\n",
      "pearson_R train: 0.848719\tval: 0.604569\n",
      "Epoch: 588\n",
      "Loss: 0.3774787916077508\n",
      "RMSE train: 0.418630\tval: 0.580190\n",
      "MAE train: 0.316594\tval: 0.434876\n",
      "pearson_R train: 0.832663\tval: 0.574052\n",
      "Epoch: 589\n",
      "Loss: 0.36799944440523785\n",
      "RMSE train: 0.384357\tval: 0.561939\n",
      "MAE train: 0.298820\tval: 0.430900\n",
      "pearson_R train: 0.836005\tval: 0.577155\n",
      "Epoch: 590\n",
      "Loss: 0.3913336793581645\n",
      "RMSE train: 0.387813\tval: 0.564577\n",
      "MAE train: 0.300612\tval: 0.431689\n",
      "pearson_R train: 0.831134\tval: 0.613204\n",
      "Epoch: 591\n",
      "Loss: 0.4052337143156264\n",
      "RMSE train: 0.370755\tval: 0.584278\n",
      "MAE train: 0.285617\tval: 0.442404\n",
      "pearson_R train: 0.845194\tval: 0.590151\n",
      "Epoch: 592\n",
      "Loss: 0.41118160221311784\n",
      "RMSE train: 0.538441\tval: 0.729311\n",
      "MAE train: 0.437428\tval: 0.577145\n",
      "pearson_R train: 0.789513\tval: 0.527952\n",
      "Epoch: 593\n",
      "Loss: 0.37546924750010174\n",
      "RMSE train: 0.374031\tval: 0.582552\n",
      "MAE train: 0.287307\tval: 0.438258\n",
      "pearson_R train: 0.844806\tval: 0.583854\n",
      "Epoch: 594\n",
      "Loss: 0.3744664159086015\n",
      "RMSE train: 0.360045\tval: 0.581336\n",
      "MAE train: 0.278983\tval: 0.439233\n",
      "pearson_R train: 0.854831\tval: 0.592257\n",
      "Epoch: 595\n",
      "Loss: 0.37583979301982456\n",
      "RMSE train: 0.425897\tval: 0.591722\n",
      "MAE train: 0.326692\tval: 0.442029\n",
      "pearson_R train: 0.834541\tval: 0.587813\n",
      "Epoch: 596\n",
      "Loss: 0.39707969294654\n",
      "RMSE train: 0.369643\tval: 0.573712\n",
      "MAE train: 0.283683\tval: 0.429221\n",
      "pearson_R train: 0.851008\tval: 0.572668\n",
      "Epoch: 597\n",
      "Loss: 0.36284253663486904\n",
      "RMSE train: 0.380747\tval: 0.574081\n",
      "MAE train: 0.289871\tval: 0.430760\n",
      "pearson_R train: 0.849886\tval: 0.597626\n",
      "Epoch: 598\n",
      "Loss: 0.371296806467904\n",
      "RMSE train: 0.353900\tval: 0.557050\n",
      "MAE train: 0.269693\tval: 0.417071\n",
      "pearson_R train: 0.864215\tval: 0.607021\n",
      "Epoch: 599\n",
      "Loss: 0.37310461865531075\n",
      "RMSE train: 0.377277\tval: 0.598544\n",
      "MAE train: 0.296941\tval: 0.450346\n",
      "pearson_R train: 0.848814\tval: 0.582716\n",
      "Epoch: 600\n",
      "Loss: 0.37911806172794765\n",
      "RMSE train: 0.382873\tval: 0.570108\n",
      "MAE train: 0.296165\tval: 0.429811\n",
      "pearson_R train: 0.834539\tval: 0.606095\n",
      "Epoch: 601\n",
      "Loss: 0.38406408826510113\n",
      "RMSE train: 0.383435\tval: 0.565070\n",
      "MAE train: 0.295443\tval: 0.433267\n",
      "pearson_R train: 0.833804\tval: 0.587308\n",
      "Epoch: 602\n",
      "Loss: 0.38982633087370133\n",
      "RMSE train: 0.373260\tval: 0.572694\n",
      "MAE train: 0.284959\tval: 0.435826\n",
      "pearson_R train: 0.853779\tval: 0.579108\n",
      "Epoch: 603\n",
      "Loss: 0.37166332205136615\n",
      "RMSE train: 0.386227\tval: 0.597244\n",
      "MAE train: 0.298989\tval: 0.449200\n",
      "pearson_R train: 0.831433\tval: 0.575044\n",
      "Epoch: 604\n",
      "Loss: 0.36946552991867065\n",
      "RMSE train: 0.390173\tval: 0.586644\n",
      "MAE train: 0.298359\tval: 0.440648\n",
      "pearson_R train: 0.845676\tval: 0.574788\n",
      "Epoch: 605\n",
      "Loss: 0.38041292627652484\n",
      "RMSE train: 0.520207\tval: 0.727730\n",
      "MAE train: 0.410366\tval: 0.558533\n",
      "pearson_R train: 0.792929\tval: 0.511026\n",
      "Epoch: 606\n",
      "Loss: 0.3829665581385295\n",
      "RMSE train: 0.422540\tval: 0.590059\n",
      "MAE train: 0.323443\tval: 0.445521\n",
      "pearson_R train: 0.828114\tval: 0.589824\n",
      "Epoch: 607\n",
      "Loss: 0.3862348132663303\n",
      "RMSE train: 0.353967\tval: 0.576885\n",
      "MAE train: 0.276015\tval: 0.437860\n",
      "pearson_R train: 0.860285\tval: 0.582211\n",
      "Epoch: 608\n",
      "Loss: 0.35962052477730644\n",
      "RMSE train: 0.351889\tval: 0.579886\n",
      "MAE train: 0.273449\tval: 0.440105\n",
      "pearson_R train: 0.862037\tval: 0.594877\n",
      "Epoch: 609\n",
      "Loss: 0.3681255943245358\n",
      "RMSE train: 0.349461\tval: 0.579782\n",
      "MAE train: 0.269997\tval: 0.440424\n",
      "pearson_R train: 0.864214\tval: 0.591452\n",
      "Epoch: 610\n",
      "Loss: 0.3860999412006802\n",
      "RMSE train: 0.356214\tval: 0.580785\n",
      "MAE train: 0.276763\tval: 0.444002\n",
      "pearson_R train: 0.858389\tval: 0.592268\n",
      "Epoch: 611\n",
      "Loss: 0.3724886377652486\n",
      "RMSE train: 0.442385\tval: 0.608863\n",
      "MAE train: 0.334955\tval: 0.461910\n",
      "pearson_R train: 0.818330\tval: 0.564767\n",
      "Epoch: 612\n",
      "Loss: 0.36792873673968846\n",
      "RMSE train: 0.361306\tval: 0.595000\n",
      "MAE train: 0.282499\tval: 0.441589\n",
      "pearson_R train: 0.854648\tval: 0.587546\n",
      "Epoch: 613\n",
      "Loss: 0.3685019082493252\n",
      "RMSE train: 0.362573\tval: 0.585811\n",
      "MAE train: 0.277355\tval: 0.434362\n",
      "pearson_R train: 0.856331\tval: 0.580072\n",
      "Epoch: 614\n",
      "Loss: 0.3846232394377391\n",
      "RMSE train: 0.357336\tval: 0.572334\n",
      "MAE train: 0.274911\tval: 0.426981\n",
      "pearson_R train: 0.861306\tval: 0.592989\n",
      "Epoch: 615\n",
      "Loss: 0.372020896938112\n",
      "RMSE train: 0.347521\tval: 0.579694\n",
      "MAE train: 0.271906\tval: 0.434558\n",
      "pearson_R train: 0.866855\tval: 0.583087\n",
      "Epoch: 616\n",
      "Loss: 0.35410060154067147\n",
      "RMSE train: 0.413922\tval: 0.618738\n",
      "MAE train: 0.318844\tval: 0.459380\n",
      "pearson_R train: 0.815365\tval: 0.585652\n",
      "Epoch: 617\n",
      "Loss: 0.40011026461919147\n",
      "RMSE train: 0.385755\tval: 0.609362\n",
      "MAE train: 0.304646\tval: 0.462835\n",
      "pearson_R train: 0.841991\tval: 0.557891\n",
      "Epoch: 618\n",
      "Loss: 0.3885719378789266\n",
      "RMSE train: 0.366261\tval: 0.561429\n",
      "MAE train: 0.281131\tval: 0.423291\n",
      "pearson_R train: 0.852830\tval: 0.601719\n",
      "Epoch: 619\n",
      "Loss: 0.3739614288012187\n",
      "RMSE train: 0.491544\tval: 0.607007\n",
      "MAE train: 0.376056\tval: 0.457572\n",
      "pearson_R train: 0.785479\tval: 0.578980\n",
      "Epoch: 620\n",
      "Loss: 0.37702668375439113\n",
      "RMSE train: 0.376810\tval: 0.576562\n",
      "MAE train: 0.288617\tval: 0.435693\n",
      "pearson_R train: 0.850483\tval: 0.585235\n",
      "Epoch: 621\n",
      "Loss: 0.3742448687553406\n",
      "RMSE train: 0.368264\tval: 0.595358\n",
      "MAE train: 0.286197\tval: 0.454036\n",
      "pearson_R train: 0.849124\tval: 0.583239\n",
      "Epoch: 622\n",
      "Loss: 0.37069333924187553\n",
      "RMSE train: 0.357348\tval: 0.579303\n",
      "MAE train: 0.272604\tval: 0.430243\n",
      "pearson_R train: 0.863823\tval: 0.575176\n",
      "Epoch: 623\n",
      "Loss: 0.3677404622236888\n",
      "RMSE train: 0.366145\tval: 0.599765\n",
      "MAE train: 0.284730\tval: 0.455180\n",
      "pearson_R train: 0.853869\tval: 0.586150\n",
      "Epoch: 624\n",
      "Loss: 0.3838013708591461\n",
      "RMSE train: 0.364149\tval: 0.589861\n",
      "MAE train: 0.279941\tval: 0.441749\n",
      "pearson_R train: 0.858915\tval: 0.566621\n",
      "Epoch: 625\n",
      "Loss: 0.36735013458463883\n",
      "RMSE train: 0.396019\tval: 0.648460\n",
      "MAE train: 0.311378\tval: 0.485910\n",
      "pearson_R train: 0.849217\tval: 0.565916\n",
      "Epoch: 626\n",
      "Loss: 0.3823709587256114\n",
      "RMSE train: 0.380276\tval: 0.593139\n",
      "MAE train: 0.294377\tval: 0.445831\n",
      "pearson_R train: 0.850269\tval: 0.583190\n",
      "Epoch: 627\n",
      "Loss: 0.34037229749891496\n",
      "RMSE train: 0.398074\tval: 0.627354\n",
      "MAE train: 0.312354\tval: 0.469867\n",
      "pearson_R train: 0.847675\tval: 0.592923\n",
      "Epoch: 628\n",
      "Loss: 0.36049604415893555\n",
      "RMSE train: 0.343395\tval: 0.581325\n",
      "MAE train: 0.265183\tval: 0.430779\n",
      "pearson_R train: 0.868764\tval: 0.584362\n",
      "Epoch: 629\n",
      "Loss: 0.37957093781895107\n",
      "RMSE train: 0.382275\tval: 0.577894\n",
      "MAE train: 0.293951\tval: 0.438730\n",
      "pearson_R train: 0.840277\tval: 0.581199\n",
      "Epoch: 630\n",
      "Loss: 0.3533140222231547\n",
      "RMSE train: 0.361584\tval: 0.569706\n",
      "MAE train: 0.280230\tval: 0.436477\n",
      "pearson_R train: 0.857651\tval: 0.571621\n",
      "Epoch: 631\n",
      "Loss: 0.37352068225542706\n",
      "RMSE train: 0.337553\tval: 0.596527\n",
      "MAE train: 0.260355\tval: 0.444630\n",
      "pearson_R train: 0.874370\tval: 0.580983\n",
      "Epoch: 632\n",
      "Loss: 0.3470782670709822\n",
      "RMSE train: 0.348885\tval: 0.571020\n",
      "MAE train: 0.267203\tval: 0.430349\n",
      "pearson_R train: 0.868009\tval: 0.602029\n",
      "Epoch: 633\n",
      "Loss: 0.36240560478634304\n",
      "RMSE train: 0.392976\tval: 0.586265\n",
      "MAE train: 0.300617\tval: 0.433759\n",
      "pearson_R train: 0.842033\tval: 0.606369\n",
      "Epoch: 634\n",
      "Loss: 0.3550787766774495\n",
      "RMSE train: 0.371429\tval: 0.568234\n",
      "MAE train: 0.289679\tval: 0.421356\n",
      "pearson_R train: 0.852220\tval: 0.603133\n",
      "Epoch: 635\n",
      "Loss: 0.3557611273394691\n",
      "RMSE train: 0.413715\tval: 0.604527\n",
      "MAE train: 0.316158\tval: 0.450001\n",
      "pearson_R train: 0.847174\tval: 0.567678\n",
      "Epoch: 636\n",
      "Loss: 0.37291286720169914\n",
      "RMSE train: 0.431872\tval: 0.642445\n",
      "MAE train: 0.338592\tval: 0.501867\n",
      "pearson_R train: 0.801649\tval: 0.515273\n",
      "Epoch: 637\n",
      "Loss: 0.3573295606507195\n",
      "RMSE train: 0.360855\tval: 0.587852\n",
      "MAE train: 0.281573\tval: 0.450805\n",
      "pearson_R train: 0.863174\tval: 0.595264\n",
      "Epoch: 638\n",
      "Loss: 0.3589339057604472\n",
      "RMSE train: 0.345882\tval: 0.599661\n",
      "MAE train: 0.270397\tval: 0.450312\n",
      "pearson_R train: 0.868395\tval: 0.584814\n",
      "Epoch: 639\n",
      "Loss: 0.35573573576079476\n",
      "RMSE train: 0.431550\tval: 0.602031\n",
      "MAE train: 0.330788\tval: 0.459085\n",
      "pearson_R train: 0.836504\tval: 0.589374\n",
      "Epoch: 640\n",
      "Loss: 0.36802377965715194\n",
      "RMSE train: 0.388311\tval: 0.611239\n",
      "MAE train: 0.303770\tval: 0.473306\n",
      "pearson_R train: 0.850833\tval: 0.589542\n",
      "Epoch: 641\n",
      "Loss: 0.37529487080044216\n",
      "RMSE train: 0.356004\tval: 0.574586\n",
      "MAE train: 0.272801\tval: 0.428902\n",
      "pearson_R train: 0.865696\tval: 0.579575\n",
      "Epoch: 642\n",
      "Loss: 0.3634350862767961\n",
      "RMSE train: 0.341425\tval: 0.594666\n",
      "MAE train: 0.263155\tval: 0.442829\n",
      "pearson_R train: 0.870758\tval: 0.579804\n",
      "Epoch: 643\n",
      "Loss: 0.339953256977929\n",
      "RMSE train: 0.448708\tval: 0.603441\n",
      "MAE train: 0.343174\tval: 0.451817\n",
      "pearson_R train: 0.822050\tval: 0.575830\n",
      "Epoch: 644\n",
      "Loss: 0.349577354060279\n",
      "RMSE train: 0.333627\tval: 0.589923\n",
      "MAE train: 0.259176\tval: 0.442332\n",
      "pearson_R train: 0.877114\tval: 0.588550\n",
      "Epoch: 645\n",
      "Loss: 0.3466264572408464\n",
      "RMSE train: 0.337882\tval: 0.590126\n",
      "MAE train: 0.260281\tval: 0.447201\n",
      "pearson_R train: 0.876336\tval: 0.594953\n",
      "Epoch: 646\n",
      "Loss: 0.34870361619525486\n",
      "RMSE train: 0.348873\tval: 0.579103\n",
      "MAE train: 0.269352\tval: 0.435809\n",
      "pearson_R train: 0.864830\tval: 0.578388\n",
      "Epoch: 647\n",
      "Loss: 0.3465391960408952\n",
      "RMSE train: 0.378201\tval: 0.588764\n",
      "MAE train: 0.288684\tval: 0.438767\n",
      "pearson_R train: 0.849330\tval: 0.582787\n",
      "Epoch: 648\n",
      "Loss: 0.34603256980578107\n",
      "RMSE train: 0.431428\tval: 0.595990\n",
      "MAE train: 0.331318\tval: 0.444852\n",
      "pearson_R train: 0.844907\tval: 0.583367\n",
      "Epoch: 649\n",
      "Loss: 0.3433716297149658\n",
      "RMSE train: 0.371427\tval: 0.616094\n",
      "MAE train: 0.294279\tval: 0.471167\n",
      "pearson_R train: 0.848002\tval: 0.553123\n",
      "Epoch: 650\n",
      "Loss: 0.33873851431740654\n",
      "RMSE train: 0.340231\tval: 0.591993\n",
      "MAE train: 0.264910\tval: 0.451430\n",
      "pearson_R train: 0.871995\tval: 0.580030\n",
      "Epoch: 651\n",
      "Loss: 0.3528872893916236\n",
      "RMSE train: 0.355372\tval: 0.591352\n",
      "MAE train: 0.273974\tval: 0.442691\n",
      "pearson_R train: 0.868874\tval: 0.583980\n",
      "Epoch: 652\n",
      "Loss: 0.3524749080340068\n",
      "RMSE train: 0.345949\tval: 0.572121\n",
      "MAE train: 0.268716\tval: 0.426716\n",
      "pearson_R train: 0.874330\tval: 0.592654\n",
      "Epoch: 653\n",
      "Loss: 0.33991704715622795\n",
      "RMSE train: 0.330908\tval: 0.579154\n",
      "MAE train: 0.254243\tval: 0.435162\n",
      "pearson_R train: 0.881116\tval: 0.607924\n",
      "Epoch: 654\n",
      "Loss: 0.3396383027235667\n",
      "RMSE train: 0.402423\tval: 0.584564\n",
      "MAE train: 0.304983\tval: 0.436945\n",
      "pearson_R train: 0.856151\tval: 0.564189\n",
      "Epoch: 655\n",
      "Loss: 0.35106731123394436\n",
      "RMSE train: 0.402553\tval: 0.584160\n",
      "MAE train: 0.308400\tval: 0.437755\n",
      "pearson_R train: 0.840917\tval: 0.574419\n",
      "Epoch: 656\n",
      "Loss: 0.34173285630014205\n",
      "RMSE train: 0.349941\tval: 0.583792\n",
      "MAE train: 0.267178\tval: 0.438744\n",
      "pearson_R train: 0.867422\tval: 0.595066\n",
      "Epoch: 657\n",
      "Loss: 0.3535550634066264\n",
      "RMSE train: 0.402047\tval: 0.627509\n",
      "MAE train: 0.317548\tval: 0.486173\n",
      "pearson_R train: 0.837234\tval: 0.574283\n",
      "Epoch: 658\n",
      "Loss: 0.3347673813501994\n",
      "RMSE train: 0.342463\tval: 0.575761\n",
      "MAE train: 0.267702\tval: 0.433824\n",
      "pearson_R train: 0.873915\tval: 0.605612\n",
      "Epoch: 659\n",
      "Loss: 0.3166047930717468\n",
      "RMSE train: 0.377622\tval: 0.583195\n",
      "MAE train: 0.284235\tval: 0.433617\n",
      "pearson_R train: 0.852516\tval: 0.597048\n",
      "Epoch: 660\n",
      "Loss: 0.327915499607722\n",
      "RMSE train: 0.362576\tval: 0.604765\n",
      "MAE train: 0.278307\tval: 0.449762\n",
      "pearson_R train: 0.856373\tval: 0.568284\n",
      "Epoch: 661\n",
      "Loss: 0.33911741111013627\n",
      "RMSE train: 0.325927\tval: 0.586305\n",
      "MAE train: 0.255020\tval: 0.444388\n",
      "pearson_R train: 0.883670\tval: 0.585311\n",
      "Epoch: 662\n",
      "Loss: 0.3294401930438148\n",
      "RMSE train: 0.328691\tval: 0.577297\n",
      "MAE train: 0.255470\tval: 0.434300\n",
      "pearson_R train: 0.881790\tval: 0.601679\n",
      "Epoch: 663\n",
      "Loss: 0.32653826144006515\n",
      "RMSE train: 0.342274\tval: 0.580583\n",
      "MAE train: 0.263816\tval: 0.436241\n",
      "pearson_R train: 0.876198\tval: 0.602264\n",
      "Epoch: 664\n",
      "Loss: 0.3112359493970871\n",
      "RMSE train: 0.374377\tval: 0.631982\n",
      "MAE train: 0.289256\tval: 0.471314\n",
      "pearson_R train: 0.859005\tval: 0.567798\n",
      "Epoch: 665\n",
      "Loss: 0.33413394623332554\n",
      "RMSE train: 0.347575\tval: 0.582382\n",
      "MAE train: 0.266217\tval: 0.437545\n",
      "pearson_R train: 0.865345\tval: 0.590064\n",
      "Epoch: 666\n",
      "Loss: 0.33356106612417435\n",
      "RMSE train: 0.348946\tval: 0.567451\n",
      "MAE train: 0.266792\tval: 0.425947\n",
      "pearson_R train: 0.876065\tval: 0.615968\n",
      "Epoch: 667\n",
      "Loss: 0.359081506729126\n",
      "RMSE train: 0.355008\tval: 0.562944\n",
      "MAE train: 0.271386\tval: 0.414771\n",
      "pearson_R train: 0.869580\tval: 0.600039\n",
      "Epoch: 668\n",
      "Loss: 0.3445274333159129\n",
      "RMSE train: 0.346701\tval: 0.580127\n",
      "MAE train: 0.267323\tval: 0.431964\n",
      "pearson_R train: 0.874921\tval: 0.587013\n",
      "Epoch: 669\n",
      "Loss: 0.335464671254158\n",
      "RMSE train: 0.334353\tval: 0.583659\n",
      "MAE train: 0.261076\tval: 0.441914\n",
      "pearson_R train: 0.876403\tval: 0.585897\n",
      "Epoch: 670\n",
      "Loss: 0.3506058255831401\n",
      "RMSE train: 0.334405\tval: 0.582921\n",
      "MAE train: 0.257334\tval: 0.434298\n",
      "pearson_R train: 0.875958\tval: 0.592066\n",
      "Epoch: 671\n",
      "Loss: 0.32843029167917037\n",
      "RMSE train: 0.337635\tval: 0.587202\n",
      "MAE train: 0.261103\tval: 0.433595\n",
      "pearson_R train: 0.885394\tval: 0.598744\n",
      "Epoch: 672\n",
      "Loss: 0.34507321649127537\n",
      "RMSE train: 0.322185\tval: 0.583378\n",
      "MAE train: 0.248167\tval: 0.430480\n",
      "pearson_R train: 0.886190\tval: 0.603214\n",
      "Epoch: 673\n",
      "Loss: 0.3393617669741313\n",
      "RMSE train: 0.339771\tval: 0.571360\n",
      "MAE train: 0.261929\tval: 0.424230\n",
      "pearson_R train: 0.890259\tval: 0.602453\n",
      "Epoch: 674\n",
      "Loss: 0.32146363457043964\n",
      "RMSE train: 0.352828\tval: 0.589968\n",
      "MAE train: 0.274787\tval: 0.453546\n",
      "pearson_R train: 0.868186\tval: 0.599249\n",
      "Epoch: 675\n",
      "Loss: 0.32979731096161735\n",
      "RMSE train: 0.330055\tval: 0.576454\n",
      "MAE train: 0.254286\tval: 0.433039\n",
      "pearson_R train: 0.881450\tval: 0.583386\n",
      "Epoch: 676\n",
      "Loss: 0.31742095616128707\n",
      "RMSE train: 0.372396\tval: 0.605021\n",
      "MAE train: 0.285073\tval: 0.453149\n",
      "pearson_R train: 0.866018\tval: 0.579877\n",
      "Epoch: 677\n",
      "Loss: 0.32984991868336994\n",
      "RMSE train: 0.333220\tval: 0.573835\n",
      "MAE train: 0.255249\tval: 0.427404\n",
      "pearson_R train: 0.884928\tval: 0.589906\n",
      "Epoch: 678\n",
      "Loss: 0.33350514041052925\n",
      "RMSE train: 0.343212\tval: 0.571036\n",
      "MAE train: 0.262412\tval: 0.428635\n",
      "pearson_R train: 0.872055\tval: 0.598516\n",
      "Epoch: 679\n",
      "Loss: 0.344109488858117\n",
      "RMSE train: 0.330316\tval: 0.585688\n",
      "MAE train: 0.254204\tval: 0.439763\n",
      "pearson_R train: 0.880487\tval: 0.586793\n",
      "Epoch: 680\n",
      "Loss: 0.34499914778603447\n",
      "RMSE train: 0.359223\tval: 0.583290\n",
      "MAE train: 0.275713\tval: 0.443275\n",
      "pearson_R train: 0.861239\tval: 0.582219\n",
      "Epoch: 681\n",
      "Loss: 0.3380376166767544\n",
      "RMSE train: 0.391650\tval: 0.638243\n",
      "MAE train: 0.309489\tval: 0.488219\n",
      "pearson_R train: 0.848576\tval: 0.547114\n",
      "Epoch: 682\n",
      "Loss: 0.32985198828909135\n",
      "RMSE train: 0.345147\tval: 0.615416\n",
      "MAE train: 0.268101\tval: 0.453990\n",
      "pearson_R train: 0.874380\tval: 0.592163\n",
      "Epoch: 683\n",
      "Loss: 0.3092297977871365\n",
      "RMSE train: 0.330474\tval: 0.586480\n",
      "MAE train: 0.253660\tval: 0.432352\n",
      "pearson_R train: 0.884618\tval: 0.589569\n",
      "Epoch: 684\n",
      "Loss: 0.3219876488049825\n",
      "RMSE train: 0.437852\tval: 0.611488\n",
      "MAE train: 0.334407\tval: 0.455583\n",
      "pearson_R train: 0.807717\tval: 0.548122\n",
      "Epoch: 685\n",
      "Loss: 0.33143091864056057\n",
      "RMSE train: 0.437464\tval: 0.614557\n",
      "MAE train: 0.331791\tval: 0.455551\n",
      "pearson_R train: 0.839329\tval: 0.557155\n",
      "Epoch: 686\n",
      "Loss: 0.3350829548305935\n",
      "RMSE train: 0.330299\tval: 0.557429\n",
      "MAE train: 0.250112\tval: 0.419028\n",
      "pearson_R train: 0.883668\tval: 0.601382\n",
      "Epoch: 687\n",
      "Loss: 0.32543469137615627\n",
      "RMSE train: 0.327628\tval: 0.575983\n",
      "MAE train: 0.253199\tval: 0.436075\n",
      "pearson_R train: 0.882189\tval: 0.584165\n",
      "Epoch: 688\n",
      "Loss: 0.3164755304654439\n",
      "RMSE train: 0.321229\tval: 0.585294\n",
      "MAE train: 0.249283\tval: 0.440478\n",
      "pearson_R train: 0.888174\tval: 0.587261\n",
      "Epoch: 689\n",
      "Loss: 0.3275099065568712\n",
      "RMSE train: 0.351490\tval: 0.581568\n",
      "MAE train: 0.269696\tval: 0.435685\n",
      "pearson_R train: 0.878423\tval: 0.587873\n",
      "Epoch: 690\n",
      "Loss: 0.3211386634243859\n",
      "RMSE train: 0.343795\tval: 0.605341\n",
      "MAE train: 0.267107\tval: 0.454370\n",
      "pearson_R train: 0.880979\tval: 0.611592\n",
      "Epoch: 691\n",
      "Loss: 0.32591233981980217\n",
      "RMSE train: 0.343206\tval: 0.568179\n",
      "MAE train: 0.262912\tval: 0.424395\n",
      "pearson_R train: 0.887310\tval: 0.597876\n",
      "Epoch: 692\n",
      "Loss: 0.3133266998661889\n",
      "RMSE train: 0.377572\tval: 0.645066\n",
      "MAE train: 0.296735\tval: 0.488860\n",
      "pearson_R train: 0.879451\tval: 0.593830\n",
      "Epoch: 693\n",
      "Loss: 0.31212590138117474\n",
      "RMSE train: 0.321428\tval: 0.582039\n",
      "MAE train: 0.247638\tval: 0.440053\n",
      "pearson_R train: 0.885985\tval: 0.589074\n",
      "Epoch: 694\n",
      "Loss: 0.30405926869975197\n",
      "RMSE train: 0.354938\tval: 0.591175\n",
      "MAE train: 0.273304\tval: 0.441224\n",
      "pearson_R train: 0.887270\tval: 0.579131\n",
      "Epoch: 695\n",
      "Loss: 0.3108076983027988\n",
      "RMSE train: 0.319295\tval: 0.583558\n",
      "MAE train: 0.249112\tval: 0.438347\n",
      "pearson_R train: 0.889932\tval: 0.595973\n",
      "Epoch: 696\n",
      "Loss: 0.3297497000959184\n",
      "RMSE train: 0.327424\tval: 0.586507\n",
      "MAE train: 0.250956\tval: 0.430119\n",
      "pearson_R train: 0.889739\tval: 0.589224\n",
      "Epoch: 697\n",
      "Loss: 0.30955345763100517\n",
      "RMSE train: 0.324424\tval: 0.582853\n",
      "MAE train: 0.248649\tval: 0.430538\n",
      "pearson_R train: 0.889732\tval: 0.587043\n",
      "Epoch: 698\n",
      "Loss: 0.3236187828911675\n",
      "RMSE train: 0.348249\tval: 0.630225\n",
      "MAE train: 0.273784\tval: 0.467162\n",
      "pearson_R train: 0.879178\tval: 0.575003\n",
      "Epoch: 699\n",
      "Loss: 0.3250463472472297\n",
      "RMSE train: 0.344011\tval: 0.573632\n",
      "MAE train: 0.261790\tval: 0.423413\n",
      "pearson_R train: 0.881791\tval: 0.579177\n",
      "Epoch: 700\n",
      "Loss: 0.3242144650883145\n",
      "RMSE train: 0.386446\tval: 0.610625\n",
      "MAE train: 0.301865\tval: 0.456197\n",
      "pearson_R train: 0.877420\tval: 0.584787\n",
      "Epoch: 701\n",
      "Loss: 0.3067765401469337\n",
      "RMSE train: 0.398878\tval: 0.607633\n",
      "MAE train: 0.304931\tval: 0.455313\n",
      "pearson_R train: 0.851544\tval: 0.576280\n",
      "Epoch: 702\n",
      "Loss: 0.30483175979720223\n",
      "RMSE train: 0.323608\tval: 0.593244\n",
      "MAE train: 0.250376\tval: 0.445363\n",
      "pearson_R train: 0.885441\tval: 0.572814\n",
      "Epoch: 703\n",
      "Loss: 0.3257984187867906\n",
      "RMSE train: 0.327760\tval: 0.604018\n",
      "MAE train: 0.254760\tval: 0.458295\n",
      "pearson_R train: 0.882413\tval: 0.575390\n",
      "Epoch: 704\n",
      "Loss: 0.33015023668607074\n",
      "RMSE train: 0.349763\tval: 0.589584\n",
      "MAE train: 0.266259\tval: 0.439293\n",
      "pearson_R train: 0.880414\tval: 0.585777\n",
      "Epoch: 705\n",
      "Loss: 0.3082308206293318\n",
      "RMSE train: 0.323988\tval: 0.595550\n",
      "MAE train: 0.251723\tval: 0.450253\n",
      "pearson_R train: 0.887137\tval: 0.601739\n",
      "Epoch: 706\n",
      "Loss: 0.30754273467593723\n",
      "RMSE train: 0.337981\tval: 0.611248\n",
      "MAE train: 0.261409\tval: 0.461800\n",
      "pearson_R train: 0.887478\tval: 0.587545\n",
      "Epoch: 707\n",
      "Loss: 0.342342389954461\n",
      "RMSE train: 0.319052\tval: 0.584444\n",
      "MAE train: 0.246578\tval: 0.440628\n",
      "pearson_R train: 0.888963\tval: 0.608161\n",
      "Epoch: 708\n",
      "Loss: 0.2976981997489929\n",
      "RMSE train: 0.376731\tval: 0.589770\n",
      "MAE train: 0.294686\tval: 0.444805\n",
      "pearson_R train: 0.890033\tval: 0.601356\n",
      "Epoch: 709\n",
      "Loss: 0.3132090171178182\n",
      "RMSE train: 0.309936\tval: 0.589173\n",
      "MAE train: 0.241694\tval: 0.443994\n",
      "pearson_R train: 0.898439\tval: 0.602252\n",
      "Epoch: 710\n",
      "Loss: 0.3081393274996016\n",
      "RMSE train: 0.321966\tval: 0.597975\n",
      "MAE train: 0.249220\tval: 0.452801\n",
      "pearson_R train: 0.893894\tval: 0.584656\n",
      "Epoch: 711\n",
      "Loss: 0.31050536698765224\n",
      "RMSE train: 0.316795\tval: 0.585347\n",
      "MAE train: 0.243593\tval: 0.431813\n",
      "pearson_R train: 0.897656\tval: 0.586402\n",
      "Epoch: 712\n",
      "Loss: 0.29983602298630607\n",
      "RMSE train: 0.335006\tval: 0.582357\n",
      "MAE train: 0.259728\tval: 0.445384\n",
      "pearson_R train: 0.885341\tval: 0.617520\n",
      "Epoch: 713\n",
      "Loss: 0.32708802156978184\n",
      "RMSE train: 0.334820\tval: 0.571607\n",
      "MAE train: 0.254618\tval: 0.429583\n",
      "pearson_R train: 0.886308\tval: 0.606978\n",
      "Epoch: 714\n",
      "Loss: 0.3126057783762614\n",
      "RMSE train: 0.333468\tval: 0.594429\n",
      "MAE train: 0.260485\tval: 0.452674\n",
      "pearson_R train: 0.877163\tval: 0.574139\n",
      "Epoch: 715\n",
      "Loss: 0.3059352735678355\n",
      "RMSE train: 0.311670\tval: 0.583948\n",
      "MAE train: 0.240180\tval: 0.436601\n",
      "pearson_R train: 0.894121\tval: 0.605684\n",
      "Epoch: 716\n",
      "Loss: 0.3007212844159868\n",
      "RMSE train: 0.340926\tval: 0.576652\n",
      "MAE train: 0.263002\tval: 0.438016\n",
      "pearson_R train: 0.873593\tval: 0.602506\n",
      "Epoch: 717\n",
      "Loss: 0.30266792906655204\n",
      "RMSE train: 0.301685\tval: 0.591031\n",
      "MAE train: 0.233902\tval: 0.446157\n",
      "pearson_R train: 0.902425\tval: 0.592496\n",
      "Epoch: 718\n",
      "Loss: 0.31255926026238334\n",
      "RMSE train: 0.372513\tval: 0.623462\n",
      "MAE train: 0.288392\tval: 0.468697\n",
      "pearson_R train: 0.851672\tval: 0.544317\n",
      "Epoch: 719\n",
      "Loss: 0.30489515264829\n",
      "RMSE train: 0.366891\tval: 0.605867\n",
      "MAE train: 0.279975\tval: 0.450336\n",
      "pearson_R train: 0.869125\tval: 0.573775\n",
      "Epoch: 720\n",
      "Loss: 0.31996399495336747\n",
      "RMSE train: 0.342724\tval: 0.587219\n",
      "MAE train: 0.264359\tval: 0.436021\n",
      "pearson_R train: 0.878369\tval: 0.597452\n",
      "Epoch: 721\n",
      "Loss: 0.3160202105840047\n",
      "RMSE train: 0.353841\tval: 0.642711\n",
      "MAE train: 0.281421\tval: 0.488131\n",
      "pearson_R train: 0.892381\tval: 0.571212\n",
      "Epoch: 722\n",
      "Loss: 0.2953062785996331\n",
      "RMSE train: 0.388873\tval: 0.580028\n",
      "MAE train: 0.293262\tval: 0.437515\n",
      "pearson_R train: 0.859519\tval: 0.593369\n",
      "Epoch: 723\n",
      "Loss: 0.3119785090287526\n",
      "RMSE train: 0.321157\tval: 0.592706\n",
      "MAE train: 0.250133\tval: 0.442550\n",
      "pearson_R train: 0.887054\tval: 0.586803\n",
      "Epoch: 724\n",
      "Loss: 0.32036446862750584\n",
      "RMSE train: 0.349707\tval: 0.593165\n",
      "MAE train: 0.269272\tval: 0.449589\n",
      "pearson_R train: 0.864434\tval: 0.561559\n",
      "Epoch: 725\n",
      "Loss: 0.3080986009703742\n",
      "RMSE train: 0.321589\tval: 0.598176\n",
      "MAE train: 0.244416\tval: 0.450325\n",
      "pearson_R train: 0.891758\tval: 0.576624\n",
      "Epoch: 726\n",
      "Loss: 0.2947682754860984\n",
      "RMSE train: 0.312913\tval: 0.590938\n",
      "MAE train: 0.240100\tval: 0.440269\n",
      "pearson_R train: 0.899333\tval: 0.578074\n",
      "Epoch: 727\n",
      "Loss: 0.32452843255466884\n",
      "RMSE train: 0.325779\tval: 0.589578\n",
      "MAE train: 0.252461\tval: 0.448460\n",
      "pearson_R train: 0.884714\tval: 0.604889\n",
      "Epoch: 728\n",
      "Loss: 0.31731445259518093\n",
      "RMSE train: 0.318446\tval: 0.610765\n",
      "MAE train: 0.245426\tval: 0.468007\n",
      "pearson_R train: 0.892051\tval: 0.576566\n",
      "Epoch: 729\n",
      "Loss: 0.30630045466952854\n",
      "RMSE train: 0.314945\tval: 0.586404\n",
      "MAE train: 0.243660\tval: 0.438040\n",
      "pearson_R train: 0.896503\tval: 0.579867\n",
      "Epoch: 730\n",
      "Loss: 0.3250436716609531\n",
      "RMSE train: 0.392326\tval: 0.598171\n",
      "MAE train: 0.300824\tval: 0.448802\n",
      "pearson_R train: 0.868482\tval: 0.586298\n",
      "Epoch: 731\n",
      "Loss: 0.3217504852347904\n",
      "RMSE train: 0.331190\tval: 0.586322\n",
      "MAE train: 0.251269\tval: 0.438578\n",
      "pearson_R train: 0.898365\tval: 0.584468\n",
      "Epoch: 732\n",
      "Loss: 0.30477414859665763\n",
      "RMSE train: 0.349615\tval: 0.611356\n",
      "MAE train: 0.271824\tval: 0.469047\n",
      "pearson_R train: 0.867282\tval: 0.551292\n",
      "Epoch: 733\n",
      "Loss: 0.30621415707800126\n",
      "RMSE train: 0.289039\tval: 0.587699\n",
      "MAE train: 0.223082\tval: 0.436047\n",
      "pearson_R train: 0.909630\tval: 0.596439\n",
      "Epoch: 734\n",
      "Loss: 0.29962561859024894\n",
      "RMSE train: 0.309588\tval: 0.580652\n",
      "MAE train: 0.239402\tval: 0.435210\n",
      "pearson_R train: 0.899327\tval: 0.588861\n",
      "Epoch: 735\n",
      "Loss: 0.2952616645230187\n",
      "RMSE train: 0.316750\tval: 0.596645\n",
      "MAE train: 0.245917\tval: 0.448265\n",
      "pearson_R train: 0.894221\tval: 0.590667\n",
      "Epoch: 736\n",
      "Loss: 0.30196091532707214\n",
      "RMSE train: 0.304353\tval: 0.604314\n",
      "MAE train: 0.237510\tval: 0.458428\n",
      "pearson_R train: 0.902184\tval: 0.589046\n",
      "Epoch: 737\n",
      "Loss: 0.2919293873839908\n",
      "RMSE train: 0.324474\tval: 0.601594\n",
      "MAE train: 0.248917\tval: 0.450747\n",
      "pearson_R train: 0.886328\tval: 0.576383\n",
      "Epoch: 738\n",
      "Loss: 0.2909617457124922\n",
      "RMSE train: 0.296210\tval: 0.597022\n",
      "MAE train: 0.230132\tval: 0.446529\n",
      "pearson_R train: 0.906466\tval: 0.597482\n",
      "Epoch: 739\n",
      "Loss: 0.2969668441348606\n",
      "RMSE train: 0.307678\tval: 0.562623\n",
      "MAE train: 0.235316\tval: 0.414022\n",
      "pearson_R train: 0.897580\tval: 0.611609\n",
      "Epoch: 740\n",
      "Loss: 0.29816437098715043\n",
      "RMSE train: 0.324483\tval: 0.578303\n",
      "MAE train: 0.249506\tval: 0.427654\n",
      "pearson_R train: 0.902315\tval: 0.600059\n",
      "Epoch: 741\n",
      "Loss: 0.3054671022627089\n",
      "RMSE train: 0.326926\tval: 0.622756\n",
      "MAE train: 0.256713\tval: 0.461143\n",
      "pearson_R train: 0.888986\tval: 0.573180\n",
      "Epoch: 742\n",
      "Loss: 0.28920923007859123\n",
      "RMSE train: 0.323214\tval: 0.604474\n",
      "MAE train: 0.251608\tval: 0.450927\n",
      "pearson_R train: 0.888951\tval: 0.581661\n",
      "Epoch: 743\n",
      "Loss: 0.29440149996015763\n",
      "RMSE train: 0.300381\tval: 0.575212\n",
      "MAE train: 0.230663\tval: 0.430066\n",
      "pearson_R train: 0.907968\tval: 0.595198\n",
      "Epoch: 744\n",
      "Loss: 0.27753027776877087\n",
      "RMSE train: 0.302933\tval: 0.585783\n",
      "MAE train: 0.233620\tval: 0.443459\n",
      "pearson_R train: 0.902153\tval: 0.591564\n",
      "Epoch: 745\n",
      "Loss: 0.2663492626614041\n",
      "RMSE train: 0.309470\tval: 0.578286\n",
      "MAE train: 0.236125\tval: 0.427423\n",
      "pearson_R train: 0.896781\tval: 0.588141\n",
      "Epoch: 746\n",
      "Loss: 0.2911979589197371\n",
      "RMSE train: 0.293202\tval: 0.586537\n",
      "MAE train: 0.226842\tval: 0.440140\n",
      "pearson_R train: 0.908509\tval: 0.595427\n",
      "Epoch: 747\n",
      "Loss: 0.28496570885181427\n",
      "RMSE train: 0.341651\tval: 0.596923\n",
      "MAE train: 0.265961\tval: 0.444683\n",
      "pearson_R train: 0.895057\tval: 0.593325\n",
      "Epoch: 748\n",
      "Loss: 0.28718460930718315\n",
      "RMSE train: 0.292010\tval: 0.584281\n",
      "MAE train: 0.226692\tval: 0.434592\n",
      "pearson_R train: 0.907779\tval: 0.610255\n",
      "Epoch: 749\n",
      "Loss: 0.28117863337198895\n",
      "RMSE train: 0.317381\tval: 0.581731\n",
      "MAE train: 0.245019\tval: 0.437897\n",
      "pearson_R train: 0.889311\tval: 0.588050\n",
      "Epoch: 750\n",
      "Loss: 0.30435268249776626\n",
      "RMSE train: 0.319901\tval: 0.576266\n",
      "MAE train: 0.249357\tval: 0.423493\n",
      "pearson_R train: 0.904088\tval: 0.608923\n",
      "Epoch: 751\n",
      "Loss: 0.2765967117415534\n",
      "RMSE train: 0.311081\tval: 0.576907\n",
      "MAE train: 0.237494\tval: 0.426202\n",
      "pearson_R train: 0.902976\tval: 0.592249\n",
      "Epoch: 752\n",
      "Loss: 0.2764413108428319\n",
      "RMSE train: 0.292357\tval: 0.590303\n",
      "MAE train: 0.227703\tval: 0.440534\n",
      "pearson_R train: 0.908778\tval: 0.574507\n",
      "Epoch: 753\n",
      "Loss: 0.26456696457333034\n",
      "RMSE train: 0.291461\tval: 0.588927\n",
      "MAE train: 0.226211\tval: 0.439353\n",
      "pearson_R train: 0.910729\tval: 0.598230\n",
      "Epoch: 754\n",
      "Loss: 0.27491045163737404\n",
      "RMSE train: 0.295578\tval: 0.593291\n",
      "MAE train: 0.228728\tval: 0.445299\n",
      "pearson_R train: 0.905606\tval: 0.585606\n",
      "Epoch: 755\n",
      "Loss: 0.29277687271436054\n",
      "RMSE train: 0.345073\tval: 0.582919\n",
      "MAE train: 0.264313\tval: 0.434271\n",
      "pearson_R train: 0.882462\tval: 0.586947\n",
      "Epoch: 756\n",
      "Loss: 0.27295379009511733\n",
      "RMSE train: 0.290617\tval: 0.593908\n",
      "MAE train: 0.225591\tval: 0.444459\n",
      "pearson_R train: 0.910587\tval: 0.611454\n",
      "Epoch: 757\n",
      "Loss: 0.2900976108180152\n",
      "RMSE train: 0.309931\tval: 0.588943\n",
      "MAE train: 0.241180\tval: 0.439197\n",
      "pearson_R train: 0.909732\tval: 0.576272\n",
      "Epoch: 758\n",
      "Loss: 0.2619217468632592\n",
      "RMSE train: 0.286495\tval: 0.593467\n",
      "MAE train: 0.221380\tval: 0.447151\n",
      "pearson_R train: 0.912934\tval: 0.590869\n",
      "Epoch: 759\n",
      "Loss: 0.272834868894683\n",
      "RMSE train: 0.308430\tval: 0.616641\n",
      "MAE train: 0.240294\tval: 0.457152\n",
      "pearson_R train: 0.909577\tval: 0.600948\n",
      "Epoch: 760\n",
      "Loss: 0.28215986490249634\n",
      "RMSE train: 0.299369\tval: 0.584532\n",
      "MAE train: 0.232673\tval: 0.441912\n",
      "pearson_R train: 0.901953\tval: 0.594770\n",
      "Epoch: 761\n",
      "Loss: 0.27721892793973285\n",
      "RMSE train: 0.295524\tval: 0.605671\n",
      "MAE train: 0.227626\tval: 0.452537\n",
      "pearson_R train: 0.907155\tval: 0.589726\n",
      "Epoch: 762\n",
      "Loss: 0.27341371609105003\n",
      "RMSE train: 0.352584\tval: 0.588765\n",
      "MAE train: 0.271125\tval: 0.440839\n",
      "pearson_R train: 0.876527\tval: 0.599492\n",
      "Epoch: 763\n",
      "Loss: 0.2788750231266022\n",
      "RMSE train: 0.322693\tval: 0.612878\n",
      "MAE train: 0.249504\tval: 0.457644\n",
      "pearson_R train: 0.887877\tval: 0.564901\n",
      "Epoch: 764\n",
      "Loss: 0.2991933226585388\n",
      "RMSE train: 0.298334\tval: 0.610338\n",
      "MAE train: 0.232373\tval: 0.453236\n",
      "pearson_R train: 0.906853\tval: 0.582766\n",
      "Epoch: 765\n",
      "Loss: 0.2935333980454339\n",
      "RMSE train: 0.294958\tval: 0.584286\n",
      "MAE train: 0.226931\tval: 0.438321\n",
      "pearson_R train: 0.904972\tval: 0.592110\n",
      "Epoch: 766\n",
      "Loss: 0.271506956881947\n",
      "RMSE train: 0.318581\tval: 0.592479\n",
      "MAE train: 0.247345\tval: 0.438678\n",
      "pearson_R train: 0.894662\tval: 0.596017\n",
      "Epoch: 767\n",
      "Loss: 0.26621027787526447\n",
      "RMSE train: 0.332195\tval: 0.573265\n",
      "MAE train: 0.255648\tval: 0.425142\n",
      "pearson_R train: 0.899927\tval: 0.594273\n",
      "Epoch: 768\n",
      "Loss: 0.2781148420439826\n",
      "RMSE train: 0.334494\tval: 0.634898\n",
      "MAE train: 0.258578\tval: 0.483579\n",
      "pearson_R train: 0.892023\tval: 0.588836\n",
      "Epoch: 769\n",
      "Loss: 0.27911486228307086\n",
      "RMSE train: 0.293930\tval: 0.588032\n",
      "MAE train: 0.228302\tval: 0.442401\n",
      "pearson_R train: 0.910870\tval: 0.606238\n",
      "Epoch: 770\n",
      "Loss: 0.2737184382147259\n",
      "RMSE train: 0.298854\tval: 0.600050\n",
      "MAE train: 0.230205\tval: 0.450180\n",
      "pearson_R train: 0.905489\tval: 0.579423\n",
      "Epoch: 771\n",
      "Loss: 0.2826543119218614\n",
      "RMSE train: 0.297438\tval: 0.576667\n",
      "MAE train: 0.231228\tval: 0.433539\n",
      "pearson_R train: 0.908617\tval: 0.591855\n",
      "Epoch: 772\n",
      "Loss: 0.28930915395418805\n",
      "RMSE train: 0.306981\tval: 0.587991\n",
      "MAE train: 0.237735\tval: 0.452727\n",
      "pearson_R train: 0.896659\tval: 0.577752\n",
      "Epoch: 773\n",
      "Loss: 0.2743229866027832\n",
      "RMSE train: 0.289668\tval: 0.590424\n",
      "MAE train: 0.225380\tval: 0.445787\n",
      "pearson_R train: 0.913749\tval: 0.595739\n",
      "Epoch: 774\n",
      "Loss: 0.28388429350323147\n",
      "RMSE train: 0.326228\tval: 0.592927\n",
      "MAE train: 0.251923\tval: 0.443357\n",
      "pearson_R train: 0.889452\tval: 0.606957\n",
      "Epoch: 775\n",
      "Loss: 0.28906885782877606\n",
      "RMSE train: 0.326649\tval: 0.603598\n",
      "MAE train: 0.254558\tval: 0.447506\n",
      "pearson_R train: 0.893063\tval: 0.592338\n",
      "Epoch: 776\n",
      "Loss: 0.2742371956507365\n",
      "RMSE train: 0.301487\tval: 0.592202\n",
      "MAE train: 0.233184\tval: 0.447400\n",
      "pearson_R train: 0.901895\tval: 0.602892\n",
      "Epoch: 777\n",
      "Loss: 0.27785119745466447\n",
      "RMSE train: 0.283503\tval: 0.589708\n",
      "MAE train: 0.220723\tval: 0.445915\n",
      "pearson_R train: 0.913028\tval: 0.588684\n",
      "Epoch: 778\n",
      "Loss: 0.2687690125571357\n",
      "RMSE train: 0.300276\tval: 0.593512\n",
      "MAE train: 0.234442\tval: 0.447204\n",
      "pearson_R train: 0.913283\tval: 0.585123\n",
      "Epoch: 779\n",
      "Loss: 0.2725427382522159\n",
      "RMSE train: 0.271522\tval: 0.591810\n",
      "MAE train: 0.209677\tval: 0.440355\n",
      "pearson_R train: 0.922219\tval: 0.600468\n",
      "Epoch: 780\n",
      "Loss: 0.26830969585312736\n",
      "RMSE train: 0.288664\tval: 0.571206\n",
      "MAE train: 0.222267\tval: 0.419949\n",
      "pearson_R train: 0.909778\tval: 0.602248\n",
      "Epoch: 781\n",
      "Loss: 0.28240475058555603\n",
      "RMSE train: 0.306168\tval: 0.610250\n",
      "MAE train: 0.234655\tval: 0.459071\n",
      "pearson_R train: 0.907687\tval: 0.590548\n",
      "Epoch: 782\n",
      "Loss: 0.2750914345184962\n",
      "RMSE train: 0.290878\tval: 0.599643\n",
      "MAE train: 0.223427\tval: 0.451195\n",
      "pearson_R train: 0.910429\tval: 0.594974\n",
      "Epoch: 783\n",
      "Loss: 0.2691159463591046\n",
      "RMSE train: 0.304323\tval: 0.588453\n",
      "MAE train: 0.235423\tval: 0.444633\n",
      "pearson_R train: 0.904190\tval: 0.578473\n",
      "Epoch: 784\n",
      "Loss: 0.27388456298245323\n",
      "RMSE train: 0.264258\tval: 0.573185\n",
      "MAE train: 0.203857\tval: 0.436854\n",
      "pearson_R train: 0.924645\tval: 0.604563\n",
      "Epoch: 785\n",
      "Loss: 0.2664532876676983\n",
      "RMSE train: 0.293660\tval: 0.592182\n",
      "MAE train: 0.225309\tval: 0.442832\n",
      "pearson_R train: 0.909608\tval: 0.586887\n",
      "Epoch: 786\n",
      "Loss: 0.2764523708158069\n",
      "RMSE train: 0.273627\tval: 0.581418\n",
      "MAE train: 0.211045\tval: 0.431959\n",
      "pearson_R train: 0.920902\tval: 0.592399\n",
      "Epoch: 787\n",
      "Loss: 0.25494184924496543\n",
      "RMSE train: 0.281352\tval: 0.586089\n",
      "MAE train: 0.218280\tval: 0.443190\n",
      "pearson_R train: 0.916895\tval: 0.604814\n",
      "Epoch: 788\n",
      "Loss: 0.26180565026071334\n",
      "RMSE train: 0.307355\tval: 0.580509\n",
      "MAE train: 0.237292\tval: 0.435213\n",
      "pearson_R train: 0.911408\tval: 0.595934\n",
      "Epoch: 789\n",
      "Loss: 0.2691296438376109\n",
      "RMSE train: 0.300926\tval: 0.577084\n",
      "MAE train: 0.232421\tval: 0.433457\n",
      "pearson_R train: 0.904932\tval: 0.615422\n",
      "Epoch: 790\n",
      "Loss: 0.2748878283633126\n",
      "RMSE train: 0.295244\tval: 0.572164\n",
      "MAE train: 0.225944\tval: 0.427425\n",
      "pearson_R train: 0.910180\tval: 0.593370\n",
      "Epoch: 791\n",
      "Loss: 0.26514728864034015\n",
      "RMSE train: 0.281698\tval: 0.589387\n",
      "MAE train: 0.219770\tval: 0.438986\n",
      "pearson_R train: 0.920833\tval: 0.591517\n",
      "Epoch: 792\n",
      "Loss: 0.2559472040997611\n",
      "RMSE train: 0.269235\tval: 0.591187\n",
      "MAE train: 0.207443\tval: 0.443712\n",
      "pearson_R train: 0.923837\tval: 0.617828\n",
      "Epoch: 793\n",
      "Loss: 0.26156312558386063\n",
      "RMSE train: 0.313000\tval: 0.584676\n",
      "MAE train: 0.239777\tval: 0.440895\n",
      "pearson_R train: 0.903099\tval: 0.601625\n",
      "Epoch: 794\n",
      "Loss: 0.266580354836252\n",
      "RMSE train: 0.299500\tval: 0.600368\n",
      "MAE train: 0.231555\tval: 0.447409\n",
      "pearson_R train: 0.905706\tval: 0.585314\n",
      "Epoch: 795\n",
      "Loss: 0.2775339318646325\n",
      "RMSE train: 0.281290\tval: 0.600613\n",
      "MAE train: 0.217647\tval: 0.456531\n",
      "pearson_R train: 0.917075\tval: 0.579057\n",
      "Epoch: 796\n",
      "Loss: 0.2588034470876058\n",
      "RMSE train: 0.310923\tval: 0.612661\n",
      "MAE train: 0.241653\tval: 0.466976\n",
      "pearson_R train: 0.902543\tval: 0.591752\n",
      "Epoch: 797\n",
      "Loss: 0.2745874524116516\n",
      "RMSE train: 0.299854\tval: 0.570589\n",
      "MAE train: 0.228351\tval: 0.419912\n",
      "pearson_R train: 0.911548\tval: 0.592013\n",
      "Epoch: 798\n",
      "Loss: 0.2640426374144024\n",
      "RMSE train: 0.369456\tval: 0.662883\n",
      "MAE train: 0.285309\tval: 0.497207\n",
      "pearson_R train: 0.892322\tval: 0.577472\n",
      "Epoch: 799\n",
      "Loss: 0.27243953777684105\n",
      "RMSE train: 0.332849\tval: 0.578569\n",
      "MAE train: 0.254975\tval: 0.436511\n",
      "pearson_R train: 0.903631\tval: 0.604413\n",
      "Epoch: 800\n",
      "Loss: 0.2800661077102025\n",
      "RMSE train: 0.313666\tval: 0.616777\n",
      "MAE train: 0.242974\tval: 0.458279\n",
      "pearson_R train: 0.900316\tval: 0.592134\n",
      "Epoch: 801\n",
      "Loss: 0.2592478659417894\n",
      "RMSE train: 0.282515\tval: 0.602631\n",
      "MAE train: 0.217617\tval: 0.452807\n",
      "pearson_R train: 0.914625\tval: 0.589638\n",
      "Epoch: 802\n",
      "Loss: 0.26175600787003833\n",
      "RMSE train: 0.293705\tval: 0.604986\n",
      "MAE train: 0.227581\tval: 0.453834\n",
      "pearson_R train: 0.905935\tval: 0.548192\n",
      "Epoch: 803\n",
      "Loss: 0.2616606288486057\n",
      "RMSE train: 0.300927\tval: 0.588593\n",
      "MAE train: 0.233355\tval: 0.442901\n",
      "pearson_R train: 0.909601\tval: 0.580622\n",
      "Epoch: 804\n",
      "Loss: 0.2597939504517449\n",
      "RMSE train: 0.277224\tval: 0.615578\n",
      "MAE train: 0.213621\tval: 0.461789\n",
      "pearson_R train: 0.924068\tval: 0.598269\n",
      "Epoch: 805\n",
      "Loss: 0.2691642791032791\n",
      "RMSE train: 0.281691\tval: 0.572847\n",
      "MAE train: 0.216995\tval: 0.429556\n",
      "pearson_R train: 0.915123\tval: 0.605883\n",
      "Epoch: 806\n",
      "Loss: 0.2523442986938689\n",
      "RMSE train: 0.292251\tval: 0.588178\n",
      "MAE train: 0.225460\tval: 0.431837\n",
      "pearson_R train: 0.917542\tval: 0.585969\n",
      "Epoch: 807\n",
      "Loss: 0.2676105731063419\n",
      "RMSE train: 0.305251\tval: 0.596111\n",
      "MAE train: 0.241319\tval: 0.454530\n",
      "pearson_R train: 0.919326\tval: 0.601385\n",
      "Epoch: 808\n",
      "Loss: 0.2775795559088389\n",
      "RMSE train: 0.328417\tval: 0.579032\n",
      "MAE train: 0.247074\tval: 0.432559\n",
      "pearson_R train: 0.893079\tval: 0.581820\n",
      "Epoch: 809\n",
      "Loss: 0.23966989252302381\n",
      "RMSE train: 0.420745\tval: 0.692532\n",
      "MAE train: 0.328247\tval: 0.525719\n",
      "pearson_R train: 0.846307\tval: 0.521624\n",
      "Epoch: 810\n",
      "Loss: 0.2731527437766393\n",
      "RMSE train: 0.284094\tval: 0.590665\n",
      "MAE train: 0.218249\tval: 0.440663\n",
      "pearson_R train: 0.919037\tval: 0.575558\n",
      "Epoch: 811\n",
      "Loss: 0.24323239591386583\n",
      "RMSE train: 0.276214\tval: 0.602162\n",
      "MAE train: 0.213260\tval: 0.444264\n",
      "pearson_R train: 0.919555\tval: 0.595755\n",
      "Epoch: 812\n",
      "Loss: 0.24256868660449982\n",
      "RMSE train: 0.349190\tval: 0.591847\n",
      "MAE train: 0.267573\tval: 0.444084\n",
      "pearson_R train: 0.896521\tval: 0.588271\n",
      "Epoch: 813\n",
      "Loss: 0.2541717721356286\n",
      "RMSE train: 0.304643\tval: 0.636428\n",
      "MAE train: 0.238028\tval: 0.477973\n",
      "pearson_R train: 0.916847\tval: 0.587688\n",
      "Epoch: 814\n",
      "Loss: 0.2555861390299267\n",
      "RMSE train: 0.273266\tval: 0.575331\n",
      "MAE train: 0.210296\tval: 0.425160\n",
      "pearson_R train: 0.919919\tval: 0.598719\n",
      "Epoch: 815\n",
      "Loss: 0.24942144420411852\n",
      "RMSE train: 0.285561\tval: 0.604950\n",
      "MAE train: 0.222107\tval: 0.454691\n",
      "pearson_R train: 0.913784\tval: 0.588558\n",
      "Epoch: 816\n",
      "Loss: 0.24507328536775377\n",
      "RMSE train: 0.268535\tval: 0.582538\n",
      "MAE train: 0.204948\tval: 0.433586\n",
      "pearson_R train: 0.924140\tval: 0.587860\n",
      "Epoch: 817\n",
      "Loss: 0.23793893721368578\n",
      "RMSE train: 0.278650\tval: 0.588750\n",
      "MAE train: 0.213964\tval: 0.439930\n",
      "pearson_R train: 0.918020\tval: 0.593219\n",
      "Epoch: 818\n",
      "Loss: 0.2437195430199305\n",
      "RMSE train: 0.280322\tval: 0.589213\n",
      "MAE train: 0.215837\tval: 0.435349\n",
      "pearson_R train: 0.922769\tval: 0.595713\n",
      "Epoch: 819\n",
      "Loss: 0.2434051881233851\n",
      "RMSE train: 0.281214\tval: 0.611592\n",
      "MAE train: 0.219421\tval: 0.456304\n",
      "pearson_R train: 0.916521\tval: 0.563233\n",
      "Epoch: 820\n",
      "Loss: 0.2533857656849755\n",
      "RMSE train: 0.258294\tval: 0.585033\n",
      "MAE train: 0.200117\tval: 0.429736\n",
      "pearson_R train: 0.928616\tval: 0.595800\n",
      "Epoch: 821\n",
      "Loss: 0.2498956173658371\n",
      "RMSE train: 0.272475\tval: 0.587290\n",
      "MAE train: 0.210444\tval: 0.437450\n",
      "pearson_R train: 0.922447\tval: 0.603133\n",
      "Epoch: 822\n",
      "Loss: 0.26812245282861924\n",
      "RMSE train: 0.269926\tval: 0.588474\n",
      "MAE train: 0.208608\tval: 0.440812\n",
      "pearson_R train: 0.921846\tval: 0.599731\n",
      "Epoch: 823\n",
      "Loss: 0.25391682320170933\n",
      "RMSE train: 0.273970\tval: 0.600231\n",
      "MAE train: 0.212425\tval: 0.443400\n",
      "pearson_R train: 0.922340\tval: 0.585892\n",
      "Epoch: 824\n",
      "Loss: 0.2531705184115304\n",
      "RMSE train: 0.283334\tval: 0.600297\n",
      "MAE train: 0.221690\tval: 0.444713\n",
      "pearson_R train: 0.922139\tval: 0.580863\n",
      "Epoch: 825\n",
      "Loss: 0.2416796452469296\n",
      "RMSE train: 0.307633\tval: 0.588116\n",
      "MAE train: 0.238211\tval: 0.437161\n",
      "pearson_R train: 0.899994\tval: 0.581260\n",
      "Epoch: 826\n",
      "Loss: 0.24520819882551828\n",
      "RMSE train: 0.265323\tval: 0.607849\n",
      "MAE train: 0.204970\tval: 0.455338\n",
      "pearson_R train: 0.927179\tval: 0.581463\n",
      "Epoch: 827\n",
      "Loss: 0.24797160923480988\n",
      "RMSE train: 0.313066\tval: 0.645300\n",
      "MAE train: 0.243135\tval: 0.490170\n",
      "pearson_R train: 0.919064\tval: 0.592518\n",
      "Epoch: 828\n",
      "Loss: 0.24686780075232187\n",
      "RMSE train: 0.340618\tval: 0.584504\n",
      "MAE train: 0.260336\tval: 0.436015\n",
      "pearson_R train: 0.900437\tval: 0.593795\n",
      "Epoch: 829\n",
      "Loss: 0.23460501101281908\n",
      "RMSE train: 0.283206\tval: 0.604313\n",
      "MAE train: 0.221038\tval: 0.451033\n",
      "pearson_R train: 0.918983\tval: 0.579400\n",
      "Epoch: 830\n",
      "Loss: 0.23976833952797783\n",
      "RMSE train: 0.302584\tval: 0.567936\n",
      "MAE train: 0.229674\tval: 0.421353\n",
      "pearson_R train: 0.913470\tval: 0.603690\n",
      "Epoch: 831\n",
      "Loss: 0.24766651292641959\n",
      "RMSE train: 0.259398\tval: 0.574116\n",
      "MAE train: 0.202326\tval: 0.428548\n",
      "pearson_R train: 0.928522\tval: 0.603793\n",
      "Epoch: 832\n",
      "Loss: 0.23430035677221087\n",
      "RMSE train: 0.284417\tval: 0.582951\n",
      "MAE train: 0.216516\tval: 0.432725\n",
      "pearson_R train: 0.921938\tval: 0.582011\n",
      "Epoch: 833\n",
      "Loss: 0.24549254609478843\n",
      "RMSE train: 0.311755\tval: 0.634246\n",
      "MAE train: 0.242967\tval: 0.482684\n",
      "pearson_R train: 0.912491\tval: 0.575077\n",
      "Epoch: 834\n",
      "Loss: 0.24966038101249272\n",
      "RMSE train: 0.371434\tval: 0.598758\n",
      "MAE train: 0.285864\tval: 0.449755\n",
      "pearson_R train: 0.886681\tval: 0.585561\n",
      "Epoch: 835\n",
      "Loss: 0.26067746347851223\n",
      "RMSE train: 0.294030\tval: 0.598745\n",
      "MAE train: 0.230017\tval: 0.449970\n",
      "pearson_R train: 0.910739\tval: 0.603997\n",
      "Epoch: 836\n",
      "Loss: 0.23806976940896776\n",
      "RMSE train: 0.350984\tval: 0.581072\n",
      "MAE train: 0.268575\tval: 0.434216\n",
      "pearson_R train: 0.905038\tval: 0.592197\n",
      "Epoch: 837\n",
      "Loss: 0.23717824618021646\n",
      "RMSE train: 0.269776\tval: 0.600201\n",
      "MAE train: 0.209822\tval: 0.449823\n",
      "pearson_R train: 0.926605\tval: 0.601107\n",
      "Epoch: 838\n",
      "Loss: 0.24875760740704006\n",
      "RMSE train: 0.244542\tval: 0.579552\n",
      "MAE train: 0.188192\tval: 0.431797\n",
      "pearson_R train: 0.935716\tval: 0.602754\n",
      "Epoch: 839\n",
      "Loss: 0.23211350705888537\n",
      "RMSE train: 0.256334\tval: 0.589043\n",
      "MAE train: 0.198681\tval: 0.439547\n",
      "pearson_R train: 0.930806\tval: 0.604416\n",
      "Epoch: 840\n",
      "Loss: 0.24587726096312204\n",
      "RMSE train: 0.285368\tval: 0.601539\n",
      "MAE train: 0.223361\tval: 0.453085\n",
      "pearson_R train: 0.913713\tval: 0.571644\n",
      "Epoch: 841\n",
      "Loss: 0.22725747525691986\n",
      "RMSE train: 0.276811\tval: 0.609411\n",
      "MAE train: 0.213719\tval: 0.458676\n",
      "pearson_R train: 0.926985\tval: 0.606106\n",
      "Epoch: 842\n",
      "Loss: 0.24722001949946085\n",
      "RMSE train: 0.270646\tval: 0.568916\n",
      "MAE train: 0.209480\tval: 0.429131\n",
      "pearson_R train: 0.926292\tval: 0.604140\n",
      "Epoch: 843\n",
      "Loss: 0.2508973876635234\n",
      "RMSE train: 0.254816\tval: 0.592898\n",
      "MAE train: 0.195466\tval: 0.439597\n",
      "pearson_R train: 0.931353\tval: 0.602419\n",
      "Epoch: 844\n",
      "Loss: 0.2384487191836039\n",
      "RMSE train: 0.379736\tval: 0.600483\n",
      "MAE train: 0.291065\tval: 0.451667\n",
      "pearson_R train: 0.878525\tval: 0.569080\n",
      "Epoch: 845\n",
      "Loss: 0.23597276873058742\n",
      "RMSE train: 0.271016\tval: 0.593418\n",
      "MAE train: 0.210863\tval: 0.438301\n",
      "pearson_R train: 0.927180\tval: 0.597418\n",
      "Epoch: 846\n",
      "Loss: 0.24278931154145134\n",
      "RMSE train: 0.265805\tval: 0.600610\n",
      "MAE train: 0.205628\tval: 0.452967\n",
      "pearson_R train: 0.927542\tval: 0.586276\n",
      "Epoch: 847\n",
      "Loss: 0.24533227582772574\n",
      "RMSE train: 0.248643\tval: 0.589616\n",
      "MAE train: 0.192398\tval: 0.443176\n",
      "pearson_R train: 0.933987\tval: 0.595529\n",
      "Epoch: 848\n",
      "Loss: 0.23261860344145033\n",
      "RMSE train: 0.293473\tval: 0.612147\n",
      "MAE train: 0.228941\tval: 0.462058\n",
      "pearson_R train: 0.918857\tval: 0.591933\n",
      "Epoch: 849\n",
      "Loss: 0.22962365878952873\n",
      "RMSE train: 0.248400\tval: 0.584267\n",
      "MAE train: 0.193111\tval: 0.435936\n",
      "pearson_R train: 0.934861\tval: 0.604609\n",
      "Epoch: 850\n",
      "Loss: 0.21702231301201713\n",
      "RMSE train: 0.256466\tval: 0.591442\n",
      "MAE train: 0.200089\tval: 0.445257\n",
      "pearson_R train: 0.933780\tval: 0.602663\n",
      "Epoch: 851\n",
      "Loss: 0.2383000502983729\n",
      "RMSE train: 0.263194\tval: 0.577030\n",
      "MAE train: 0.204647\tval: 0.426055\n",
      "pearson_R train: 0.932905\tval: 0.607591\n",
      "Epoch: 852\n",
      "Loss: 0.23714450829558903\n",
      "RMSE train: 0.249282\tval: 0.591005\n",
      "MAE train: 0.194147\tval: 0.438307\n",
      "pearson_R train: 0.935711\tval: 0.602584\n",
      "Epoch: 853\n",
      "Loss: 0.2180777672264311\n",
      "RMSE train: 0.249330\tval: 0.583206\n",
      "MAE train: 0.192630\tval: 0.436717\n",
      "pearson_R train: 0.933390\tval: 0.596473\n",
      "Epoch: 854\n",
      "Loss: 0.22903376486566332\n",
      "RMSE train: 0.283091\tval: 0.583664\n",
      "MAE train: 0.221695\tval: 0.432216\n",
      "pearson_R train: 0.928353\tval: 0.611970\n",
      "Epoch: 855\n",
      "Loss: 0.22938921882046592\n",
      "RMSE train: 0.266085\tval: 0.584094\n",
      "MAE train: 0.206306\tval: 0.439502\n",
      "pearson_R train: 0.927542\tval: 0.603057\n",
      "Epoch: 856\n",
      "Loss: 0.2253828677866194\n",
      "RMSE train: 0.257905\tval: 0.590662\n",
      "MAE train: 0.199253\tval: 0.437395\n",
      "pearson_R train: 0.928276\tval: 0.581782\n",
      "Epoch: 857\n",
      "Loss: 0.2296864108906852\n",
      "RMSE train: 0.298916\tval: 0.595167\n",
      "MAE train: 0.230332\tval: 0.444177\n",
      "pearson_R train: 0.909108\tval: 0.600656\n",
      "Epoch: 858\n",
      "Loss: 0.23447861605220371\n",
      "RMSE train: 0.301878\tval: 0.628686\n",
      "MAE train: 0.235204\tval: 0.467161\n",
      "pearson_R train: 0.911155\tval: 0.568985\n",
      "Epoch: 859\n",
      "Loss: 0.2723269495699141\n",
      "RMSE train: 0.271901\tval: 0.598235\n",
      "MAE train: 0.213555\tval: 0.454007\n",
      "pearson_R train: 0.929288\tval: 0.597061\n",
      "Epoch: 860\n",
      "Loss: 0.24493033356136745\n",
      "RMSE train: 0.280375\tval: 0.595218\n",
      "MAE train: 0.219662\tval: 0.452531\n",
      "pearson_R train: 0.922062\tval: 0.614572\n",
      "Epoch: 861\n",
      "Loss: 0.22319087386131287\n",
      "RMSE train: 0.276401\tval: 0.580741\n",
      "MAE train: 0.214967\tval: 0.442065\n",
      "pearson_R train: 0.919318\tval: 0.599184\n",
      "Epoch: 862\n",
      "Loss: 0.23222198088963827\n",
      "RMSE train: 0.267923\tval: 0.582022\n",
      "MAE train: 0.207323\tval: 0.432683\n",
      "pearson_R train: 0.923263\tval: 0.603168\n",
      "Epoch: 863\n",
      "Loss: 0.22301491101582846\n",
      "RMSE train: 0.265130\tval: 0.590964\n",
      "MAE train: 0.203834\tval: 0.445817\n",
      "pearson_R train: 0.927055\tval: 0.589719\n",
      "Epoch: 864\n",
      "Loss: 0.24830540849102867\n",
      "RMSE train: 0.269748\tval: 0.591142\n",
      "MAE train: 0.211020\tval: 0.443815\n",
      "pearson_R train: 0.922417\tval: 0.583272\n",
      "Epoch: 865\n",
      "Loss: 0.2358922859032949\n",
      "RMSE train: 0.268524\tval: 0.569732\n",
      "MAE train: 0.211232\tval: 0.428107\n",
      "pearson_R train: 0.932440\tval: 0.602185\n",
      "Epoch: 866\n",
      "Loss: 0.234471062819163\n",
      "RMSE train: 0.300584\tval: 0.571331\n",
      "MAE train: 0.234203\tval: 0.428654\n",
      "pearson_R train: 0.923058\tval: 0.620958\n",
      "Epoch: 867\n",
      "Loss: 0.22806769609451294\n",
      "RMSE train: 0.249309\tval: 0.585656\n",
      "MAE train: 0.193748\tval: 0.437017\n",
      "pearson_R train: 0.934637\tval: 0.595967\n",
      "Epoch: 868\n",
      "Loss: 0.2335545221964518\n",
      "RMSE train: 0.248544\tval: 0.597054\n",
      "MAE train: 0.194579\tval: 0.444125\n",
      "pearson_R train: 0.937238\tval: 0.601045\n",
      "Epoch: 869\n",
      "Loss: 0.2217780566877789\n",
      "RMSE train: 0.257070\tval: 0.575798\n",
      "MAE train: 0.200713\tval: 0.421725\n",
      "pearson_R train: 0.932543\tval: 0.601076\n",
      "Epoch: 870\n",
      "Loss: 0.2278085963593589\n",
      "RMSE train: 0.254998\tval: 0.590175\n",
      "MAE train: 0.197129\tval: 0.437542\n",
      "pearson_R train: 0.930256\tval: 0.598672\n",
      "Epoch: 871\n",
      "Loss: 0.23936830792162153\n",
      "RMSE train: 0.275780\tval: 0.591172\n",
      "MAE train: 0.214182\tval: 0.445729\n",
      "pearson_R train: 0.923768\tval: 0.586032\n",
      "Epoch: 872\n",
      "Loss: 0.22614817983574337\n",
      "RMSE train: 0.251177\tval: 0.602080\n",
      "MAE train: 0.194919\tval: 0.453713\n",
      "pearson_R train: 0.934215\tval: 0.595911\n",
      "Epoch: 873\n",
      "Loss: 0.23378017048041025\n",
      "RMSE train: 0.256011\tval: 0.606606\n",
      "MAE train: 0.199594\tval: 0.453022\n",
      "pearson_R train: 0.931643\tval: 0.590489\n",
      "Epoch: 874\n",
      "Loss: 0.2322857826948166\n",
      "RMSE train: 0.240065\tval: 0.605335\n",
      "MAE train: 0.186863\tval: 0.451890\n",
      "pearson_R train: 0.941022\tval: 0.591774\n",
      "Epoch: 875\n",
      "Loss: 0.22679178085592058\n",
      "RMSE train: 0.249457\tval: 0.612201\n",
      "MAE train: 0.194191\tval: 0.460313\n",
      "pearson_R train: 0.935524\tval: 0.582292\n",
      "Epoch: 876\n",
      "Loss: 0.2251267217927509\n",
      "RMSE train: 0.303689\tval: 0.593186\n",
      "MAE train: 0.236841\tval: 0.442064\n",
      "pearson_R train: 0.928169\tval: 0.579218\n",
      "Epoch: 877\n",
      "Loss: 0.22549354367785984\n",
      "RMSE train: 0.250654\tval: 0.608236\n",
      "MAE train: 0.195925\tval: 0.447769\n",
      "pearson_R train: 0.936422\tval: 0.593598\n",
      "Epoch: 878\n",
      "Loss: 0.21223558651076424\n",
      "RMSE train: 0.260729\tval: 0.600695\n",
      "MAE train: 0.202922\tval: 0.446129\n",
      "pearson_R train: 0.927669\tval: 0.588961\n",
      "Epoch: 879\n",
      "Loss: 0.21518025133344862\n",
      "RMSE train: 0.260059\tval: 0.616598\n",
      "MAE train: 0.201352\tval: 0.460342\n",
      "pearson_R train: 0.931147\tval: 0.583028\n",
      "Epoch: 880\n",
      "Loss: 0.21346494058767954\n",
      "RMSE train: 0.242454\tval: 0.589341\n",
      "MAE train: 0.185736\tval: 0.444423\n",
      "pearson_R train: 0.938529\tval: 0.607683\n",
      "Epoch: 881\n",
      "Loss: 0.231478836801317\n",
      "RMSE train: 0.248418\tval: 0.596806\n",
      "MAE train: 0.195461\tval: 0.447924\n",
      "pearson_R train: 0.936350\tval: 0.593593\n",
      "Epoch: 882\n",
      "Loss: 0.22634590665499368\n",
      "RMSE train: 0.234453\tval: 0.591702\n",
      "MAE train: 0.181103\tval: 0.444870\n",
      "pearson_R train: 0.943120\tval: 0.602474\n",
      "Epoch: 883\n",
      "Loss: 0.22600491344928741\n",
      "RMSE train: 0.249817\tval: 0.582620\n",
      "MAE train: 0.191732\tval: 0.437652\n",
      "pearson_R train: 0.933196\tval: 0.591875\n",
      "Epoch: 884\n",
      "Loss: 0.20777732133865356\n",
      "RMSE train: 0.251175\tval: 0.598569\n",
      "MAE train: 0.195645\tval: 0.451149\n",
      "pearson_R train: 0.938676\tval: 0.581017\n",
      "Epoch: 885\n",
      "Loss: 0.20488024089071485\n",
      "RMSE train: 0.260070\tval: 0.603665\n",
      "MAE train: 0.204494\tval: 0.453417\n",
      "pearson_R train: 0.940103\tval: 0.574701\n",
      "Epoch: 886\n",
      "Loss: 0.2124932242764367\n",
      "RMSE train: 0.248060\tval: 0.590978\n",
      "MAE train: 0.192211\tval: 0.440099\n",
      "pearson_R train: 0.940410\tval: 0.589859\n",
      "Epoch: 887\n",
      "Loss: 0.2067873759402169\n",
      "RMSE train: 0.239742\tval: 0.574921\n",
      "MAE train: 0.186241\tval: 0.428193\n",
      "pearson_R train: 0.940568\tval: 0.597747\n",
      "Epoch: 888\n",
      "Loss: 0.20687721338537005\n",
      "RMSE train: 0.240752\tval: 0.583325\n",
      "MAE train: 0.188494\tval: 0.435676\n",
      "pearson_R train: 0.939224\tval: 0.603517\n",
      "Epoch: 889\n",
      "Loss: 0.21000726024309793\n",
      "RMSE train: 0.234574\tval: 0.583535\n",
      "MAE train: 0.181777\tval: 0.437781\n",
      "pearson_R train: 0.941109\tval: 0.591999\n",
      "Epoch: 890\n",
      "Loss: 0.2216578010055754\n",
      "RMSE train: 0.260282\tval: 0.628853\n",
      "MAE train: 0.200984\tval: 0.466527\n",
      "pearson_R train: 0.937744\tval: 0.582900\n",
      "Epoch: 891\n",
      "Loss: 0.2295458432700899\n",
      "RMSE train: 0.251044\tval: 0.584399\n",
      "MAE train: 0.194508\tval: 0.434131\n",
      "pearson_R train: 0.936234\tval: 0.593756\n",
      "Epoch: 892\n",
      "Loss: 0.24578514695167542\n",
      "RMSE train: 0.259818\tval: 0.598639\n",
      "MAE train: 0.196931\tval: 0.438155\n",
      "pearson_R train: 0.929126\tval: 0.597203\n",
      "Epoch: 893\n",
      "Loss: 0.23332512378692627\n",
      "RMSE train: 0.283037\tval: 0.563018\n",
      "MAE train: 0.217106\tval: 0.425087\n",
      "pearson_R train: 0.928498\tval: 0.600592\n",
      "Epoch: 894\n",
      "Loss: 0.21337177687221104\n",
      "RMSE train: 0.252137\tval: 0.603225\n",
      "MAE train: 0.194573\tval: 0.458850\n",
      "pearson_R train: 0.937046\tval: 0.597016\n",
      "Epoch: 895\n",
      "Loss: 0.24459858238697052\n",
      "RMSE train: 0.255205\tval: 0.592793\n",
      "MAE train: 0.195131\tval: 0.444261\n",
      "pearson_R train: 0.936798\tval: 0.570228\n",
      "Epoch: 896\n",
      "Loss: 0.21332251694467333\n",
      "RMSE train: 0.251592\tval: 0.601335\n",
      "MAE train: 0.196416\tval: 0.450305\n",
      "pearson_R train: 0.936916\tval: 0.597946\n",
      "Epoch: 897\n",
      "Loss: 0.22585473954677582\n",
      "RMSE train: 0.247083\tval: 0.582150\n",
      "MAE train: 0.190536\tval: 0.437073\n",
      "pearson_R train: 0.938713\tval: 0.589586\n",
      "Epoch: 898\n",
      "Loss: 0.21258358491791618\n",
      "RMSE train: 0.254123\tval: 0.611741\n",
      "MAE train: 0.196988\tval: 0.470534\n",
      "pearson_R train: 0.936951\tval: 0.594843\n",
      "Epoch: 899\n",
      "Loss: 0.20876755151483747\n",
      "RMSE train: 0.239473\tval: 0.588561\n",
      "MAE train: 0.186467\tval: 0.438482\n",
      "pearson_R train: 0.938547\tval: 0.580111\n",
      "Epoch: 900\n",
      "Loss: 0.22377544475926292\n",
      "RMSE train: 0.238343\tval: 0.585661\n",
      "MAE train: 0.185147\tval: 0.432353\n",
      "pearson_R train: 0.939817\tval: 0.606592\n",
      "Epoch: 901\n",
      "Loss: 0.2155605951944987\n",
      "RMSE train: 0.255747\tval: 0.602589\n",
      "MAE train: 0.202542\tval: 0.451738\n",
      "pearson_R train: 0.941611\tval: 0.601817\n",
      "Epoch: 902\n",
      "Loss: 0.22017624643113878\n",
      "RMSE train: 0.237573\tval: 0.575375\n",
      "MAE train: 0.183260\tval: 0.436360\n",
      "pearson_R train: 0.940652\tval: 0.602423\n",
      "Epoch: 903\n",
      "Loss: 0.2134101837873459\n",
      "RMSE train: 0.251362\tval: 0.588644\n",
      "MAE train: 0.196591\tval: 0.444439\n",
      "pearson_R train: 0.938298\tval: 0.600138\n",
      "Epoch: 904\n",
      "Loss: 0.2128165364265442\n",
      "RMSE train: 0.244346\tval: 0.594517\n",
      "MAE train: 0.188915\tval: 0.447623\n",
      "pearson_R train: 0.940602\tval: 0.597723\n",
      "Epoch: 905\n",
      "Loss: 0.20235303541024527\n",
      "RMSE train: 0.253806\tval: 0.601223\n",
      "MAE train: 0.199535\tval: 0.457177\n",
      "pearson_R train: 0.941617\tval: 0.606734\n",
      "Epoch: 906\n",
      "Loss: 0.21545610162946913\n",
      "RMSE train: 0.231688\tval: 0.580929\n",
      "MAE train: 0.179005\tval: 0.434974\n",
      "pearson_R train: 0.943264\tval: 0.607452\n",
      "Epoch: 907\n",
      "Loss: 0.21513262556658852\n",
      "RMSE train: 0.235229\tval: 0.570247\n",
      "MAE train: 0.181746\tval: 0.427221\n",
      "pearson_R train: 0.941193\tval: 0.614228\n",
      "Epoch: 908\n",
      "Loss: 0.2232176893287235\n",
      "RMSE train: 0.263619\tval: 0.585967\n",
      "MAE train: 0.204369\tval: 0.437956\n",
      "pearson_R train: 0.930890\tval: 0.601193\n",
      "Epoch: 909\n",
      "Loss: 0.22168250050809649\n",
      "RMSE train: 0.247486\tval: 0.592785\n",
      "MAE train: 0.192433\tval: 0.451230\n",
      "pearson_R train: 0.935742\tval: 0.577683\n",
      "Epoch: 910\n",
      "Loss: 0.20769876738389334\n",
      "RMSE train: 0.237319\tval: 0.589701\n",
      "MAE train: 0.183839\tval: 0.446609\n",
      "pearson_R train: 0.942254\tval: 0.596317\n",
      "Epoch: 911\n",
      "Loss: 0.20735230379634434\n",
      "RMSE train: 0.280056\tval: 0.580035\n",
      "MAE train: 0.213941\tval: 0.434582\n",
      "pearson_R train: 0.923704\tval: 0.584865\n",
      "Epoch: 912\n",
      "Loss: 0.20447324381934273\n",
      "RMSE train: 0.242961\tval: 0.587912\n",
      "MAE train: 0.187642\tval: 0.442234\n",
      "pearson_R train: 0.938914\tval: 0.599841\n",
      "Epoch: 913\n",
      "Loss: 0.2118215345674091\n",
      "RMSE train: 0.264205\tval: 0.607707\n",
      "MAE train: 0.203361\tval: 0.454426\n",
      "pearson_R train: 0.930266\tval: 0.576152\n",
      "Epoch: 914\n",
      "Loss: 0.21346519225173527\n",
      "RMSE train: 0.243445\tval: 0.580805\n",
      "MAE train: 0.190069\tval: 0.437052\n",
      "pearson_R train: 0.937395\tval: 0.608067\n",
      "Epoch: 915\n",
      "Loss: 0.2175451699230406\n",
      "RMSE train: 0.241135\tval: 0.590373\n",
      "MAE train: 0.187076\tval: 0.439685\n",
      "pearson_R train: 0.938021\tval: 0.595566\n",
      "Epoch: 916\n",
      "Loss: 0.195604360765881\n",
      "RMSE train: 0.233653\tval: 0.600160\n",
      "MAE train: 0.183984\tval: 0.452234\n",
      "pearson_R train: 0.947569\tval: 0.593848\n",
      "Epoch: 917\n",
      "Loss: 0.19788190888033974\n",
      "RMSE train: 0.243783\tval: 0.582550\n",
      "MAE train: 0.190685\tval: 0.434640\n",
      "pearson_R train: 0.940353\tval: 0.597671\n",
      "Epoch: 918\n",
      "Loss: 0.20437200864156088\n",
      "RMSE train: 0.249080\tval: 0.606206\n",
      "MAE train: 0.190420\tval: 0.457101\n",
      "pearson_R train: 0.943312\tval: 0.622221\n",
      "Epoch: 919\n",
      "Loss: 0.20310060348775652\n",
      "RMSE train: 0.245554\tval: 0.568179\n",
      "MAE train: 0.189264\tval: 0.424079\n",
      "pearson_R train: 0.935674\tval: 0.603108\n",
      "Epoch: 920\n",
      "Loss: 0.20206019944614834\n",
      "RMSE train: 0.228141\tval: 0.589781\n",
      "MAE train: 0.178556\tval: 0.438263\n",
      "pearson_R train: 0.947552\tval: 0.596726\n",
      "Epoch: 921\n",
      "Loss: 0.20384792486826578\n",
      "RMSE train: 0.245963\tval: 0.575024\n",
      "MAE train: 0.191206\tval: 0.435701\n",
      "pearson_R train: 0.940877\tval: 0.616942\n",
      "Epoch: 922\n",
      "Loss: 0.20901759134398568\n",
      "RMSE train: 0.223443\tval: 0.587803\n",
      "MAE train: 0.172722\tval: 0.437454\n",
      "pearson_R train: 0.948318\tval: 0.585964\n",
      "Epoch: 923\n",
      "Loss: 0.20379702581299675\n",
      "RMSE train: 0.254749\tval: 0.592852\n",
      "MAE train: 0.199329\tval: 0.442934\n",
      "pearson_R train: 0.938043\tval: 0.601949\n",
      "Epoch: 924\n",
      "Loss: 0.2065242694483863\n",
      "RMSE train: 0.239918\tval: 0.591809\n",
      "MAE train: 0.186026\tval: 0.441992\n",
      "pearson_R train: 0.943043\tval: 0.603436\n",
      "Epoch: 925\n",
      "Loss: 0.2302103555864758\n",
      "RMSE train: 0.253288\tval: 0.583183\n",
      "MAE train: 0.194678\tval: 0.440457\n",
      "pearson_R train: 0.932609\tval: 0.588844\n",
      "Epoch: 926\n",
      "Loss: 0.20469146304660374\n",
      "RMSE train: 0.240844\tval: 0.613795\n",
      "MAE train: 0.186684\tval: 0.464112\n",
      "pearson_R train: 0.945707\tval: 0.593981\n",
      "Epoch: 927\n",
      "Loss: 0.196083449655109\n",
      "RMSE train: 0.232718\tval: 0.572614\n",
      "MAE train: 0.180228\tval: 0.427891\n",
      "pearson_R train: 0.943273\tval: 0.599545\n",
      "Epoch: 928\n",
      "Loss: 0.2032697598139445\n",
      "RMSE train: 0.237050\tval: 0.613601\n",
      "MAE train: 0.183624\tval: 0.464043\n",
      "pearson_R train: 0.945087\tval: 0.587135\n",
      "Epoch: 929\n",
      "Loss: 0.19765947262446085\n",
      "RMSE train: 0.276860\tval: 0.589287\n",
      "MAE train: 0.216638\tval: 0.443109\n",
      "pearson_R train: 0.929070\tval: 0.595752\n",
      "Epoch: 930\n",
      "Loss: 0.20868372751606834\n",
      "RMSE train: 0.226840\tval: 0.608271\n",
      "MAE train: 0.177513\tval: 0.453122\n",
      "pearson_R train: 0.948573\tval: 0.584084\n",
      "Epoch: 931\n",
      "Loss: 0.20600703193081749\n",
      "RMSE train: 0.239558\tval: 0.611016\n",
      "MAE train: 0.185725\tval: 0.461413\n",
      "pearson_R train: 0.948607\tval: 0.597629\n",
      "Epoch: 932\n",
      "Loss: 0.19223489198419783\n",
      "RMSE train: 0.223667\tval: 0.591874\n",
      "MAE train: 0.174712\tval: 0.442169\n",
      "pearson_R train: 0.947965\tval: 0.594709\n",
      "Epoch: 933\n",
      "Loss: 0.2009186198314031\n",
      "RMSE train: 0.240007\tval: 0.579066\n",
      "MAE train: 0.185207\tval: 0.431557\n",
      "pearson_R train: 0.941848\tval: 0.607813\n",
      "Epoch: 934\n",
      "Loss: 0.1983808610174391\n",
      "RMSE train: 0.217939\tval: 0.592187\n",
      "MAE train: 0.169583\tval: 0.442298\n",
      "pearson_R train: 0.952264\tval: 0.594492\n",
      "Epoch: 935\n",
      "Loss: 0.20085517565409342\n",
      "RMSE train: 0.229069\tval: 0.577819\n",
      "MAE train: 0.177585\tval: 0.428890\n",
      "pearson_R train: 0.945231\tval: 0.601144\n",
      "Epoch: 936\n",
      "Loss: 0.19857682122124565\n",
      "RMSE train: 0.229161\tval: 0.582314\n",
      "MAE train: 0.178885\tval: 0.435548\n",
      "pearson_R train: 0.946611\tval: 0.608821\n",
      "Epoch: 937\n",
      "Loss: 0.19378111925390032\n",
      "RMSE train: 0.228748\tval: 0.582051\n",
      "MAE train: 0.178486\tval: 0.438090\n",
      "pearson_R train: 0.946150\tval: 0.597611\n",
      "Epoch: 938\n",
      "Loss: 0.1994645893573761\n",
      "RMSE train: 0.232490\tval: 0.613552\n",
      "MAE train: 0.180653\tval: 0.464716\n",
      "pearson_R train: 0.945646\tval: 0.573070\n",
      "Epoch: 939\n",
      "Loss: 0.2082392391231325\n",
      "RMSE train: 0.224671\tval: 0.581306\n",
      "MAE train: 0.173804\tval: 0.429754\n",
      "pearson_R train: 0.946572\tval: 0.592915\n",
      "Epoch: 940\n",
      "Loss: 0.19939720630645752\n",
      "RMSE train: 0.224335\tval: 0.603395\n",
      "MAE train: 0.171784\tval: 0.446819\n",
      "pearson_R train: 0.948195\tval: 0.592993\n",
      "Epoch: 941\n",
      "Loss: 0.2071691850821177\n",
      "RMSE train: 0.226849\tval: 0.596114\n",
      "MAE train: 0.174959\tval: 0.453850\n",
      "pearson_R train: 0.946573\tval: 0.587225\n",
      "Epoch: 942\n",
      "Loss: 0.20498742991023594\n",
      "RMSE train: 0.276722\tval: 0.608000\n",
      "MAE train: 0.214522\tval: 0.458329\n",
      "pearson_R train: 0.934326\tval: 0.579903\n",
      "Epoch: 943\n",
      "Loss: 0.20251405239105225\n",
      "RMSE train: 0.237633\tval: 0.583748\n",
      "MAE train: 0.184766\tval: 0.437393\n",
      "pearson_R train: 0.941177\tval: 0.578470\n",
      "Epoch: 944\n",
      "Loss: 0.18757389651404488\n",
      "RMSE train: 0.238831\tval: 0.588464\n",
      "MAE train: 0.184532\tval: 0.438435\n",
      "pearson_R train: 0.940890\tval: 0.597911\n",
      "Epoch: 945\n",
      "Loss: 0.1942952854765786\n",
      "RMSE train: 0.256065\tval: 0.591519\n",
      "MAE train: 0.198637\tval: 0.450819\n",
      "pearson_R train: 0.936835\tval: 0.608882\n",
      "Epoch: 946\n",
      "Loss: 0.20529692206117842\n",
      "RMSE train: 0.228494\tval: 0.588293\n",
      "MAE train: 0.177291\tval: 0.444226\n",
      "pearson_R train: 0.945673\tval: 0.586793\n",
      "Epoch: 947\n",
      "Loss: 0.19860803915394676\n",
      "RMSE train: 0.297978\tval: 0.586066\n",
      "MAE train: 0.228289\tval: 0.436781\n",
      "pearson_R train: 0.921944\tval: 0.588978\n",
      "Epoch: 948\n",
      "Loss: 0.20801063047515023\n",
      "RMSE train: 0.234008\tval: 0.589275\n",
      "MAE train: 0.181807\tval: 0.445112\n",
      "pearson_R train: 0.943176\tval: 0.593866\n",
      "Epoch: 949\n",
      "Loss: 0.18664422962400648\n",
      "RMSE train: 0.220256\tval: 0.590119\n",
      "MAE train: 0.170395\tval: 0.448544\n",
      "pearson_R train: 0.950561\tval: 0.601396\n",
      "Epoch: 950\n",
      "Loss: 0.18435383008586037\n",
      "RMSE train: 0.215967\tval: 0.593510\n",
      "MAE train: 0.167772\tval: 0.446625\n",
      "pearson_R train: 0.952238\tval: 0.593839\n",
      "Epoch: 951\n",
      "Loss: 0.1900178160932329\n",
      "RMSE train: 0.219995\tval: 0.586077\n",
      "MAE train: 0.170975\tval: 0.446248\n",
      "pearson_R train: 0.950147\tval: 0.605053\n",
      "Epoch: 952\n",
      "Loss: 0.1822730733288659\n",
      "RMSE train: 0.238545\tval: 0.589419\n",
      "MAE train: 0.184564\tval: 0.445288\n",
      "pearson_R train: 0.948668\tval: 0.586880\n",
      "Epoch: 953\n",
      "Loss: 0.17725696828630236\n",
      "RMSE train: 0.226680\tval: 0.606214\n",
      "MAE train: 0.175558\tval: 0.457183\n",
      "pearson_R train: 0.948526\tval: 0.593534\n",
      "Epoch: 954\n",
      "Loss: 0.20775224765141806\n",
      "RMSE train: 0.228571\tval: 0.603262\n",
      "MAE train: 0.177479\tval: 0.451894\n",
      "pearson_R train: 0.947041\tval: 0.584274\n",
      "Epoch: 955\n",
      "Loss: 0.20122435026698643\n",
      "RMSE train: 0.223284\tval: 0.588891\n",
      "MAE train: 0.172398\tval: 0.441370\n",
      "pearson_R train: 0.948440\tval: 0.579291\n",
      "Epoch: 956\n",
      "Loss: 0.19708693689770168\n",
      "RMSE train: 0.220693\tval: 0.591916\n",
      "MAE train: 0.170285\tval: 0.438459\n",
      "pearson_R train: 0.950343\tval: 0.601643\n",
      "Epoch: 957\n",
      "Loss: 0.18702130185233223\n",
      "RMSE train: 0.242658\tval: 0.583091\n",
      "MAE train: 0.191296\tval: 0.433473\n",
      "pearson_R train: 0.949365\tval: 0.594532\n",
      "Epoch: 958\n",
      "Loss: 0.1766303214761946\n",
      "RMSE train: 0.222083\tval: 0.600098\n",
      "MAE train: 0.169847\tval: 0.450273\n",
      "pearson_R train: 0.952386\tval: 0.604075\n",
      "Epoch: 959\n",
      "Loss: 0.18236271209186977\n",
      "RMSE train: 0.211537\tval: 0.592593\n",
      "MAE train: 0.163434\tval: 0.443024\n",
      "pearson_R train: 0.953524\tval: 0.600513\n",
      "Epoch: 960\n",
      "Loss: 0.1831805408000946\n",
      "RMSE train: 0.227214\tval: 0.584735\n",
      "MAE train: 0.177896\tval: 0.439623\n",
      "pearson_R train: 0.952541\tval: 0.594299\n",
      "Epoch: 961\n",
      "Loss: 0.1954435391558541\n",
      "RMSE train: 0.223469\tval: 0.599889\n",
      "MAE train: 0.172989\tval: 0.454040\n",
      "pearson_R train: 0.949962\tval: 0.597955\n",
      "Epoch: 962\n",
      "Loss: 0.18029778202374777\n",
      "RMSE train: 0.235514\tval: 0.580549\n",
      "MAE train: 0.184588\tval: 0.433941\n",
      "pearson_R train: 0.948312\tval: 0.594676\n",
      "Epoch: 963\n",
      "Loss: 0.17600811686780718\n",
      "RMSE train: 0.228338\tval: 0.603301\n",
      "MAE train: 0.178067\tval: 0.452443\n",
      "pearson_R train: 0.947644\tval: 0.586102\n",
      "Epoch: 964\n",
      "Loss: 0.1925095154179467\n",
      "RMSE train: 0.219835\tval: 0.592551\n",
      "MAE train: 0.169596\tval: 0.443669\n",
      "pearson_R train: 0.952473\tval: 0.611427\n",
      "Epoch: 965\n",
      "Loss: 0.18542875349521637\n",
      "RMSE train: 0.222023\tval: 0.599088\n",
      "MAE train: 0.170752\tval: 0.449736\n",
      "pearson_R train: 0.950442\tval: 0.600595\n",
      "Epoch: 966\n",
      "Loss: 0.18995136519273123\n",
      "RMSE train: 0.225605\tval: 0.583753\n",
      "MAE train: 0.175273\tval: 0.433371\n",
      "pearson_R train: 0.946849\tval: 0.608909\n",
      "Epoch: 967\n",
      "Loss: 0.2058997650941213\n",
      "RMSE train: 0.224510\tval: 0.587470\n",
      "MAE train: 0.176217\tval: 0.437782\n",
      "pearson_R train: 0.953474\tval: 0.596170\n",
      "Epoch: 968\n",
      "Loss: 0.18972584936353895\n",
      "RMSE train: 0.231771\tval: 0.599200\n",
      "MAE train: 0.178762\tval: 0.453706\n",
      "pearson_R train: 0.944569\tval: 0.587829\n",
      "Epoch: 969\n",
      "Loss: 0.19463821252187094\n",
      "RMSE train: 0.282123\tval: 0.659494\n",
      "MAE train: 0.215977\tval: 0.499102\n",
      "pearson_R train: 0.951620\tval: 0.606406\n",
      "Epoch: 970\n",
      "Loss: 0.19786219464408028\n",
      "RMSE train: 0.222082\tval: 0.575587\n",
      "MAE train: 0.173580\tval: 0.427247\n",
      "pearson_R train: 0.950217\tval: 0.590791\n",
      "Epoch: 971\n",
      "Loss: 0.20332296192646027\n",
      "RMSE train: 0.244352\tval: 0.614731\n",
      "MAE train: 0.189084\tval: 0.460055\n",
      "pearson_R train: 0.943618\tval: 0.593043\n",
      "Epoch: 972\n",
      "Loss: 0.1921910030974282\n",
      "RMSE train: 0.209707\tval: 0.580814\n",
      "MAE train: 0.162825\tval: 0.435068\n",
      "pearson_R train: 0.953554\tval: 0.597661\n",
      "Epoch: 973\n",
      "Loss: 0.18959550393952262\n",
      "RMSE train: 0.224002\tval: 0.601835\n",
      "MAE train: 0.175687\tval: 0.447918\n",
      "pearson_R train: 0.950959\tval: 0.599803\n",
      "Epoch: 974\n",
      "Loss: 0.1911588278081682\n",
      "RMSE train: 0.219937\tval: 0.585915\n",
      "MAE train: 0.168470\tval: 0.434626\n",
      "pearson_R train: 0.948444\tval: 0.587691\n",
      "Epoch: 975\n",
      "Loss: 0.1974381622340944\n",
      "RMSE train: 0.233633\tval: 0.602499\n",
      "MAE train: 0.179664\tval: 0.442362\n",
      "pearson_R train: 0.943407\tval: 0.594643\n",
      "Epoch: 976\n",
      "Loss: 0.17584185467825997\n",
      "RMSE train: 0.217574\tval: 0.601564\n",
      "MAE train: 0.170295\tval: 0.447933\n",
      "pearson_R train: 0.953709\tval: 0.581918\n",
      "Epoch: 977\n",
      "Loss: 0.18129272924529183\n",
      "RMSE train: 0.213667\tval: 0.599851\n",
      "MAE train: 0.166869\tval: 0.449258\n",
      "pearson_R train: 0.955052\tval: 0.589497\n",
      "Epoch: 978\n",
      "Loss: 0.17711627814504835\n",
      "RMSE train: 0.216215\tval: 0.612052\n",
      "MAE train: 0.167409\tval: 0.456148\n",
      "pearson_R train: 0.953186\tval: 0.579948\n",
      "Epoch: 979\n",
      "Loss: 0.1912624869081709\n",
      "RMSE train: 0.226659\tval: 0.577589\n",
      "MAE train: 0.175565\tval: 0.435162\n",
      "pearson_R train: 0.950947\tval: 0.579629\n",
      "Epoch: 980\n",
      "Loss: 0.1821668048699697\n",
      "RMSE train: 0.229883\tval: 0.620468\n",
      "MAE train: 0.174711\tval: 0.462636\n",
      "pearson_R train: 0.952744\tval: 0.590330\n",
      "Epoch: 981\n",
      "Loss: 0.18835577368736267\n",
      "RMSE train: 0.214073\tval: 0.587750\n",
      "MAE train: 0.167678\tval: 0.442706\n",
      "pearson_R train: 0.951481\tval: 0.585185\n",
      "Epoch: 982\n",
      "Loss: 0.2024300346771876\n",
      "RMSE train: 0.205492\tval: 0.594714\n",
      "MAE train: 0.157331\tval: 0.444536\n",
      "pearson_R train: 0.957148\tval: 0.592858\n",
      "Epoch: 983\n",
      "Loss: 0.18777969976266226\n",
      "RMSE train: 0.237889\tval: 0.613413\n",
      "MAE train: 0.184534\tval: 0.463528\n",
      "pearson_R train: 0.942917\tval: 0.589719\n",
      "Epoch: 984\n",
      "Loss: 0.18005182014571297\n",
      "RMSE train: 0.215588\tval: 0.584275\n",
      "MAE train: 0.167748\tval: 0.439276\n",
      "pearson_R train: 0.953666\tval: 0.591358\n",
      "Epoch: 985\n",
      "Loss: 0.1820336249139574\n",
      "RMSE train: 0.222286\tval: 0.589590\n",
      "MAE train: 0.171572\tval: 0.436148\n",
      "pearson_R train: 0.950897\tval: 0.599037\n",
      "Epoch: 986\n",
      "Loss: 0.20148199962245095\n",
      "RMSE train: 0.223027\tval: 0.583828\n",
      "MAE train: 0.174075\tval: 0.434481\n",
      "pearson_R train: 0.948866\tval: 0.594833\n",
      "Epoch: 987\n",
      "Loss: 0.17564203176233503\n",
      "RMSE train: 0.215910\tval: 0.606133\n",
      "MAE train: 0.167525\tval: 0.451117\n",
      "pearson_R train: 0.952683\tval: 0.593728\n",
      "Epoch: 988\n",
      "Loss: 0.18326406511995527\n",
      "RMSE train: 0.216991\tval: 0.600747\n",
      "MAE train: 0.169939\tval: 0.450435\n",
      "pearson_R train: 0.953996\tval: 0.593316\n",
      "Epoch: 989\n",
      "Loss: 0.18830043739742702\n",
      "RMSE train: 0.204175\tval: 0.582179\n",
      "MAE train: 0.160706\tval: 0.430369\n",
      "pearson_R train: 0.956849\tval: 0.601298\n",
      "Epoch: 990\n",
      "Loss: 0.1894445535209444\n",
      "RMSE train: 0.229020\tval: 0.599743\n",
      "MAE train: 0.177176\tval: 0.450026\n",
      "pearson_R train: 0.948578\tval: 0.588632\n",
      "Epoch: 991\n",
      "Loss: 0.18332314822408888\n",
      "RMSE train: 0.210448\tval: 0.603255\n",
      "MAE train: 0.163979\tval: 0.454261\n",
      "pearson_R train: 0.954454\tval: 0.589920\n",
      "Epoch: 992\n",
      "Loss: 0.1772516386376487\n",
      "RMSE train: 0.230894\tval: 0.607361\n",
      "MAE train: 0.179542\tval: 0.455323\n",
      "pearson_R train: 0.947204\tval: 0.580932\n",
      "Epoch: 993\n",
      "Loss: 0.18938416077031028\n",
      "RMSE train: 0.228820\tval: 0.584420\n",
      "MAE train: 0.177068\tval: 0.437797\n",
      "pearson_R train: 0.947461\tval: 0.594958\n",
      "Epoch: 994\n",
      "Loss: 0.1790087173382441\n",
      "RMSE train: 0.205160\tval: 0.595202\n",
      "MAE train: 0.160270\tval: 0.446868\n",
      "pearson_R train: 0.955818\tval: 0.572782\n",
      "Epoch: 995\n",
      "Loss: 0.17856574720806545\n",
      "RMSE train: 0.223345\tval: 0.625650\n",
      "MAE train: 0.176961\tval: 0.466966\n",
      "pearson_R train: 0.955396\tval: 0.575174\n",
      "Epoch: 996\n",
      "Loss: 0.17192092372311485\n",
      "RMSE train: 0.207989\tval: 0.597129\n",
      "MAE train: 0.158080\tval: 0.452070\n",
      "pearson_R train: 0.955181\tval: 0.577376\n",
      "Epoch: 997\n",
      "Loss: 0.18276577525668675\n",
      "RMSE train: 0.224619\tval: 0.609868\n",
      "MAE train: 0.171948\tval: 0.456249\n",
      "pearson_R train: 0.954636\tval: 0.589999\n",
      "Epoch: 998\n",
      "Loss: 0.17211899326907265\n",
      "RMSE train: 0.204035\tval: 0.588082\n",
      "MAE train: 0.159461\tval: 0.437970\n",
      "pearson_R train: 0.956377\tval: 0.587936\n",
      "Epoch: 999\n",
      "Loss: 0.17710008554988438\n",
      "RMSE train: 0.223913\tval: 0.601770\n",
      "MAE train: 0.178830\tval: 0.452036\n",
      "pearson_R train: 0.954002\tval: 0.586752\n",
      "Epoch: 1000\n",
      "Loss: 0.18401474588447148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/20 19:52:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train: 0.214127\tval: 0.597057\n",
      "MAE train: 0.167561\tval: 0.441810\n",
      "pearson_R train: 0.953187\tval: 0.589041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/20 19:52:42 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/01/20 19:52:42 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/01/20 19:52:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (RMSE), RMSE train: 0.441807\tval: 0.551311\n",
      "Best (RMSE), MAE train: 0.336492\tval: 0.411289\n",
      "Best (RMSE), pearson_R train: 0.792374\tval: 0.602925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ck1d-ER/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/ck1d-ER/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Set a MLflow Experiment\n",
    "# mlflow.end_run()\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"GIN MDCK regression\")\n",
    "\n",
    "# MLflow tag\n",
    "mlflow_tag = \"GIN adapted from Molscaling code, with annealing, no pretraining, with output_layer train/eval statements, without as many global vars\"\n",
    "\n",
    "# Set hyperparameters\n",
    "dataset_name = 'mdck'\n",
    "num_tasks = get_num_task(dataset_name)\n",
    "split = 'random'\n",
    "batch_size = 256\n",
    "num_layer = 5\n",
    "emb_dim = 300\n",
    "dropout_ratio = 0.5\n",
    "lr = 5e-4\n",
    "epochs = 1000\n",
    "pubchem_pretrain = False\n",
    "lr_cosine_length = 400000\n",
    "lr_warmup_steps = 1e4\n",
    "params = {\n",
    "    'dataset_name': dataset_name,\n",
    "    'split': split,\n",
    "    'batch_size': batch_size,\n",
    "    'num_layer': num_layer,\n",
    "    'emb_dim': emb_dim,\n",
    "    'dropout_ratio': dropout_ratio,\n",
    "    'lr': lr,\n",
    "    'epochs': epochs,\n",
    "    'pubchem_pretrain': pubchem_pretrain,\n",
    "    'lr_cosine_length': lr_cosine_length,\n",
    "    'lr_warmup_steps': lr_warmup_steps,\n",
    "}\n",
    "\n",
    "# Set your dataset directory\n",
    "dataset_folder = '/home/ubuntu/adme/MolScaling/datasets/molecule_net/'\n",
    "dataset = MoleculeDataset(dataset_folder + dataset_name, dataset=dataset_name)\n",
    "\n",
    "# Set device and seed\n",
    "device = torch.device('cuda') \n",
    "seed = 0\n",
    "seed_all(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Initalize model\n",
    "model_param_group = []\n",
    "model = GNN(num_layer=num_layer, emb_dim=emb_dim, drop_ratio=dropout_ratio).to(device)\n",
    "output_layer = MLP(in_channels=emb_dim, hidden_channels=emb_dim, \n",
    "                    out_channels=num_tasks, num_layers=1, dropout=0).to(device)\n",
    "\n",
    "if pubchem_pretrain:\n",
    "    output_model_dir = '/home/ubuntu/adme/MolScaling/model_saved/'\n",
    "    model_root = 'PubChem_Pretrained.pth'\n",
    "    model.load_state_dict(torch.load(output_model_dir + model_root, map_location='cuda:0'))\n",
    "    print('======= Model Loaded =======')\n",
    "    \n",
    "model_param_group.append({'params': output_layer.parameters(),'lr': lr})\n",
    "model_param_group.append({'params': model.parameters(), 'lr': lr})\n",
    "print(model)                \n",
    "\n",
    "# Initalize optimizer and metrics\n",
    "optimizer = optim.Adam(model_param_group, lr=lr, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, lr_cosine_length)\n",
    "warmup_steps = 0\n",
    "reg_criterion = torch.nn.MSELoss()\n",
    "train_result_list, val_result_list, test_result_list = [], [], []\n",
    "metric_list = ['RMSE', 'MAE', 'pearson_R']\n",
    "best_val_mae, best_val_idx = 1e10, 0\n",
    "\n",
    "# Split data\n",
    "if split == 'scaffold':\n",
    "    smiles_list = pd.read_csv(dataset_folder + dataset_name + '/processed/smiles.csv',\n",
    "                                header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset, (train_smiles, valid_smiles, test_smiles), (_,_,_) = scaffold_split(\n",
    "        dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.2, frac_test=0, return_smiles=True)\n",
    "        # dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, return_smiles=True)\n",
    "    print('split via scaffold')\n",
    "elif split == 'random':\n",
    "    smiles_list = pd.read_csv(dataset_folder + dataset_name + '/processed/smiles.csv',\n",
    "                                header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset, (train_smiles, valid_smiles, test_smiles),_ = random_split(\n",
    "        dataset, null_value=0, frac_train=0.8, frac_valid=0.2, frac_test=0, seed=seed, smiles_list=smiles_list)\n",
    "        # dataset, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed, smiles_list=smiles_list)\n",
    "    print('randomly split')\n",
    "\n",
    "# Set dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=8)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                             shuffle=False, num_workers=8)\n",
    "\n",
    "meann, mad = compute_mean_mad(train_dataset.data.y)\n",
    "train_func = train_general\n",
    "eval_func = eval_general\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.set_tag(\"Description\", mlflow_tag)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_func(\n",
    "            model=model, \n",
    "            output_layer=output_layer,\n",
    "            params=params,\n",
    "            device=device, \n",
    "            loader=train_loader, \n",
    "            reg_criterion=reg_criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "        print('Epoch: {}\\nLoss: {}'.format(epoch, loss))\n",
    "\n",
    "        train_result, train_target, train_pred = eval_func(\n",
    "            model, output_layer, device, train_loader)\n",
    "        train_result_list.append(train_result)\n",
    "        \n",
    "        # test_result, test_target, test_pred = eval_func(model, device, test_loader)\n",
    "        # test_result_list.append(test_result)\n",
    "        \n",
    "        val_result, val_target, val_pred = eval_func(\n",
    "            model, output_layer, device, val_loader)\n",
    "        val_result_list.append(val_result)\n",
    "\n",
    "        for metric in metric_list:\n",
    "            print('{} train: {:.6f}\\tval: {:.6f}'.format(metric, train_result[metric], val_result[metric]))\n",
    "            # print('{} train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}'.format(metric, train_result[metric], val_result[metric], test_result[metric]))\n",
    "\n",
    "        if val_result['MAE'] < best_val_mae:\n",
    "            best_val_mae = val_result['MAE']\n",
    "            best_val_idx = epoch - 1\n",
    "            \n",
    "\n",
    "        train_result = {'train_' + key: value for key, value in train_result.items()}\n",
    "        mlflow.log_metrics(train_result, step=epoch)\n",
    "        mlflow.log_metrics(val_result, step=epoch)\n",
    "        mlflow.log_metric('train_loss', loss, step=epoch)\n",
    "        \n",
    "    # signature = infer_signature(train_dataset, train_pred)\n",
    "    model_info = mlflow.pytorch.log_model(model, \"model\")\n",
    "    model_info = mlflow.pytorch.log_model(output_layer, \"output_layer\")\n",
    "\n",
    "for metric in metric_list:\n",
    "    print('Best (RMSE), {} train: {:.6f}\\tval: {:.6f}'.format(\n",
    "        metric, train_result_list[best_val_idx][metric], val_result_list[best_val_idx][metric]))\n",
    "    # print('Best (RMSE), {} train: {:.6f}\\tval: {:.6f}\\ttest: {:.6f}'.format(\n",
    "        # metric, train_result_list[best_val_idx][metric], val_result_list[best_val_idx][metric], test_result_list[best_val_idx][metric]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ck1d-ER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
